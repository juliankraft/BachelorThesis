{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchvision.transforms import v2\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "from ba_dev.dataset import MammaliaDataImage\n",
    "from ba_dev.datamodule import MammaliaDataModule\n",
    "from ba_dev.transform import ImagePipeline\n",
    "from ba_dev.model import LightningModelImage\n",
    "from ba_dev.trainer import MammaliaTrainer\n",
    "from ba_dev.utils import count_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_banner(text, width=80, border_char='-'):\n",
    "    inner_width = width - 4\n",
    "    line = border_char * (width - 2)\n",
    "    centered = text.center(inner_width)\n",
    "\n",
    "    print(f'+{line}+')\n",
    "    print(f'| {centered} |')\n",
    "    print(f'+{line}+')\n",
    "\n",
    "\n",
    "def read_config_yaml(config_path):\n",
    "    try:\n",
    "        with open(config_path) as f:\n",
    "            return yaml.load(f, Loader=yaml.FullLoader)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            f'Config file not found at {config_path}. Please provide a valid path.'\n",
    "            )\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(\n",
    "            f'Error parsing YAML file at {config_path}: {e}'\n",
    "            )\n",
    "\n",
    "\n",
    "def set_up_image_pipeline(cfg):\n",
    "    pre_ops = []\n",
    "    if cfg['to_rgb']:\n",
    "        pre_ops.append(('to_rgb', {}))\n",
    "    if cfg['crop_by_bb']:\n",
    "        pre_ops.append(('crop_by_bb', {'crop_shape': cfg['crop_by_bb']}))\n",
    "\n",
    "    ops = []\n",
    "    if cfg['to_tensor']:\n",
    "        ops.append(v2.ToImage())\n",
    "        ops.append(v2.ToDtype(torch.float32, scale=True))\n",
    "\n",
    "    resize = cfg['resize']\n",
    "    if resize:\n",
    "        if isinstance(resize, int):\n",
    "            ops.append(v2.Resize((resize, resize)))\n",
    "        elif isinstance(resize, Sequence) and len(resize) == 2:\n",
    "            ops.append(v2.Resize((resize)))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Invalid resize value: {resize}. Must be int or Sequence of two ints.'\n",
    "                )\n",
    "\n",
    "    norm = cfg['normalize']\n",
    "    if norm:\n",
    "        if isinstance(norm, dict):\n",
    "            mean = norm['mean']\n",
    "            std = norm['std']\n",
    "        elif isinstance(norm, str):\n",
    "            if norm.lower() == 'imagenet':\n",
    "                mean = [0.485, 0.456, 0.406]\n",
    "                std = [0.229, 0.224, 0.225]\n",
    "            else:\n",
    "                stats = torch.load(norm)\n",
    "                mean = stats['mean']\n",
    "                std = stats['std']\n",
    "        ops.append(v2.Normalize(mean=mean, std=std))\n",
    "\n",
    "    image_pipeline = ImagePipeline(\n",
    "        pre_ops=pre_ops,\n",
    "        transform=v2.Compose(ops)\n",
    "        )\n",
    "\n",
    "    augment = cfg['augmentation']\n",
    "    if not augment:\n",
    "        augmented_image_pipeline = None\n",
    "    else:\n",
    "        ops_aug = list(ops)\n",
    "\n",
    "        for entry in augment:\n",
    "            name, params = next(iter(entry.items()))\n",
    "            Op = getattr(v2, name, None)\n",
    "            if Op is None:\n",
    "                raise ValueError(f'Unknown transform: {name!r}')\n",
    "            ops_aug.append(Op(**(params or {})))\n",
    "\n",
    "        augmented_image_pipeline = ImagePipeline(\n",
    "                    pre_ops=pre_ops,\n",
    "                    transform=v2.Compose(ops_aug)\n",
    "                    )\n",
    "\n",
    "    return image_pipeline, augmented_image_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_output_dir = \"/cfs/earth/scratch/kraftjul/BA/output/test\"\n",
    "args_config_path = \"/cfs/earth/scratch/kraftjul/BA/code/run/config_template.yaml\"\n",
    "args_dev_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Path(args_output_dir)\n",
    "if dir.exists():\n",
    "    shutil.rmtree(dir)\n",
    "\n",
    "dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|                       !!!   Running in dev mode   !!!                        |\n",
      "+------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "if args_dev_run:\n",
    "    print_banner('!!!   Running in dev mode   !!!', width=80)\n",
    "\n",
    "config_path = Path(args_config_path)\n",
    "output_dir = Path(args_output_dir)\n",
    "experiment_info_path = output_dir / 'experiment_info.yaml'\n",
    "\n",
    "if not output_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'Output directory {output_dir} does not exist. Please provide a valid path.'\n",
    "    )\n",
    "\n",
    "cfg = read_config_yaml(config_path)\n",
    "\n",
    "try:\n",
    "    shutil.copy2(config_path, experiment_info_path)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to copy config file to {experiment_info_path}: {e}\"\n",
    "        )\n",
    "\n",
    "# setting up image pipeline\n",
    "image_pipeline, augmented_image_pipeline = set_up_image_pipeline(cfg['image_pipeline'])\n",
    "\n",
    "# setting up datamodule config\n",
    "label_key = 'test_labels' if args_dev_run else 'labels'\n",
    "dataset_raw = cfg.get('dataset') or {}\n",
    "paths = cfg['paths']\n",
    "dataset_kwargs = {\n",
    "    'path_labelfiles': paths[label_key],\n",
    "    'path_to_dataset': paths['dataset'],\n",
    "    'path_to_detector_output': paths['md_output'],\n",
    "    **dataset_raw\n",
    "    }\n",
    "\n",
    "datamodule_raw = cfg.get('data_module') or {}\n",
    "datamodule_cfg = {\n",
    "    'dataset_cls': MammaliaDataImage,\n",
    "    'image_pipeline': image_pipeline,\n",
    "    'augmented_image_pipeline': augmented_image_pipeline,\n",
    "    **datamodule_raw\n",
    "    }\n",
    "\n",
    "# setting up model config\n",
    "model_cfg = cfg['model']\n",
    "\n",
    "# setting up trainer config\n",
    "trainer_raw = cfg.get('trainer') or {}\n",
    "\n",
    "_not_dev_defaults = {\n",
    "    'limit_train_batches': 1.0,\n",
    "    'limit_val_batches': 1.0,\n",
    "    'limit_test_batches': 1.0,\n",
    "    'limit_predict_batches': 1.0,\n",
    "    'max_epochs': -1,\n",
    "    'log_every_n_steps': 10,\n",
    "    }\n",
    "\n",
    "if args_dev_run:\n",
    "    dev_run_args = {\n",
    "        'limit_train_batches': 1,\n",
    "        'limit_val_batches': 1,\n",
    "        'limit_test_batches': 1,\n",
    "        'limit_predict_batches': 1,\n",
    "        'max_epochs': 1,\n",
    "        'log_every_n_steps': 1\n",
    "        }\n",
    "else:\n",
    "    dev_run_args = (trainer_raw.get('not_dev') or {}).copy()\n",
    "    for key, value in _not_dev_defaults.items():\n",
    "        if key not in dev_run_args:\n",
    "            dev_run_args[key] = value\n",
    "\n",
    "trainer_kwargs = {\n",
    "    **(trainer_raw.get('trainer_kwargs') or {}),\n",
    "    **dev_run_args\n",
    "    }\n",
    "\n",
    "trainer_cfg = (trainer_raw.get('base_args') or {}).copy()\n",
    "trainer_cfg['trainer_kwargs'] = trainer_kwargs\n",
    "\n",
    "trainer_do_predict = trainer_raw['do_predict']\n",
    "\n",
    "# setting up folds or cross-validation\n",
    "cross_val = cfg['cross_val']['apply']\n",
    "n_folds = cfg['cross_val']['n_folds']\n",
    "test_fold = cfg['cross_val']['test_fold']\n",
    "\n",
    "if cross_val:\n",
    "    if args_dev_run:\n",
    "        folds = range(2)\n",
    "    else:\n",
    "        folds = range(n_folds)\n",
    "else:\n",
    "    folds = [test_fold]\n",
    "\n",
    "log_dir = output_dir / 'logs'\n",
    "\n",
    "run_params = {}\n",
    "if cross_val:\n",
    "    run_params['folds'] = {}\n",
    "    \n",
    "first_pass = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|                      Running cross-validation fold 1/2                       |\n",
      "+------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/BA_package/ba_dev/dataset.py:250: UserWarning: With the detection confidence of 0.25,\n",
      "8 sequences had no detections and will be excluded.\n",
      "Excluded sequences: [6000161, 6000163, 6000293, 6000530, 6000691, 6000372, 6000953, 6000186]\n",
      "  warnings.warn(\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory /cfs/earth/scratch/kraftjul/BA/output/test/logs/fold_0/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | backbone      | EfficientNet     | 4.0 M  | train\n",
      "1 | criterion     | CrossEntropyLoss | 0      | train\n",
      "2 | train_metrics | MetricCollection | 0      | train\n",
      "3 | val_metrics   | MetricCollection | 0      | train\n",
      "4 | test_metrics  | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.051    Total estimated model params size (MB)\n",
      "347       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11e0454d92a42559dc28c9ff1d52a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc44d330b864719a68c1418dcfedcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f076d36aae47798d0fb0006aa42e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbba099a30d4ebe83fc688a3f716b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /cfs/earth/scratch/kraftjul/BA/output/test/logs/fold_0/checkpoints/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                 0.796875\n",
      "      test_bal_acc              0.19921875\n",
      "        test_loss           1.2343661785125732\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at /cfs/earth/scratch/kraftjul/BA/output/test/logs/fold_0/checkpoints/best.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3921db66dc7f49cabc47c55895ca9b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|                      Running cross-validation fold 2/2                       |\n",
      "+------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/BA_package/ba_dev/dataset.py:250: UserWarning: With the detection confidence of 0.25,\n",
      "8 sequences had no detections and will be excluded.\n",
      "Excluded sequences: [6000161, 6000163, 6000293, 6000530, 6000691, 6000372, 6000953, 6000186]\n",
      "  warnings.warn(\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory /cfs/earth/scratch/kraftjul/BA/output/test/logs/fold_1/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | backbone      | EfficientNet     | 4.0 M  | train\n",
      "1 | criterion     | CrossEntropyLoss | 0      | train\n",
      "2 | train_metrics | MetricCollection | 0      | train\n",
      "3 | val_metrics   | MetricCollection | 0      | train\n",
      "4 | test_metrics  | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.051    Total estimated model params size (MB)\n",
      "347       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15a3c3bcf154b1abd6657e02592fe81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eddc6a4aaef4e6789ab66b440b54565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6b146108f44c8da4e6192aeb16c017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847fc6359f134827bc802322f2767580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /cfs/earth/scratch/kraftjul/BA/output/test/logs/fold_1/checkpoints/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "      test_bal_acc                  1.0\n",
      "        test_loss           0.8041755557060242\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at /cfs/earth/scratch/kraftjul/BA/output/test/logs/fold_1/checkpoints/best.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ba428e470c4d3aa80d74247d14b70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|                            Experiment completed!                             |\n",
      "+------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# running the experiment\n",
    "for fold in folds:\n",
    "    if cross_val:\n",
    "        trainer_log_dir = log_dir / f'fold_{fold}'\n",
    "        print_statement = f'Running cross-validation fold {fold+1}/{len(folds)}'\n",
    "        \n",
    "    else:\n",
    "        trainer_log_dir = log_dir\n",
    "        print_statement = f'Running Experiment with test fold = {test_fold}'\n",
    "\n",
    "    print_banner(print_statement, width=80)\n",
    "    trainer_log_dir.mkdir(parents=True)\n",
    "\n",
    "    dm_cfg = datamodule_cfg.copy()\n",
    "    dm_cfg['dataset_kwargs'] = dataset_kwargs.copy()\n",
    "    datamodule = MammaliaDataModule(\n",
    "                    n_folds=5,\n",
    "                    test_fold=fold,\n",
    "                    **dm_cfg,\n",
    "                    )\n",
    "\n",
    "    m_cfg = model_cfg.copy()\n",
    "    model = LightningModelImage(\n",
    "                    num_classes=datamodule.num_classes,\n",
    "                    class_weights=datamodule.class_weights,\n",
    "                    **m_cfg\n",
    "                    )\n",
    "\n",
    "    t_cfg = trainer_cfg.copy()\n",
    "    trainer = MammaliaTrainer(\n",
    "                    log_dir=trainer_log_dir,\n",
    "                    **t_cfg\n",
    "                    )\n",
    "    \n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        datamodule=datamodule\n",
    "        )\n",
    "\n",
    "    test_metrics = trainer.test(\n",
    "        model=model,\n",
    "        datamodule=datamodule\n",
    "        )\n",
    "\n",
    "\n",
    "    if trainer_do_predict:\n",
    "        best_ckpt = trainer_log_dir / 'checkpoints' / 'best.ckpt'\n",
    "        trainer.predict(\n",
    "            model=model,\n",
    "            datamodule=datamodule,\n",
    "            ckpt_path=best_ckpt,\n",
    "            return_predictions=False\n",
    "            )\n",
    "    \n",
    "    fold_params = {}\n",
    "    fold_params['test_metrics'] = test_metrics[0]\n",
    "    fold_params['class_weights'] = datamodule.class_weights.tolist()\n",
    "\n",
    "    if first_pass:\n",
    "        dataset = datamodule.get_dataset('pred')\n",
    "        df = dataset.get_ds_with_folds()\n",
    "        df.to_csv(log_dir / 'dataset.csv', index=False)\n",
    "        run_params['trainable_params'] = count_trainable_parameters(model)\n",
    "        run_params['label_decoder'] = dataset.label_decoder\n",
    "        run_params['num_classes'] = datamodule.num_classes\n",
    "        del dataset, df  \n",
    "        first_pass = False\n",
    "    \n",
    "    if cross_val:\n",
    "        fold_params['test_fold'] = fold\n",
    "        run_params['folds'][fold] = fold_params\n",
    "    else:\n",
    "        run_params.update(fold_params)\n",
    "\n",
    "run_output = {'output': run_params}\n",
    "with open(experiment_info_path, \"a\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    yaml.dump(run_output, f, default_flow_style=False)\n",
    "\n",
    "print_banner('Experiment completed!', width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6070, 2.0583, 0.7733, 1.7436])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from ba_dev.dataset import MammaliaDataSequence, MammaliaDataImage\n",
    "from ba_dev.datamodule import MammaliaDataModule\n",
    "from ba_dev.transform import ImagePipeline, BatchImagePipeline\n",
    "from ba_dev.model import LightningModelImage\n",
    "from ba_dev.trainer import MammaliaTrainer\n",
    "from ba_dev.utils import load_path_yaml\n",
    "\n",
    "paths = load_path_yaml('/cfs/earth/scratch/kraftjul/BA/data/path_config.yml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/BA_package/ba_dev/dataset.py:250: UserWarning: With the detection confidence of 0.25,\n",
      "8 sequences had no detections and will be excluded.\n",
      "Excluded sequences: [6000161, 6000163, 6000293, 6000530, 6000691, 6000372, 6000953, 6000186]\n",
      "  warnings.warn(\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "stats = torch.load(paths['feature_stats'])\n",
    "\n",
    "image_pipeline = ImagePipeline(\n",
    "        pre_ops=[\n",
    "            ('to_rgb', {}),\n",
    "            ('crop_by_bb', {'crop_shape': 1.0})\n",
    "            ],\n",
    "        transform=v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((224, 224)),\n",
    "            v2.Normalize(\n",
    "                mean=stats['mean'],\n",
    "                std=stats['std']\n",
    "                )\n",
    "            ])\n",
    "        )\n",
    "\n",
    "dataset_kwargs = {\n",
    "        'path_labelfiles': paths['test_labels'],\n",
    "        'path_to_dataset': paths['dataset'],\n",
    "        'path_to_detector_output': paths['md_output'],\n",
    "        }\n",
    "\n",
    "datamodule = MammaliaDataModule(\n",
    "                dataset_cls=MammaliaDataImage,\n",
    "                dataset_kwargs=dataset_kwargs,\n",
    "                n_folds=5,\n",
    "                test_fold=0,\n",
    "                image_pipeline=image_pipeline,\n",
    "                augmented_image_pipeline=None,\n",
    "                batch_size=32,\n",
    "                num_workers=1,\n",
    "                pin_memory=True,\n",
    "                )\n",
    "\n",
    "model = LightningModelImage(\n",
    "            num_classes=datamodule.num_classes,\n",
    "            class_weights=datamodule.class_weights,\n",
    "            backbone_name='efficientnet_b0',\n",
    "            backbone_pretrained=True,\n",
    "            backbone_weights='DEFAULT',\n",
    "            optimizer_name='AdamW',\n",
    "            optimizer_kwargs={\n",
    "                'lr': 1e-3,\n",
    "                'weight_decay': 1e-5,\n",
    "                'amsgrad': False\n",
    "                },\n",
    "            scheduler_name='CosineAnnealingLR',\n",
    "            scheduler_kwargs={'T_max': 5},\n",
    "            )\n",
    "\n",
    "log_dir = Path('/cfs/earth/scratch/kraftjul/BA/output/test')\n",
    "if log_dir.exists():\n",
    "    shutil.rmtree(log_dir)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer = MammaliaTrainer(\n",
    "            log_dir=log_dir,\n",
    "            pred_writer_log_keys=['class_id', 'set', 'pred_id', 'probs'],\n",
    "            pred_writer_prob_precision=4,\n",
    "            accelerator='cpu',\n",
    "            patience=5,\n",
    "            trainer_kwargs={\n",
    "                'log_every_n_steps': 1,\n",
    "                'max_epochs': 1,\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory /cfs/earth/scratch/kraftjul/BA/output/test/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | backbone      | EfficientNet     | 4.0 M  | train\n",
      "1 | criterion     | CrossEntropyLoss | 0      | train\n",
      "2 | train_metrics | MetricCollection | 0      | train\n",
      "3 | val_metrics   | MetricCollection | 0      | train\n",
      "4 | test_metrics  | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.051    Total estimated model params size (MB)\n",
      "347       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1988486a0df4a91b92def6f916a5603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e573585f17412d803b36fbf035d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d8ea33a5a45bdabc8f1e92b9043ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bdf7576c354f55b1a0ea851c8f6be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9027237296104431\n",
      "      test_bal_acc          0.5089578628540039\n",
      "        test_loss           0.3492245376110077\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "testmetrics = trainer.test(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.3492245376110077,\n",
       "  'test_acc': 0.9027237296104431,\n",
       "  'test_bal_acc': 0.5089578628540039}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory /cfs/earth/scratch/kraftjul/BA/output/test/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | backbone      | EfficientNet     | 4.0 M  | train\n",
      "1 | criterion     | CrossEntropyLoss | 0      | train\n",
      "2 | train_metrics | MetricCollection | 0      | train\n",
      "3 | val_metrics   | MetricCollection | 0      | train\n",
      "4 | test_metrics  | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.051    Total estimated model params size (MB)\n",
      "347       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ae0749cbf349f4a175a74c05e980b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cd1fb11b9c435382ef37a6435acda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8b3a44a1ae4149aa8e7dfba414892e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08d72314ed24ec8a21f81cf8c252e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W503 22:56:28.291574881 NNPACK.cpp:62] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Restoring states from the checkpoint path at /cfs/earth/scratch/kraftjul/BA/output/test/checkpoints/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.7898832559585571\n",
      "      test_bal_acc          0.6755728721618652\n",
      "        test_loss           0.9037746787071228\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at /cfs/earth/scratch/kraftjul/BA/output/test/checkpoints/best.ckpt\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251dde83f4254073a9ff0a6df7eebad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "best_ckpt = trainer.checkpoint_callback.best_model_path\n",
    "trainer.predict(\n",
    "    model,\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=best_ckpt,\n",
    "    return_predictions=False  # our PredictionWriter will write to CSV\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /cfs/earth/scratch/kraftjul/BA/output/test/checkpoints/best.ckpt\n",
      "Loaded model weights from the checkpoint at /cfs/earth/scratch/kraftjul/BA/output/test/checkpoints/best.ckpt\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af2922f2bbb4cbea47315687eeaeee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run predict and capture the return value\n",
    "predictions = trainer.predict(\n",
    "    model,\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path='best',            # or best_ckpt = trainer.checkpoint_callback.best_model_path\n",
    "    return_predictions=True      # make sure you return, not False\n",
    ")\n",
    "\n",
    "# `predictions` is now a list of per‐batch outputs\n",
    "# (each entry is whatever your `predict_step` returned,\n",
    "# e.g. a dict with keys: 'class_id','bbox','conf','seq_id','set','file_path','preds','probs')\n",
    "\n",
    "# inspect the first batch:\n",
    "first_batch = predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_id': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'bbox': [tensor([0.0000e+00, 4.0000e-04, 6.3900e-02, 0.0000e+00, 1.1710e-01, 4.0000e-04,\n",
       "          4.0000e-04, 0.0000e+00, 9.2200e-02, 4.4580e-01, 2.1900e-02, 1.3570e-01,\n",
       "          2.8220e-01, 1.0440e-01, 1.3470e-01, 1.3570e-01, 1.3810e-01, 1.3370e-01,\n",
       "          4.0000e-04, 2.1330e-01, 4.0000e-04, 1.4000e-03, 2.5000e-01, 4.0000e-04,\n",
       "          4.0000e-04, 0.0000e+00, 1.2150e-01, 1.5960e-01, 1.4400e-01, 4.5400e-02,\n",
       "          9.8100e-02, 4.0180e-01, 1.5180e-01, 6.1470e-01, 2.1480e-01, 2.3090e-01,\n",
       "          2.1330e-01, 5.2920e-01, 5.3660e-01, 2.1920e-01, 1.2500e-01, 6.9720e-01,\n",
       "          1.3470e-01, 9.5700e-02, 5.3270e-01, 5.3850e-01, 5.3900e-01, 5.3220e-01,\n",
       "          5.3220e-01, 5.3220e-01, 5.4730e-01, 6.7720e-01, 5.3220e-01, 5.3360e-01,\n",
       "          5.1120e-01, 5.5320e-01, 5.3560e-01, 5.3220e-01, 5.2680e-01, 5.3120e-01,\n",
       "          5.4240e-01, 5.4000e-01, 5.3120e-01, 6.7040e-01], dtype=torch.float64),\n",
       "  tensor([0.4687, 0.3203, 0.5429, 0.5279, 0.4915, 0.2545, 0.5716, 0.5429, 0.5058,\n",
       "          0.4765, 0.5136, 0.5156, 0.5579, 0.4967, 0.5214, 0.5058, 0.4947, 0.5195,\n",
       "          0.7096, 0.7317, 0.7207, 0.7005, 0.6816, 0.7272, 0.7278, 0.7519, 0.5423,\n",
       "          0.4954, 0.5195, 0.6152, 0.4700, 0.5436, 0.5462, 0.5442, 0.5130, 0.5240,\n",
       "          0.4973, 0.5449, 0.5299, 0.4941, 0.4687, 0.5325, 0.5598, 0.5644, 0.5527,\n",
       "          0.5488, 0.5449, 0.5527, 0.5520, 0.5475, 0.5546, 0.6334, 0.5559, 0.5410,\n",
       "          0.5286, 0.5364, 0.5403, 0.5579, 0.5494, 0.5559, 0.5351, 0.5397, 0.5559,\n",
       "          0.5696], dtype=torch.float64),\n",
       "  tensor([0.4111, 0.4101, 0.1953, 0.4008, 0.5185, 0.4174, 0.3950, 0.3735, 0.1625,\n",
       "          0.3759, 0.2324, 0.3315, 0.2446, 0.1513, 0.1816, 0.2377, 0.1674, 0.1826,\n",
       "          0.3598, 0.5537, 0.2822, 0.4301, 0.5605, 0.2631, 0.2099, 0.2563, 0.1606,\n",
       "          0.1420, 0.1381, 0.1181, 0.3066, 0.4213, 0.4541, 0.2250, 0.2236, 0.3930,\n",
       "          0.2309, 0.3007, 0.2856, 0.1474, 0.2333, 0.1533, 0.2309, 0.0869, 0.2783,\n",
       "          0.2734, 0.2724, 0.2792, 0.2807, 0.2788, 0.2626, 0.1923, 0.2802, 0.2783,\n",
       "          0.2998, 0.2568, 0.2763, 0.2792, 0.2851, 0.2807, 0.2680, 0.2719, 0.2812,\n",
       "          0.1459], dtype=torch.float64),\n",
       "  tensor([0.3671, 0.5136, 0.3268, 0.3001, 0.2434, 0.5800, 0.2656, 0.2871, 0.2584,\n",
       "          0.1796, 0.2506, 0.1861, 0.4062, 0.2532, 0.2740, 0.1868, 0.2083, 0.2747,\n",
       "          0.2688, 0.2454, 0.2578, 0.2779, 0.2955, 0.2493, 0.2402, 0.2259, 0.2858,\n",
       "          0.2916, 0.2753, 0.2220, 0.2395, 0.1588, 0.1699, 0.1503, 0.2024, 0.1822,\n",
       "          0.2070, 0.1503, 0.1764, 0.3333, 0.2408, 0.1653, 0.1529, 0.0390, 0.1380,\n",
       "          0.1419, 0.1458, 0.1380, 0.1386, 0.1432, 0.1367, 0.2500, 0.1347, 0.1503,\n",
       "          0.1614, 0.1536, 0.1503, 0.1328, 0.1412, 0.1347, 0.1549, 0.1510, 0.1347,\n",
       "          0.2408], dtype=torch.float64)],\n",
       " 'conf': tensor([0.9830, 0.9740, 0.9720, 0.9720, 0.9700, 0.9690, 0.9690, 0.9670, 0.9540,\n",
       "         0.9530, 0.9510, 0.9480, 0.9460, 0.9440, 0.9420, 0.9410, 0.9330, 0.9210,\n",
       "         0.9020, 0.8790, 0.8770, 0.8490, 0.8110, 0.6510, 0.4570, 0.3850, 0.9410,\n",
       "         0.9280, 0.9220, 0.8880, 0.9780, 0.9730, 0.9720, 0.9700, 0.9700, 0.9660,\n",
       "         0.9640, 0.9610, 0.9610, 0.9490, 0.9480, 0.9220, 0.8700, 0.3240, 0.9740,\n",
       "         0.9730, 0.9720, 0.9720, 0.9710, 0.9710, 0.9700, 0.9690, 0.9690, 0.9690,\n",
       "         0.9690, 0.9680, 0.9680, 0.9680, 0.9670, 0.9670, 0.9660, 0.9660, 0.9660,\n",
       "         0.9650], dtype=torch.float64),\n",
       " 'seq_id': tensor([4007156, 4007156, 4007156, 4007156, 4007156, 4007156, 4007156, 4007156,\n",
       "         4007156, 4007156, 4007156, 4007156, 4007156, 4007156, 4007156, 4007156,\n",
       "         4007156, 4007156, 4007156, 4007156, 4007156, 4007156, 4007156, 4007156,\n",
       "         4007156, 4007156, 4011466, 4011466, 4011466, 4011466, 1001887, 1001887,\n",
       "         1001887, 1001887, 1001887, 1001887, 1001887, 1001887, 1001887, 1001887,\n",
       "         1001887, 1001887, 1001887, 1001887, 4010684, 4010684, 4010684, 4010684,\n",
       "         4010684, 4010684, 4010684, 4010684, 4010684, 4010684, 4010684, 4010684,\n",
       "         4010684, 4010684, 4010684, 4010684, 4010684, 4010684, 4010684, 4010684]),\n",
       " 'set': ['train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'val',\n",
       "  'val',\n",
       "  'val',\n",
       "  'val',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test',\n",
       "  'test'],\n",
       " 'file': ['IMG_6165.JPG',\n",
       "  'IMG_6164.JPG',\n",
       "  'IMG_6162.JPG',\n",
       "  'IMG_6168.JPG',\n",
       "  'IMG_6178.JPG',\n",
       "  'IMG_6163.JPG',\n",
       "  'IMG_6166.JPG',\n",
       "  'IMG_6167.JPG',\n",
       "  'IMG_6155.JPG',\n",
       "  'IMG_6179.JPG',\n",
       "  'IMG_6156.JPG',\n",
       "  'IMG_6159.JPG',\n",
       "  'IMG_6177.JPG',\n",
       "  'IMG_6154.JPG',\n",
       "  'IMG_6160.JPG',\n",
       "  'IMG_6158.JPG',\n",
       "  'IMG_6157.JPG',\n",
       "  'IMG_6161.JPG',\n",
       "  'IMG_6170.JPG',\n",
       "  'IMG_6175.JPG',\n",
       "  'IMG_6169.JPG',\n",
       "  'IMG_6171.JPG',\n",
       "  'IMG_6176.JPG',\n",
       "  'IMG_6172.JPG',\n",
       "  'IMG_6173.JPG',\n",
       "  'IMG_6174.JPG',\n",
       "  'IMG_5448.JPG',\n",
       "  'IMG_5447.JPG',\n",
       "  'IMG_5446.JPG',\n",
       "  'IMG_5449.JPG',\n",
       "  'IMG_3040.JPG',\n",
       "  'IMG_3038.JPG',\n",
       "  'IMG_3039.JPG',\n",
       "  'IMG_3035.JPG',\n",
       "  'IMG_3048.JPG',\n",
       "  'IMG_3046.JPG',\n",
       "  'IMG_3047.JPG',\n",
       "  'IMG_3036.JPG',\n",
       "  'IMG_3037.JPG',\n",
       "  'IMG_3049.JPG',\n",
       "  'IMG_3041.JPG',\n",
       "  'IMG_3034.JPG',\n",
       "  'IMG_3042.JPG',\n",
       "  'IMG_3050.JPG',\n",
       "  'IMG_0619.JPG',\n",
       "  'IMG_0630.JPG',\n",
       "  'IMG_0642.JPG',\n",
       "  'IMG_0647.JPG',\n",
       "  'IMG_0628.JPG',\n",
       "  'IMG_0651.JPG',\n",
       "  'IMG_0618.JPG',\n",
       "  'IMG_0610.JPG',\n",
       "  'IMG_0629.JPG',\n",
       "  'IMG_0637.JPG',\n",
       "  'IMG_0652.JPG',\n",
       "  'IMG_0617.JPG',\n",
       "  'IMG_0624.JPG',\n",
       "  'IMG_0645.JPG',\n",
       "  'IMG_0640.JPG',\n",
       "  'IMG_0643.JPG',\n",
       "  'IMG_0620.JPG',\n",
       "  'IMG_0623.JPG',\n",
       "  'IMG_0635.JPG',\n",
       "  'IMG_0609.JPG'],\n",
       " 'preds': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "         2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'probs': tensor([[9.8355e-01, 2.8202e-04, 1.5514e-02, 6.5372e-04],\n",
       "         [9.9845e-01, 6.9802e-05, 1.1409e-03, 3.3793e-04],\n",
       "         [9.9890e-01, 1.2192e-05, 1.0876e-03, 4.8964e-06],\n",
       "         [9.9499e-01, 2.5338e-04, 4.0729e-03, 6.8432e-04],\n",
       "         [9.9907e-01, 1.8117e-04, 3.5400e-05, 7.1509e-04],\n",
       "         [9.9565e-01, 1.9552e-04, 3.6682e-03, 4.8374e-04],\n",
       "         [9.5502e-01, 2.5933e-03, 4.0288e-02, 2.1006e-03],\n",
       "         [9.5273e-01, 2.8992e-03, 2.0015e-02, 2.4355e-02],\n",
       "         [9.9943e-01, 2.5773e-05, 5.4141e-04, 4.1269e-06],\n",
       "         [9.1612e-01, 1.9639e-02, 2.7445e-02, 3.6799e-02],\n",
       "         [6.2223e-01, 2.1308e-02, 3.5575e-01, 7.0988e-04],\n",
       "         [9.9471e-01, 1.5543e-03, 4.9707e-04, 3.2415e-03],\n",
       "         [9.9975e-01, 1.2985e-05, 2.4839e-06, 2.3920e-04],\n",
       "         [9.9484e-01, 4.6639e-05, 5.0974e-03, 1.5679e-05],\n",
       "         [9.9998e-01, 9.1712e-07, 1.6283e-07, 1.6516e-05],\n",
       "         [9.9999e-01, 1.9872e-06, 5.0458e-06, 1.0684e-06],\n",
       "         [1.0000e+00, 3.8801e-08, 2.0076e-07, 9.5622e-09],\n",
       "         [1.0000e+00, 2.4125e-08, 1.8665e-07, 2.1231e-08],\n",
       "         [9.9921e-01, 5.8668e-04, 2.0296e-04, 1.8444e-06],\n",
       "         [9.5921e-01, 3.6605e-04, 4.0137e-02, 2.8826e-04],\n",
       "         [9.8554e-01, 2.5620e-04, 1.4170e-02, 3.1978e-05],\n",
       "         [9.9397e-01, 8.3837e-04, 5.1132e-03, 7.3460e-05],\n",
       "         [9.5269e-01, 1.0015e-02, 3.6784e-02, 5.1525e-04],\n",
       "         [7.8433e-02, 5.2141e-02, 8.5075e-01, 1.8677e-02],\n",
       "         [1.0540e-01, 1.6309e-01, 6.3367e-01, 9.7838e-02],\n",
       "         [7.8954e-02, 7.6955e-02, 7.8094e-01, 6.3146e-02],\n",
       "         [2.8756e-01, 8.9871e-04, 7.1149e-01, 5.8011e-05],\n",
       "         [9.9949e-01, 9.2660e-06, 5.0171e-04, 2.5886e-07],\n",
       "         [9.9682e-01, 9.2640e-06, 3.1637e-03, 6.4930e-06],\n",
       "         [9.4702e-03, 1.2354e-04, 9.8972e-01, 6.8316e-04],\n",
       "         [9.9999e-01, 1.2828e-05, 6.3291e-07, 1.2778e-07],\n",
       "         [9.6830e-01, 2.5425e-04, 3.1390e-02, 5.3509e-05],\n",
       "         [9.9852e-01, 1.0133e-04, 1.3251e-03, 5.6117e-05],\n",
       "         [9.9987e-01, 7.4848e-05, 5.6317e-05, 1.2040e-06],\n",
       "         [9.9913e-01, 1.0690e-04, 7.4471e-04, 1.6059e-05],\n",
       "         [9.9987e-01, 2.2401e-05, 9.0847e-05, 1.4910e-05],\n",
       "         [9.9625e-01, 9.9522e-05, 3.6296e-03, 2.2085e-05],\n",
       "         [9.9996e-01, 1.5199e-05, 2.4509e-05, 4.1023e-06],\n",
       "         [1.0000e+00, 8.4649e-07, 1.4598e-06, 9.5378e-08],\n",
       "         [9.9995e-01, 1.7800e-05, 3.0229e-05, 1.0492e-06],\n",
       "         [1.0000e+00, 9.4212e-09, 2.0778e-07, 2.2979e-09],\n",
       "         [9.9579e-01, 1.4386e-04, 4.0544e-03, 1.6196e-05],\n",
       "         [9.9932e-01, 1.9416e-05, 6.5832e-04, 3.6189e-06],\n",
       "         [1.3708e-01, 1.3000e-01, 3.6780e-01, 3.6512e-01],\n",
       "         [9.9983e-01, 1.5270e-05, 5.2656e-05, 1.0318e-04],\n",
       "         [9.9951e-01, 4.4618e-05, 6.1878e-05, 3.8179e-04],\n",
       "         [9.9976e-01, 4.1219e-05, 1.6140e-05, 1.7997e-04],\n",
       "         [9.9265e-01, 4.0033e-04, 3.0263e-04, 6.6488e-03],\n",
       "         [9.9891e-01, 1.1838e-04, 1.7393e-04, 7.9910e-04],\n",
       "         [9.9898e-01, 3.0879e-05, 3.9169e-05, 9.4790e-04],\n",
       "         [9.9850e-01, 2.6378e-04, 1.1550e-04, 1.1189e-03],\n",
       "         [9.9963e-01, 1.0968e-06, 3.6466e-04, 1.5621e-06],\n",
       "         [9.9758e-01, 1.7065e-04, 8.0629e-05, 2.1646e-03],\n",
       "         [9.9986e-01, 8.1367e-06, 4.5430e-05, 8.4431e-05],\n",
       "         [9.9969e-01, 3.4392e-05, 1.9106e-05, 2.5285e-04],\n",
       "         [9.9999e-01, 2.1601e-06, 2.9546e-06, 5.2518e-06],\n",
       "         [9.9997e-01, 1.9959e-06, 1.9405e-05, 5.3072e-06],\n",
       "         [9.5886e-01, 5.0368e-04, 7.4012e-04, 3.9893e-02],\n",
       "         [9.9469e-01, 2.7067e-04, 5.9697e-04, 4.4415e-03],\n",
       "         [9.8538e-01, 7.4183e-04, 6.9287e-04, 1.3187e-02],\n",
       "         [9.9997e-01, 2.9204e-06, 4.4501e-06, 1.8213e-05],\n",
       "         [9.9999e-01, 1.8028e-06, 3.4236e-06, 3.9001e-06],\n",
       "         [9.9204e-01, 3.6984e-04, 6.8786e-04, 6.9015e-03],\n",
       "         [1.0000e+00, 2.3327e-07, 5.9309e-07, 1.1985e-07]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_batch = []\n",
    "batch_size = len(first_batch['class_id'])\n",
    "\n",
    "for i in range(batch_size):\n",
    "    item_dict = {\n",
    "        'class_id': first_batch['class_id'][i].item(),\n",
    "        'bbox': [tensor[i].item() for tensor in first_batch['bbox']],\n",
    "        'conf': first_batch['conf'][i].item(),\n",
    "        'seq_id': first_batch['seq_id'][i].item(),\n",
    "        'set': first_batch['set'][i],\n",
    "        'file': first_batch['file'][i],\n",
    "        'preds': first_batch['preds'][i].item(),\n",
    "        'probs': first_batch['probs'][i].tolist()\n",
    "    }\n",
    "    reconstructed_batch.append(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch_size)\n",
    "len(reconstructed_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reconstruct_batch(batch):\n",
    "    batch_size = len(batch['class_id'])\n",
    "    reconstructed = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        bbox = [tensor[i].item() for tensor in batch['bbox']]\n",
    "        item = {\n",
    "            'class_id': batch['class_id'][i].item(),\n",
    "            'bbox': bbox,\n",
    "            'conf': batch['conf'][i].item(),\n",
    "            'seq_id': batch['seq_id'][i].item(),\n",
    "            'set': batch['set'][i],\n",
    "            'file': batch['file'][i],\n",
    "            'pred': batch['preds'][i].item(),\n",
    "            'probs': batch['probs'][i].tolist()\n",
    "        }\n",
    "        reconstructed.append(item)\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "# Usage\n",
    "reconstructed_batch = reconstruct_batch(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_id': 0,\n",
       " 'bbox': [0.0, 0.4687, 0.4111, 0.3671],\n",
       " 'conf': 0.983,\n",
       " 'seq_id': 4007156,\n",
       " 'set': 'train',\n",
       " 'file': 'IMG_6165.JPG',\n",
       " 'pred': 0,\n",
       " 'probs': [0.983549952507019,\n",
       "  0.00028202414978295565,\n",
       "  0.015514460392296314,\n",
       "  0.0006537171429954469]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000001541920938\n"
     ]
    }
   ],
   "source": [
    "total = 0.\n",
    "\n",
    "list = samples[0]['probs'].tolist()\n",
    "for i in list:\n",
    "    total += i\n",
    "print(total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_id: has type:<class 'torch.Tensor'>\n",
      "tensor(0)\n",
      "bbox: has type:<class 'torch.Tensor'>\n",
      "tensor([0.0000e+00, 4.0000e-04, 6.3900e-02, 0.0000e+00, 1.1710e-01, 4.0000e-04,\n",
      "        4.0000e-04, 0.0000e+00, 9.2200e-02, 4.4580e-01, 2.1900e-02, 1.3570e-01,\n",
      "        2.8220e-01, 1.0440e-01, 1.3470e-01, 1.3570e-01, 1.3810e-01, 1.3370e-01,\n",
      "        4.0000e-04, 2.1330e-01, 4.0000e-04, 1.4000e-03, 2.5000e-01, 4.0000e-04,\n",
      "        4.0000e-04, 0.0000e+00, 1.2150e-01, 1.5960e-01, 1.4400e-01, 4.5400e-02,\n",
      "        9.8100e-02, 4.0180e-01, 1.5180e-01, 6.1470e-01, 2.1480e-01, 2.3090e-01,\n",
      "        2.1330e-01, 5.2920e-01, 5.3660e-01, 2.1920e-01, 1.2500e-01, 6.9720e-01,\n",
      "        1.3470e-01, 9.5700e-02, 5.3270e-01, 5.3850e-01, 5.3900e-01, 5.3220e-01,\n",
      "        5.3220e-01, 5.3220e-01, 5.4730e-01, 6.7720e-01, 5.3220e-01, 5.3360e-01,\n",
      "        5.1120e-01, 5.5320e-01, 5.3560e-01, 5.3220e-01, 5.2680e-01, 5.3120e-01,\n",
      "        5.4240e-01, 5.4000e-01, 5.3120e-01, 6.7040e-01], dtype=torch.float64)\n",
      "conf: has type:<class 'torch.Tensor'>\n",
      "tensor(0.9830, dtype=torch.float64)\n",
      "seq_id: has type:<class 'torch.Tensor'>\n",
      "tensor(4007156)\n",
      "set: has type:<class 'str'>\n",
      "train\n",
      "file: has type:<class 'str'>\n",
      "IMG_6165.JPG\n",
      "preds: has type:<class 'torch.Tensor'>\n",
      "tensor(0)\n",
      "probs: has type:<class 'torch.Tensor'>\n",
      "tensor([9.9914e-01, 8.0950e-05, 7.7729e-04, 1.8287e-06])\n"
     ]
    }
   ],
   "source": [
    "keys = ['class_id', 'bbox', 'conf', 'seq_id', 'set', 'file', 'preds', 'probs']\n",
    "\n",
    "for key in keys:\n",
    "    item = first_batch[key][0]\n",
    "\n",
    "    print(f'{key}: has type:{type(item)}')\n",
    "\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(batch['sample'])        # shape [B, num_classes]\n",
    "probs  = F.softmax(logits, dim=1)\n",
    "preds = torch.argmax(probs, dim=1)    # shape [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 3, 0, 3, 3, 3, 2, 2, 2, 1, 2, 3, 2, 0, 1, 2, 0, 0, 0, 1, 2, 3, 2,\n",
       "        0, 0, 3, 2, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 0, 0, 3, 0, 1, 0, 3, 3, 2, 0, 2, 0, 2, 2, 2, 2, 0, 3, 2, 3, 0, 1,\n",
       "        0, 0, 2, 0, 2, 0, 0, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['class_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2812)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = (preds == batch['class_id']).sum()\n",
    "total = batch['class_id'].numel()\n",
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_acc = train_acc(logits, batch['class_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2812)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3109, grad_fn=<NllLossBackward0>)\n",
      "1.3109310865402222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/core/module.py:441: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "output = model.training_step(batch, batch_idx=0)\n",
    "print(output)            # should be a tensor == the loss\n",
    "print(output.item())     # the scalar loss valueprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | backbone  | ResNet           | 23.5 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.065    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      "), 'lr_scheduler': {'scheduler': <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x15543264e740>, 'monitor': 'val_loss', 'interval': 'epoch'}}\n",
      "✅ Scheduler hook-up looks good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dabe463537a41f28c2848cb0e074af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c6a9fd88b948f6b0f000960d445d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ fast_dev_run with scheduler completed without error\n"
     ]
    }
   ],
   "source": [
    "# 2) Check configure_optimizers output\n",
    "cfg = model.configure_optimizers()\n",
    "print(cfg)\n",
    "# You should see a dict of the form {'optimizer': <AdamW>, 'lr_scheduler': {…}}\n",
    "\n",
    "assert isinstance(cfg, dict)\n",
    "assert 'optimizer' in cfg\n",
    "assert 'lr_scheduler' in cfg\n",
    "print(\"✅ Scheduler hook-up looks good\")\n",
    "\n",
    "# 3) Run a single training+validation batch through Lightning\n",
    "#    to make sure the scheduler.step() call doesn’t error out.\n",
    "trainer = Trainer(\n",
    "    fast_dev_run=True,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    # devices=1, accelerator='gpu'   # add if you have a GPU\n",
    ")\n",
    "trainer.fit(model, datamodule)\n",
    "print(\"✅ fast_dev_run with scheduler completed without error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tensor shape: torch.Size([32, 3, 224, 224])\n",
      "Label tensor shape:  torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/pyt ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | backbone  | ResNet           | 23.5 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.065    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:        torch.Size([32, 4])\n",
      "configure_optimizers() returned: {'optimizer': AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      "), 'lr_scheduler': {'scheduler': <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x1554350abc40>, 'monitor': 'val_loss', 'interval': 'epoch'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/cfs/earth/scratch/kraftjul/.conda/envs/mega/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fd2481095643048701c3d30450913c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c584694345fc4dffa3fd488ae92b10ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# 2) Grab one batch from train\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "x, y = batch['sample'], batch['class_id']\n",
    "\n",
    "print(\"Sample tensor shape:\", x.shape)   # expect (32, 3, 224, 224)\n",
    "print(\"Label tensor shape: \", y.shape)   # expect (32,)\n",
    "\n",
    "# 3) Forward pass\n",
    "logits = model(x)\n",
    "print(\"Logits shape:       \", logits.shape)  # expect (32, num_classes)\n",
    "\n",
    "# 4) Optimizer/scheduler check\n",
    "opt_cfg = model.configure_optimizers()\n",
    "print(\"configure_optimizers() returned:\", opt_cfg)\n",
    "\n",
    "# 5) One‐step Lightning run\n",
    "trainer = Trainer(fast_dev_run=True, logger=False, enable_checkpointing=False)\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for this batch:\n",
      "  sample  0: apodemus_sp (true: apodemus_sp)\n",
      "  sample  1: apodemus_sp (true: apodemus_sp)\n",
      "  sample  2: soricidae (true: soricidae)\n",
      "  sample  3: cricetidae (true: cricetidae)\n",
      "  sample  4: cricetidae (true: cricetidae)\n",
      "  sample  5: mustela_erminea (true: mustela_erminea)\n",
      "  sample  6: apodemus_sp (true: apodemus_sp)\n",
      "  sample  7: apodemus_sp (true: apodemus_sp)\n",
      "  sample  8: apodemus_sp (true: apodemus_sp)\n",
      "  sample  9: apodemus_sp (true: apodemus_sp)\n",
      "  sample 10: soricidae (true: soricidae)\n",
      "  sample 11: cricetidae (true: cricetidae)\n",
      "  sample 12: apodemus_sp (true: mustela_erminea)\n",
      "  sample 13: cricetidae (true: cricetidae)\n",
      "  sample 14: cricetidae (true: cricetidae)\n",
      "  sample 15: apodemus_sp (true: apodemus_sp)\n",
      "  sample 16: cricetidae (true: cricetidae)\n",
      "  sample 17: soricidae (true: soricidae)\n",
      "  sample 18: mustela_erminea (true: mustela_erminea)\n",
      "  sample 19: apodemus_sp (true: apodemus_sp)\n",
      "  sample 20: cricetidae (true: cricetidae)\n",
      "  sample 21: apodemus_sp (true: apodemus_sp)\n",
      "  sample 22: cricetidae (true: cricetidae)\n",
      "  sample 23: apodemus_sp (true: cricetidae)\n",
      "  sample 24: cricetidae (true: cricetidae)\n",
      "  sample 25: apodemus_sp (true: apodemus_sp)\n",
      "  sample 26: apodemus_sp (true: soricidae)\n",
      "  sample 27: apodemus_sp (true: apodemus_sp)\n",
      "  sample 28: cricetidae (true: cricetidae)\n",
      "  sample 29: cricetidae (true: cricetidae)\n",
      "  sample 30: apodemus_sp (true: apodemus_sp)\n",
      "  sample 31: cricetidae (true: cricetidae)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "x, y_true = batch['sample'], batch['class_id']\n",
    "\n",
    "logits = model(x)                        # (B, num_classes)\n",
    "probs  = torch.softmax(logits, dim=1)    # (B, num_classes)\n",
    "preds  = torch.argmax(probs, dim=1)      # (B,)\n",
    "\n",
    "# map indices → names\n",
    "decoder = datamodule.get_label_decoder()\n",
    "pred_names = [decoder[int(i)] for i in preds]\n",
    "\n",
    "print(\"Predicted classes for this batch:\")\n",
    "for i, name in enumerate(pred_names):\n",
    "    print(f\"  sample {i:>2d}: {name} (true: {decoder[int(y_true[i])]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
