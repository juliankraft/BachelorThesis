{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import csv\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Sequence\n",
    "\n",
    "from os import PathLike\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from megadetector.detection.run_detector import load_detector, model_string_to_model_version\n",
    "from megadetector.detection.run_detector_batch import process_images, write_results_to_file\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from ba_dev.dataset import MammaliaData, MammaliaDataImage\n",
    "from ba_dev.transform import ImagePipeline, BatchImagePipeline\n",
    "from ba_dev.utils import load_config_yaml\n",
    "\n",
    "paths = load_config_yaml('../path_config.yml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/earth/scratch/kraftjul/BA_package/ba_dev/dataset.py:211: UserWarning: With the detection confidence of 0.25,\n",
      "8 sequences had no detections and will be excluded.\n",
      "Excluded sequences: [6000161, 6000163, 6000293, 6000530, 6000691, 6000372, 6000953, 6000186]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = paths['dataset']\n",
    "path_labelfiles = paths['test_labels']\n",
    "path_to_detector_output = paths['md_output']\n",
    "detector_model='mdv5a'\n",
    "mode='train'\n",
    "\n",
    "dataset = MammaliaDataImage(\n",
    "    path_labelfiles=path_labelfiles,\n",
    "    path_to_dataset=path_to_dataset,\n",
    "    path_to_detector_output=path_to_detector_output,\n",
    "    detector_model=detector_model,\n",
    "    mode=mode,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline = ImagePipeline(\n",
    "                path_to_dataset=path_to_dataset,\n",
    "                pre_ops = [\n",
    "                    ('to_rgb', {}),\n",
    "                    ('crop_by_bb', {})\n",
    "                ],\n",
    "                transform = v2.Compose([\n",
    "                                v2.ToImage(),\n",
    "                                v2.ToDtype(torch.float32, scale=True),\n",
    "                                v2.Resize((224, 224)),\n",
    "                                ])\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "row = dataset[77]\n",
    "\n",
    "image = pipline(row['file_path'], row['bbox'])\n",
    "\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pipline = BatchImagePipeline(\n",
    "                path_to_dataset=path_to_dataset,\n",
    "                num_workers=4,\n",
    "                pre_ops = [\n",
    "                    ('to_rgb', {}),\n",
    "                    ('crop_by_bb', {}),\n",
    "                ],\n",
    "                transform = v2.Compose([\n",
    "                                v2.ToImage(),\n",
    "                                v2.ToDtype(torch.float32, scale=True),\n",
    "                                v2.Resize((224, 224)),\n",
    "                                ])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "list_of_paths = []\n",
    "list_of_bboxes = []\n",
    "\n",
    "samples = [random.randint(0, len(dataset)) for _ in range(100)]\n",
    "\n",
    "for i in samples:\n",
    "    row = dataset[i]\n",
    "\n",
    "    list_of_paths.append(row['file_path'])\n",
    "    list_of_bboxes.append(row['bbox'])\n",
    "\n",
    "images = batch_pipline(list_of_paths, list_of_bboxes)\n",
    "\n",
    "for image in images:\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Feature Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        data = []\n",
    "        for i in range(100):\n",
    "            w = random.randint(10, 100)\n",
    "            h = random.randint(10, 100)\n",
    "            img = torch.randn(3, w, h)\n",
    "            data.append(img)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=10, num_workers=1, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([-0.0027,  0.0015,  0.0001])\n",
      "Std: tensor([0.9996, 0.9989, 1.0009])\n"
     ]
    }
   ],
   "source": [
    "channel_sum = torch.zeros(3)\n",
    "pixel_count = 0\n",
    "\n",
    "for batch in loader:\n",
    "    for img in batch:\n",
    "        pixel_count += img.shape[1] * img.shape[2]\n",
    "        for c in range(img.shape[0]):\n",
    "            channel_sum[c] += img[c].sum()\n",
    "\n",
    "mean = channel_sum / pixel_count\n",
    "\n",
    "channel_diff_squared_sum = torch.zeros(3)\n",
    "for batch in loader:\n",
    "    for img in batch:\n",
    "        img_centered_squared = (img - mean[:, None, None]) ** 2\n",
    "        for c in range(img_centered_squared.shape[0]):\n",
    "            channel_diff_squared_sum[c] += img_centered_squared[c].sum()\n",
    "\n",
    "std = torch.sqrt(channel_diff_squared_sum / pixel_count)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from os import PathLike\n",
    "\n",
    "class MammaliaDatasetFeatureStats(MammaliaDataImage):\n",
    "    def __init__(\n",
    "            self,\n",
    "            path_labelfiles: str | PathLike,\n",
    "            path_to_dataset: str | PathLike,\n",
    "            path_to_detector_output: str | PathLike,\n",
    "            detector_model: str | None = None,\n",
    "            applied_detection_confidence: float = 0.25,\n",
    "            available_detection_confidence: float = 0.25,\n",
    "            random_seed: int = 55,\n",
    "            test_size: float = 0.2,\n",
    "            n_folds: int = 5,\n",
    "            val_fold: int = 0,\n",
    "            mode: str = 'init',\n",
    "            ):\n",
    "        super().__init__(\n",
    "            path_labelfiles=path_labelfiles,\n",
    "            path_to_dataset=path_to_dataset,\n",
    "            path_to_detector_output=path_to_detector_output,\n",
    "            detector_model=detector_model,\n",
    "            applied_detection_confidence=applied_detection_confidence,\n",
    "            available_detection_confidence=available_detection_confidence,\n",
    "            random_seed=random_seed,\n",
    "            test_size=test_size,\n",
    "            n_folds=n_folds,\n",
    "            val_fold=val_fold,\n",
    "            mode=mode\n",
    "        )\n",
    "\n",
    "        self.image_pipline = ImagePipeline(\n",
    "                path_to_dataset=self.path_to_dataset,\n",
    "                pre_ops = [\n",
    "                    ('to_rgb', {}),\n",
    "                    ('crop_by_bb', {})\n",
    "                ],\n",
    "                transform = v2.Compose([\n",
    "                                v2.ToImage(),\n",
    "                                v2.ToDtype(torch.float32, scale=True),\n",
    "                                ])\n",
    "                )\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "            row_index = self.row_map[index]\n",
    "            row = self.ds.iloc[row_index]\n",
    "\n",
    "            image_path = row['file_path']\n",
    "            bbox = row['bbox']\n",
    "            return self.image_pipline(image_path, bbox)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0017, -0.0062,  0.0045]), tensor([0.9961, 0.9990, 0.9995]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(100, 3, 24, 24)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "for data in loader:\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0009, -0.0008,  0.0051]), tensor([1.0028, 1.0004, 0.9990]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(100, 3, 24, 24)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "channel_sum = 0.\n",
    "channel_squared_sum = 0.\n",
    "num_pixels = 0\n",
    "\n",
    "for data in loader:\n",
    "    # data shape: [B, C, H, W]\n",
    "    data = data.float()\n",
    "    B, C, H, W = data.shape\n",
    "    pixels = data.view(B, C, -1)\n",
    "\n",
    "    channel_sum += pixels.sum(dim=(0, 2))  # sum over B and all pixels\n",
    "    channel_squared_sum += (pixels ** 2).sum(dim=(0, 2))\n",
    "    num_pixels += B * H * W\n",
    "\n",
    "mean = channel_sum / num_pixels\n",
    "std = (channel_squared_sum / num_pixels - mean ** 2).sqrt()\n",
    "\n",
    "mean, std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
