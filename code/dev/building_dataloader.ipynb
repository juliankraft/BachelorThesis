{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import csv\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Any, TypedDict\n",
    "\n",
    "from megadetector.detection.run_detector import load_detector, model_string_to_model_version\n",
    "from megadetector.detection.run_detector_batch import process_images, write_results_to_file\n",
    "\n",
    "from os import PathLike\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path_config(path_to_config):\n",
    "    with open(path_to_config, 'r') as f:\n",
    "        path_config = yaml.safe_load(f)\n",
    "    return {k: Path(v) for k, v in path_config.items()}\n",
    "\n",
    "paths = load_path_config('/cfs/earth/scratch/kraftjul/BA/code/path_config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaDetectorRunner:\n",
    "    \"\"\"\n",
    "    A class to run the MegaDetector model on images. Designed to be used on a set of image sequences,\n",
    "    only loading the model once and running it on all sequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str | PathLike\n",
    "        Path to the MegaDetector model file. Or a string representing the model version available online.\n",
    "    confidence : float\n",
    "        Confidence threshold for the model. Default is 0.25.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_path: str | PathLike, \n",
    "            confidence: float = 0.25\n",
    "            ):\n",
    "        \n",
    "        self.model = load_detector(str(model_path))\n",
    "        self.confidence = confidence\n",
    "\n",
    "    def run_on_images(\n",
    "            self,\n",
    "            images: list[PathLike],\n",
    "            output_file_path: PathLike = None,\n",
    "            ):\n",
    "\n",
    "        results = process_images(\n",
    "            im_files=images,\n",
    "            detector=self.model,\n",
    "            confidence_threshold=self.confidence,\n",
    "            quiet=True\n",
    "        )\n",
    "\n",
    "        all_confidences = []\n",
    "\n",
    "        for r in results:\n",
    "            r[\"file\"] = r[\"file\"].name\n",
    "\n",
    "            r[\"detections\"] = [\n",
    "                det for det in r.get(\"detections\", [])\n",
    "                if det[\"category\"] == \"1\"\n",
    "            ]\n",
    "        \n",
    "            all_confidences.extend(det[\"conf\"] for det in r[\"detections\"])\n",
    "\n",
    "        all_confidences.sort(reverse=True)\n",
    "        \n",
    "        if output_file_path is not None:\n",
    "            with open(output_file_path, \"w\") as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "\n",
    "        return all_confidences      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammaliaData(Dataset):\n",
    "    \"\"\"\n",
    "    A class to load and process the Mammalia dataset. It can be uset for the initial detection of the images\n",
    "    utilizing the MegaDetector model, or for training a custom model for classification on the detected images.\n",
    "    The dataset is divided into training and testing sets based on the sequence IDs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_labelfiles : str | PathLike\n",
    "        Path to the directory containing the label files.\n",
    "    path_to_dataset : str | PathLike\n",
    "        Path to the main directory of the dataset, referenced in the labelfiles.\n",
    "    path_to_detector_output : str | PathLike\n",
    "        Path to the directory where the detector output is available for training or where the output will be saved\n",
    "        if detection is applied.\n",
    "    categories_to_drop : list[str], optional\n",
    "        By default all non-empty labels are used. To drop certain labels from the dataset, provide a list of labels to drop.\n",
    "        In detect mode, this parameter is ignored.\n",
    "    detector_model : str\n",
    "        If a detector model is provided, the detection will be applied to the whole dataset and stored for training.\n",
    "        The model must be one of the available models in the MegaDetector repository.\n",
    "        The default is None. A valid detection output must be available at the path_to_detector_output.\n",
    "    detection_confidence : float\n",
    "        The detection is done with a confidence of 0.25 by default to provide some flexibility\n",
    "        with the training. The confidence can be set to a higher value to reduce the number of detections used from\n",
    "        the output. The default is 0.25.\n",
    "    sample_length : int\n",
    "        For trainig this parameter specifies the range (1 - sample_length) of randomly seletded samples per sequence.\n",
    "        For testing this parameter specifies the maximum number of samples per sequence.\n",
    "        The default is 10.\n",
    "    sample_img_size : [int, int]\n",
    "        The size to which the detected areas are resized. The default is [224, 224].\n",
    "    mode : str\n",
    "        The mode in which the dataset is used. Can be either 'train', 'test' or 'init' defining which data will be \n",
    "        sampled and adjusting how it is sampled. The default is 'train'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            path_labelfiles: str | PathLike,\n",
    "            path_to_dataset: str | PathLike,\n",
    "            path_to_detector_output: str | PathLike,\n",
    "            categories_to_drop: list[str] = None,\n",
    "            detector_model: str = None,\n",
    "            detection_confidence: float = 0.25,\n",
    "            sample_length: int = 10,\n",
    "            sample_img_size: [int, int] = [224, 224],\n",
    "            mode: str = 'train',\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode in ['train', 'test', 'init']:\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a mode from ['train', 'test'].\")\n",
    "        \n",
    "        if detection_confidence < 0.25:\n",
    "            raise ValueError(\"Detection confidence must be at least 0.25.\")\n",
    "        \n",
    "        self.categories_to_drop = categories_to_drop if categories_to_drop is not None else []\n",
    "        self.detection_confidence = detection_confidence\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_img_size = sample_img_size\n",
    "\n",
    "        self.path_labelfiles = Path(path_labelfiles)\n",
    "        if not self.path_labelfiles.exists():\n",
    "            raise ValueError(\"The path to the label files does not exist.\")\n",
    "        \n",
    "        self.path_to_dataset = Path(path_to_dataset)\n",
    "        if not path_to_dataset.exists():\n",
    "            raise ValueError(\"The path to the dataset does not exist.\")\n",
    "\n",
    "        self.path_to_detector_output = Path(path_to_detector_output)\n",
    "        self.detector_model = detector_model\n",
    "\n",
    "        self.ds_full = self.reading_all_metadata(\n",
    "                    list_of_files = self.getting_all_files_of_type(self.path_labelfiles, file_type='.csv'),\n",
    "                    categories_to_drop = self.categories_to_drop\n",
    "                    )        \n",
    "\n",
    "        if self.mode == 'init':\n",
    "            if self.detector_model is not None:\n",
    "                self.run_detector()\n",
    "            else:\n",
    "                if not any(self.path_to_detector_output.glob(\"*.json\")):\n",
    "                    raise ValueError('A valid detection output must be available at the path_to_detector_output.')\n",
    "        \n",
    "        if self.ds_full['seq_id'].duplicated().any():\n",
    "            duplicates = self.ds_full['seq_id'][self.ds_full['seq_id'].duplicated()].unique()\n",
    "            raise ValueError(f\"Duplicate seq_id(s) found in metadata: {duplicates[:5]} ...\")\n",
    "\n",
    "        train_seq_ids, test_seq_ids = train_test_split(\n",
    "                                            self.ds_full['seq_id'],\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=55,\n",
    "                                            stratify=self.ds_full['label2']\n",
    "                                            )\n",
    "\n",
    "        filtered_train_seq_ids = self.exclude_ids_with_no_detections(\n",
    "            set_type='train',\n",
    "            sequences_to_filter=train_seq_ids\n",
    "        )\n",
    "\n",
    "        filtered_test_seq_ids = self.exclude_ids_with_no_detections(\n",
    "            set_type='test',\n",
    "            sequences_to_filter=test_seq_ids\n",
    "        )\n",
    " \n",
    "        if self.mode in ['train', 'init']:\n",
    "            active_seq_ids = filtered_train_seq_ids\n",
    "        elif self.mode == 'test':\n",
    "            active_seq_ids = filtered_test_seq_ids\n",
    "           \n",
    "        self.ds = self.ds_full[self.ds_full['seq_id'].isin(active_seq_ids)]\n",
    "        self.seq_ids = self.ds['seq_id'].tolist()\n",
    "\n",
    "    def getting_all_files_of_type(\n",
    "            self, \n",
    "            path: str | PathLike, \n",
    "            file_type: str = None, \n",
    "            get_full_path: bool = True\n",
    "            ) -> list[str]:\n",
    "        \n",
    "        path = Path(path)\n",
    "        files = []\n",
    "        for file in os.listdir(path):\n",
    "            if file_type is None or file.endswith(file_type):\n",
    "                if get_full_path:\n",
    "                    files.append(path / file)\n",
    "                else:\n",
    "                    files.append(file)\n",
    "        return files\n",
    "    \n",
    "    def reading_all_metadata(\n",
    "            self,\n",
    "            list_of_files: list[PathLike],\n",
    "            categories_to_drop: list[str] = []\n",
    "            ) -> pd.DataFrame:\n",
    "        \n",
    "        metadata = pd.DataFrame()\n",
    "        for file in list_of_files:\n",
    "            metadata = pd.concat([metadata, pd.read_csv(file)], ignore_index=True)\n",
    "            metadata = metadata.dropna(subset=['label2'])\n",
    "            metadata = metadata[~metadata['label2'].isin(categories_to_drop)]\n",
    "        return metadata\n",
    "    \n",
    "    def exclude_ids_with_no_detections(\n",
    "            self,\n",
    "            set_type: str,\n",
    "            sequences_to_filter: list[int],\n",
    "            ) -> list[int]:\n",
    "        \n",
    "        detection_summary = self.get_detection_summary(\n",
    "            usecols=[\"seq_id\", \"max_conf\"]\n",
    "            )\n",
    "        \n",
    "        seq_ids_to_exclude_set = set(detection_summary[detection_summary[\"max_conf\"] < self.detection_confidence][\"seq_id\"].tolist())\n",
    "        seq_ids_to_filter_set = set(sequences_to_filter)\n",
    "\n",
    "        excluded_seq_ids = list(seq_ids_to_filter_set & seq_ids_to_exclude_set)\n",
    "\n",
    "        if excluded_seq_ids:\n",
    "            suffix = \"\" if len(excluded_seq_ids) <= 10 else \" ...\"\n",
    "            warnings.warn(\n",
    "                f\"With the current detection confidence of {self.detection_confidence},\\n\"\n",
    "                f\"{len(excluded_seq_ids)} sequences of the {set_type} set had no detections and will be excluded.\\n\"\n",
    "                f\"Excluded sequences: {excluded_seq_ids[:10]}{suffix}\",\n",
    "                UserWarning\n",
    "            )\n",
    "        \n",
    "        return list(seq_ids_to_filter_set - seq_ids_to_exclude_set)\n",
    "\n",
    "    def get_detection_summary(\n",
    "            self,\n",
    "            usecols: list[str] = None,\n",
    "            ) -> pd.DataFrame:\n",
    "        \n",
    "        return pd.read_csv(\n",
    "                self.path_to_detector_output / \"detection_summary.csv\",\n",
    "                usecols=usecols\n",
    "                )\n",
    "    \n",
    "    def get_clss_weight(                                        # still to be implemented\n",
    "            self\n",
    "            ) -> torch.Tensor:\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            raise ValueError(\"Class weights are not available in test mode.\")\n",
    "        \n",
    "        class_weights = 5\n",
    "        \n",
    "        return class_weights\n",
    "    \n",
    "    def get_all_images_of_sequence(\n",
    "            self, \n",
    "            seq_id: int,\n",
    "            dataframe: pd.DataFrame = None,\n",
    "            )-> dict[str, PathLike]:\n",
    "        \n",
    "        if dataframe is None:\n",
    "            dataframe = self.ds_full\n",
    "\n",
    "        image_dict = {}\n",
    "        row = dataframe.loc[dataframe['seq_id'] == seq_id].squeeze()\n",
    "        seq_path = Path(row['Directory'])\n",
    "        all_files = row['all_files'].split(',')\n",
    "        for file in all_files:\n",
    "            image_dict[file] = self.path_to_dataset / seq_path / file\n",
    "        return image_dict\n",
    "\n",
    "    def run_detector(\n",
    "            self,\n",
    "            ) -> None:\n",
    "        \n",
    "        if self.detector_model is None:\n",
    "            raise ValueError('Method not available - No detector model provided.')\n",
    "        elif self.detector_model not in model_string_to_model_version.keys():\n",
    "            raise ValueError(f\"The model {self.detector_model} is not supported. Please choose from {model_string_to_model_version.keys()}.\")\n",
    "        elif not self.path_to_detector_output.exists():\n",
    "                os.makedirs(self.path_to_detector_output)\n",
    "        elif any(self.path_to_detector_output.iterdir()):\n",
    "            raise ValueError(\"The path to the detector output contains files. Please clear or choose a different path.\")\n",
    "          \n",
    "        runner = MegaDetectorRunner(\n",
    "            model_path=self.detector_model,\n",
    "            confidence=0.25\n",
    "            )\n",
    "\n",
    "        metadata = self.reading_all_metadata(\n",
    "                    list_of_files = self.getting_all_files_of_type(self.path_labelfiles, file_type='.csv'),\n",
    "                    )\n",
    "            \n",
    "        sequences = metadata['seq_id'].unique().tolist()\n",
    "\n",
    "        detection_rows = []\n",
    "\n",
    "        for seq_id in sequences:\n",
    "            seq_images = list(self.get_all_images_of_sequence(seq_id).values())\n",
    "            output_file_path = self.path_to_detector_output / f\"{seq_id}.json\"\n",
    "            detections = runner.run_on_images(\n",
    "                images=seq_images,\n",
    "                output_file_path=output_file_path\n",
    "                )\n",
    "\n",
    "            detection_row = {\n",
    "                    \"seq_id\": seq_id,\n",
    "                    \"max_conf\": max(detections) if len(detections) > 0 else 0,\n",
    "                    \"n_detections\": len(detections),\n",
    "                    \"conf_list\": json.dumps(detections)\n",
    "                }\n",
    "            \n",
    "            detection_rows.append(detection_row)\n",
    "        \n",
    "        all_detections = pd.DataFrame(detection_rows, columns=[\"seq_id\", \"max_conf\", \"n_detections\", \"conf_list\"])\n",
    "\n",
    "        all_detections.to_csv(\n",
    "            self.path_to_detector_output / \"detection_summary.csv\", \n",
    "            index=False,\n",
    "            quoting=csv.QUOTE_NONNUMERIC\n",
    "            )\n",
    "            \n",
    "    def getting_bb_list_for_seq(\n",
    "            self,\n",
    "            seq_id: int,\n",
    "            confidence: float = None,\n",
    "            ) -> list[dict]:\n",
    "        \n",
    "        if self.mode != 'detect':\n",
    "            raise ValueError(\"Only available if dataset is in detect mode.\")\n",
    "        \n",
    "        if confidence is None:\n",
    "            confidence = self.detection_confidence\n",
    "\n",
    "        path_to_detection_results = self.path_to_detector_output / f\"{seq_id}.json\"\n",
    "        with open(path_to_detection_results, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        bb_list = []\n",
    "\n",
    "        for entry in data:\n",
    "            file_name = entry['file']\n",
    "            detections = entry.get('detections', [])\n",
    "\n",
    "            for det in detections:\n",
    "                if det['category'] == \"1\" and det['conf'] >= confidence:\n",
    "                    bb_list({\n",
    "                        'file': file_name,\n",
    "                        'conf': det['conf'],\n",
    "                        'bbox': det['bbox']\n",
    "                    })\n",
    "        \n",
    "        bb_list = sorted(bb_list, key=lambda x: x['conf'], reverse=True)\n",
    "\n",
    "        return bb_list\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:               # still to be implemented\n",
    "        seq_id = self.seq_ids[index]\n",
    "\n",
    "        images = self.get_all_images_of_sequence(seq_id)\n",
    "        bounding_boxes = self.getting_bb_list_for_seq(seq_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_testset = Path('/cfs/earth/scratch/kraftjul/BA/data/test_set')\n",
    "output_path = Path('/cfs/earth/scratch/kraftjul/BA/output')\n",
    "categories_to_drop=['other', 'glis_glis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bypassing download of already-downloaded file md_v5a.0.0.pt\n",
      "Model v5a.0.0 available at /tmp/megadetector_models/md_v5a.0.0.pt\n",
      "Bypassing imports for model type yolov5\n",
      "Loading PT detector with compatibility mode classic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Fusing layers... \n",
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n",
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n",
      "/tmp/ipykernel_982648/1435634374.py:164: UserWarning: With the current detection confidence of 0.25,\n",
      "7 sequences of the train set had no detections and will be excluded.\n",
      "Excluded sequences: [6000161, 6000163, 6000293, 6000530, 6000691, 6000372, 6000953]\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_982648/1435634374.py:164: UserWarning: With the current detection confidence of 0.25,\n",
      "1 sequences of the test set had no detections and will be excluded.\n",
      "Excluded sequences: [6000186]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = paths['dataset']\n",
    "path_labelfiles = Path('/cfs/earth/scratch/kraftjul/BA/data/test_set_large')\n",
    "path_to_detector_output = path_labelfiles / 'MD_out'\n",
    "detector_model = 'mdv5a'\n",
    "mode = 'init'\n",
    "\n",
    "\n",
    "dataset = MammaliaData(\n",
    "    path_to_dataset = path_to_dataset,\n",
    "    path_labelfiles = path_labelfiles,\n",
    "    path_to_detector_output = path_to_detector_output,\n",
    "    detector_model = detector_model,\n",
    "    mode = mode,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cfs/earth/scratch/iunr/shared/iunr-mammaliabox/dataset')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "path_to_labelfiles = Path(\"/cfs/earth/scratch/iunr/shared/iunr-mammaliabox/dataset/info/labels\")\n",
    "dataset_root = Path(\"/cfs/earth/scratch/iunr/shared/iunr-mammaliabox/dataset\")\n",
    "target_dir = Path(\"/cfs/earth/scratch/kraftjul/BA/data/test_set_large\")\n",
    "output_metadata_csv = target_dir / \"metadata_larger_sample_set.csv\"\n",
    "\n",
    "\n",
    "# Load metadata\n",
    "metadata = dataset.reading_all_metadata(\n",
    "    list_of_files=dataset.getting_all_files_of_type(path_to_labelfiles, file_type='.csv'),\n",
    "    categories_to_drop=['other', 'glis_glis']\n",
    ")\n",
    "\n",
    "metadata_filtered = metadata[metadata['n_files']<60]\n",
    "\n",
    "metadata_sampled = metadata_filtered.groupby(\"label2\", group_keys=False).sample(n=40, random_state=42)\n",
    "\n",
    "metadata_sampled.to_csv(output_metadata_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>SerialNumber</th>\n",
       "      <th>seq_nr</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>Directory</th>\n",
       "      <th>DateTime_start</th>\n",
       "      <th>DateTime_end</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>first_file</th>\n",
       "      <th>last_file</th>\n",
       "      <th>n_files</th>\n",
       "      <th>all_files</th>\n",
       "      <th>label</th>\n",
       "      <th>duplicate_label</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11567</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HG09194945</td>\n",
       "      <td>233</td>\n",
       "      <td>4007156</td>\n",
       "      <td>sessions/session_04/W2-WK02</td>\n",
       "      <td>2020-06-09T23:21:46Z</td>\n",
       "      <td>2020-06-09T23:22:18Z</td>\n",
       "      <td>32.0</td>\n",
       "      <td>IMG_6154.JPG</td>\n",
       "      <td>IMG_6180.JPG</td>\n",
       "      <td>27</td>\n",
       "      <td>IMG_6154.JPG,IMG_6155.JPG,IMG_6156.JPG,IMG_615...</td>\n",
       "      <td>apodemus_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>apodemus_sp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15877</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HF07158832</td>\n",
       "      <td>180</td>\n",
       "      <td>4011466</td>\n",
       "      <td>sessions/session_04/W5-KH08</td>\n",
       "      <td>2020-06-28T22:25:32Z</td>\n",
       "      <td>2020-06-28T22:25:38Z</td>\n",
       "      <td>6.0</td>\n",
       "      <td>IMG_5446.JPG</td>\n",
       "      <td>IMG_5454.JPG</td>\n",
       "      <td>9</td>\n",
       "      <td>IMG_5446.JPG,IMG_5447.JPG,IMG_5448.JPG,IMG_544...</td>\n",
       "      <td>apodemus_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>apodemus_sp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1</td>\n",
       "      <td>H550HF08161305</td>\n",
       "      <td>229</td>\n",
       "      <td>1001887</td>\n",
       "      <td>sessions/session_01/H550HF08161305_2</td>\n",
       "      <td>2019-09-10T02:06:30Z</td>\n",
       "      <td>2019-09-10T02:07:31Z</td>\n",
       "      <td>61.0</td>\n",
       "      <td>IMG_3034.JPG</td>\n",
       "      <td>IMG_3051.JPG</td>\n",
       "      <td>18</td>\n",
       "      <td>IMG_3034.JPG,IMG_3035.JPG,IMG_3036.JPG,IMG_303...</td>\n",
       "      <td>apodemus_sp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>apodemus_sp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15095</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HF07158933</td>\n",
       "      <td>34</td>\n",
       "      <td>4010684</td>\n",
       "      <td>sessions/session_04/W4-WK02</td>\n",
       "      <td>2020-06-20T23:40:40Z</td>\n",
       "      <td>2020-06-20T23:42:00Z</td>\n",
       "      <td>80.0</td>\n",
       "      <td>IMG_0607.JPG</td>\n",
       "      <td>IMG_0654.JPG</td>\n",
       "      <td>48</td>\n",
       "      <td>IMG_0607.JPG,IMG_0608.JPG,IMG_0609.JPG,IMG_061...</td>\n",
       "      <td>apodemus_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>apodemus_sp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>77</td>\n",
       "      <td>4000175</td>\n",
       "      <td>sessions/session_04/Testwoche1/KH08</td>\n",
       "      <td>2020-05-11T21:16:28Z</td>\n",
       "      <td>2020-05-11T21:16:30Z</td>\n",
       "      <td>2.0</td>\n",
       "      <td>RCNX1125.JPG</td>\n",
       "      <td>RCNX1127.JPG</td>\n",
       "      <td>3</td>\n",
       "      <td>RCNX1125.JPG,RCNX1126.JPG,RCNX1127.JPG</td>\n",
       "      <td>apodemus_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>apodemus_sp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21906</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HF07158839</td>\n",
       "      <td>222</td>\n",
       "      <td>4017495</td>\n",
       "      <td>sessions/session_04/W7-R25</td>\n",
       "      <td>2020-07-16T22:03:52Z</td>\n",
       "      <td>2020-07-16T22:03:52Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IMG_3736.JPG</td>\n",
       "      <td>IMG_3738.JPG</td>\n",
       "      <td>3</td>\n",
       "      <td>IMG_3736.JPG,IMG_3737.JPG,IMG_3738.JPG</td>\n",
       "      <td>sorex_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>soricidae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12723</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HG09194886</td>\n",
       "      <td>174</td>\n",
       "      <td>4008312</td>\n",
       "      <td>sessions/session_04/W3-M7</td>\n",
       "      <td>2020-06-18T04:43:02Z</td>\n",
       "      <td>2020-06-18T04:43:02Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IMG_3646.JPG</td>\n",
       "      <td>IMG_3648.JPG</td>\n",
       "      <td>3</td>\n",
       "      <td>IMG_3646.JPG,IMG_3647.JPG,IMG_3648.JPG</td>\n",
       "      <td>crocidura_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>soricidae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20378</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HG09194894</td>\n",
       "      <td>161</td>\n",
       "      <td>4015967</td>\n",
       "      <td>sessions/session_04/W6-R26</td>\n",
       "      <td>2020-07-11T22:25:42Z</td>\n",
       "      <td>2020-07-11T22:25:44Z</td>\n",
       "      <td>2.0</td>\n",
       "      <td>IMG_2272.JPG</td>\n",
       "      <td>IMG_2274.JPG</td>\n",
       "      <td>3</td>\n",
       "      <td>IMG_2272.JPG,IMG_2273.JPG,IMG_2274.JPG</td>\n",
       "      <td>sorex_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>soricidae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18825</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HF07158832</td>\n",
       "      <td>147</td>\n",
       "      <td>4014414</td>\n",
       "      <td>sessions/session_04/W6-M2</td>\n",
       "      <td>2020-07-05T05:16:24Z</td>\n",
       "      <td>2020-07-05T05:16:26Z</td>\n",
       "      <td>2.0</td>\n",
       "      <td>IMG_3880.JPG</td>\n",
       "      <td>IMG_3882.JPG</td>\n",
       "      <td>3</td>\n",
       "      <td>IMG_3880.JPG,IMG_3881.JPG,IMG_3882.JPG</td>\n",
       "      <td>crocidura_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>soricidae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14607</th>\n",
       "      <td>4</td>\n",
       "      <td>H550HF07158878</td>\n",
       "      <td>14</td>\n",
       "      <td>4010196</td>\n",
       "      <td>sessions/session_04/W4-M8</td>\n",
       "      <td>2020-06-20T15:29:08Z</td>\n",
       "      <td>2020-06-20T15:29:14Z</td>\n",
       "      <td>6.0</td>\n",
       "      <td>IMG_0286.JPG</td>\n",
       "      <td>IMG_0291.JPG</td>\n",
       "      <td>6</td>\n",
       "      <td>IMG_0286.JPG,IMG_0287.JPG,IMG_0288.JPG,IMG_028...</td>\n",
       "      <td>crocidura_sp</td>\n",
       "      <td>False</td>\n",
       "      <td>soricidae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       session    SerialNumber  seq_nr   seq_id  \\\n",
       "11567        4  H550HG09194945     233  4007156   \n",
       "15877        4  H550HF07158832     180  4011466   \n",
       "1815         1  H550HF08161305     229  1001887   \n",
       "15095        4  H550HF07158933      34  4010684   \n",
       "4586         4               H      77  4000175   \n",
       "...        ...             ...     ...      ...   \n",
       "21906        4  H550HF07158839     222  4017495   \n",
       "12723        4  H550HG09194886     174  4008312   \n",
       "20378        4  H550HG09194894     161  4015967   \n",
       "18825        4  H550HF07158832     147  4014414   \n",
       "14607        4  H550HF07158878      14  4010196   \n",
       "\n",
       "                                  Directory        DateTime_start  \\\n",
       "11567           sessions/session_04/W2-WK02  2020-06-09T23:21:46Z   \n",
       "15877           sessions/session_04/W5-KH08  2020-06-28T22:25:32Z   \n",
       "1815   sessions/session_01/H550HF08161305_2  2019-09-10T02:06:30Z   \n",
       "15095           sessions/session_04/W4-WK02  2020-06-20T23:40:40Z   \n",
       "4586    sessions/session_04/Testwoche1/KH08  2020-05-11T21:16:28Z   \n",
       "...                                     ...                   ...   \n",
       "21906            sessions/session_04/W7-R25  2020-07-16T22:03:52Z   \n",
       "12723             sessions/session_04/W3-M7  2020-06-18T04:43:02Z   \n",
       "20378            sessions/session_04/W6-R26  2020-07-11T22:25:42Z   \n",
       "18825             sessions/session_04/W6-M2  2020-07-05T05:16:24Z   \n",
       "14607             sessions/session_04/W4-M8  2020-06-20T15:29:08Z   \n",
       "\n",
       "               DateTime_end  duration_seconds    first_file     last_file  \\\n",
       "11567  2020-06-09T23:22:18Z              32.0  IMG_6154.JPG  IMG_6180.JPG   \n",
       "15877  2020-06-28T22:25:38Z               6.0  IMG_5446.JPG  IMG_5454.JPG   \n",
       "1815   2019-09-10T02:07:31Z              61.0  IMG_3034.JPG  IMG_3051.JPG   \n",
       "15095  2020-06-20T23:42:00Z              80.0  IMG_0607.JPG  IMG_0654.JPG   \n",
       "4586   2020-05-11T21:16:30Z               2.0  RCNX1125.JPG  RCNX1127.JPG   \n",
       "...                     ...               ...           ...           ...   \n",
       "21906  2020-07-16T22:03:52Z               0.0  IMG_3736.JPG  IMG_3738.JPG   \n",
       "12723  2020-06-18T04:43:02Z               0.0  IMG_3646.JPG  IMG_3648.JPG   \n",
       "20378  2020-07-11T22:25:44Z               2.0  IMG_2272.JPG  IMG_2274.JPG   \n",
       "18825  2020-07-05T05:16:26Z               2.0  IMG_3880.JPG  IMG_3882.JPG   \n",
       "14607  2020-06-20T15:29:14Z               6.0  IMG_0286.JPG  IMG_0291.JPG   \n",
       "\n",
       "       n_files                                          all_files  \\\n",
       "11567       27  IMG_6154.JPG,IMG_6155.JPG,IMG_6156.JPG,IMG_615...   \n",
       "15877        9  IMG_5446.JPG,IMG_5447.JPG,IMG_5448.JPG,IMG_544...   \n",
       "1815        18  IMG_3034.JPG,IMG_3035.JPG,IMG_3036.JPG,IMG_303...   \n",
       "15095       48  IMG_0607.JPG,IMG_0608.JPG,IMG_0609.JPG,IMG_061...   \n",
       "4586         3             RCNX1125.JPG,RCNX1126.JPG,RCNX1127.JPG   \n",
       "...        ...                                                ...   \n",
       "21906        3             IMG_3736.JPG,IMG_3737.JPG,IMG_3738.JPG   \n",
       "12723        3             IMG_3646.JPG,IMG_3647.JPG,IMG_3648.JPG   \n",
       "20378        3             IMG_2272.JPG,IMG_2273.JPG,IMG_2274.JPG   \n",
       "18825        3             IMG_3880.JPG,IMG_3881.JPG,IMG_3882.JPG   \n",
       "14607        6  IMG_0286.JPG,IMG_0287.JPG,IMG_0288.JPG,IMG_028...   \n",
       "\n",
       "              label duplicate_label       label2  \n",
       "11567   apodemus_sp           False  apodemus_sp  \n",
       "15877   apodemus_sp           False  apodemus_sp  \n",
       "1815    apodemus_sp             0.0  apodemus_sp  \n",
       "15095   apodemus_sp           False  apodemus_sp  \n",
       "4586    apodemus_sp           False  apodemus_sp  \n",
       "...             ...             ...          ...  \n",
       "21906      sorex_sp           False    soricidae  \n",
       "12723  crocidura_sp           False    soricidae  \n",
       "20378      sorex_sp           False    soricidae  \n",
       "18825  crocidura_sp           False    soricidae  \n",
       "14607  crocidura_sp           False    soricidae  \n",
       "\n",
       "[160 rows x 15 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_sampled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
