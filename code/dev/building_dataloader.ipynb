{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Any, TypedDict\n",
    "\n",
    "from megadetector.detection.run_detector import load_detector, model_string_to_model_version\n",
    "from megadetector.detection.run_detector_batch import process_images, write_results_to_file\n",
    "\n",
    "from os import PathLike\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path_config(path_to_config):\n",
    "    with open(path_to_config, 'r') as f:\n",
    "        path_config = yaml.safe_load(f)\n",
    "    return {k: Path(v) for k, v in path_config.items()}\n",
    "\n",
    "paths = load_path_config('/cfs/earth/scratch/kraftjul/BA/code/path_config.yml')\n",
    "\n",
    "path_to_testset = Path('/cfs/earth/scratch/kraftjul/BA/test_set')\n",
    "output_path = Path('/cfs/earth/scratch/kraftjul/BA/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaDetectorRunner:\n",
    "    \"\"\"\n",
    "    A class to run the MegaDetector model on images. Designed to be used on a set of image sequences,\n",
    "    only loading the model once and running it on all sequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str | PathLike\n",
    "        Path to the MegaDetector model file. Or a string representing the model version available online.\n",
    "    confidence : float\n",
    "        Confidence threshold for the model. Default is 0.25.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_path: str | PathLike, \n",
    "            confidence: float = 0.25\n",
    "            ):\n",
    "        \n",
    "        self.model = load_detector(str(model_path))\n",
    "        self.confidence = confidence\n",
    "\n",
    "    def run_on_images(\n",
    "            self,\n",
    "            images: list[PathLike],\n",
    "            output_file_path: PathLike = None,\n",
    "            ):\n",
    "\n",
    "        results = process_images(\n",
    "            im_files=images,\n",
    "            detector=self.model,\n",
    "            confidence_threshold=self.confidence,\n",
    "            quiet=True\n",
    "        )\n",
    "\n",
    "        all_confidences = []\n",
    "\n",
    "        for r in results:\n",
    "            r[\"file\"] = r[\"file\"].name\n",
    "\n",
    "            r[\"detections\"] = [\n",
    "                det for det in r.get(\"detections\", [])\n",
    "                if det[\"category\"] == \"1\"\n",
    "            ]\n",
    "        \n",
    "            all_confidences.extend(det[\"conf\"] for det in r[\"detections\"])\n",
    "\n",
    "        all_confidences.sort(reverse=True)\n",
    "        \n",
    "        if output_file_path is not None:\n",
    "            with open(output_file_path, \"w\") as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "\n",
    "        return all_confidences      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammaliaData(Dataset):\n",
    "    \"\"\"\n",
    "    A class to load and process the Mammalia dataset. It can be uset for the initial detection of the images\n",
    "    utilizing the MegaDetector model, or for training a custom model for classification on the detected images.\n",
    "    The dataset is divided into training and testing sets based on the sequence IDs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_labelfiles : str | PathLike\n",
    "        Path to the directory containing the label files.\n",
    "    path_to_dataset : str | PathLike\n",
    "        Path to the main directory of the dataset, referenced in the labelfiles.\n",
    "    path_to_detector_output : str | PathLike\n",
    "        Path to the directory where the detector output is available for training or where the output will be saved\n",
    "        if detection is applied.\n",
    "    categories_to_drop : list[str], optional\n",
    "        By default all non-empty labels are used. To drop certain labels from the dataset, provide a list of labels to drop.\n",
    "        In detect mode, this parameter is ignored.\n",
    "    detector_model : str\n",
    "        If a detector model is provided, the detection will be applied to the whole dataset and stored for training.\n",
    "        The model must be one of the available models in the MegaDetector repository.\n",
    "        The default is None. A valid detection output must be available at the path_to_detector_output.\n",
    "    detection_confidence : float\n",
    "        The detection is done with a confidence of 0.25 by default to provide some flexibility\n",
    "        with the training. The confidence can be set to a higher value to reduce the number of detections used from\n",
    "        the output. The default is 0.25.\n",
    "    sample_length : int\n",
    "        For trainig this parameter specifies the range (1 - sample_length) of randomly seletded samples per sequence.\n",
    "        For testing this parameter specifies the maximum number of samples per sequence.\n",
    "        The default is 10.\n",
    "    sample_img_size : [int, int]\n",
    "        The size to which the detected areas are resized. The default is [224, 224].\n",
    "    mode : str\n",
    "        The mode in which the dataset is used. Can be either 'train', 'test' defining which data will be sampled and\n",
    "        adjusting how it is sampled. The default is 'train'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            path_labelfiles: str | PathLike,\n",
    "            path_to_dataset: str | PathLike,\n",
    "            path_to_detector_output: str | PathLike,\n",
    "            categories_to_drop: list[str] = None,\n",
    "            detector_model: str = None,\n",
    "            detection_confidence: float = 0.25,\n",
    "            sample_length: int = 10,\n",
    "            sample_img_size: [int, int] = [224, 224],\n",
    "            mode: str = 'train',\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode in ['train', 'test']:\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            raise ValueError(\"Please choose a mode from ['train', 'test'].\")\n",
    "        \n",
    "        if detection_confidence < 0.25:\n",
    "            raise ValueError(\"Detection confidence must be at least 0.25.\")\n",
    "        \n",
    "        self.categories_to_drop = categories_to_drop if categories_to_drop is not None else []\n",
    "        self.detection_confidence = detection_confidence\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_img_size = sample_img_size\n",
    "\n",
    "        self.path_labelfiles = Path(path_labelfiles)\n",
    "        if not self.path_labelfiles.exists():\n",
    "            raise ValueError(\"The path to the label files does not exist.\")\n",
    "        \n",
    "        self.path_to_dataset = Path(path_to_dataset)\n",
    "        if not path_to_dataset.exists():\n",
    "            raise ValueError(\"The path to the dataset does not exist.\")\n",
    "\n",
    "        self.path_to_detector_output = Path(path_to_detector_output)\n",
    "        self.detector_model = detector_model\n",
    "\n",
    "        if self.detector_model is not None:\n",
    "            self.run_detector()\n",
    "        else:\n",
    "            if not any(self.path_to_detector_output.glob(\"*.json\")):\n",
    "                raise ValueError('A valid detection output must be available at the path_to_detector_output.')\n",
    "        \n",
    "        self.ds_full = self.reading_all_metadata(\n",
    "                    list_of_files = self.getting_all_files_of_type(self.path_labelfiles, file_type='.csv'),\n",
    "                    categories_to_drop = self.categories_to_drop\n",
    "                    )\n",
    "        \n",
    "        if self.ds_full['seq_id'].duplicated().any():\n",
    "            duplicates = self.ds_full['seq_id'][self.ds_full['seq_id'].duplicated()].unique()\n",
    "            raise ValueError(f\"Duplicate seq_id(s) found in metadata: {duplicates[:5]} ...\")\n",
    "\n",
    "        train_seq_ids, test_seq_ids = train_test_split(\n",
    "                                            self.ds_full['seq_id'],\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=55,\n",
    "                                            stratify=self.ds_full['label2']\n",
    "                                            )\n",
    "\n",
    "        filtered_train_seq_ids = self.exclude_ids_with_no_detections(\n",
    "            set_type='train',\n",
    "            sequences_to_filter=train_seq_ids\n",
    "        )\n",
    "\n",
    "        filtered_test_seq_ids = self.exclude_ids_with_no_detections(\n",
    "            set_type='test',\n",
    "            sequences_to_filter=test_seq_ids\n",
    "        )\n",
    " \n",
    "        if self.mode == 'train':\n",
    "            active_seq_ids = filtered_train_seq_ids\n",
    "        elif self.mode == 'test':\n",
    "            active_seq_ids = filtered_test_seq_ids\n",
    "           \n",
    "        self.ds = self.ds_full[self.ds_full['seq_id'].isin(active_seq_ids)]\n",
    "        self.seq_ids = self.ds['seq_id'].tolist()\n",
    "\n",
    "    def getting_all_files_of_type(\n",
    "            self, \n",
    "            path: str | PathLike, \n",
    "            file_type: str = None, \n",
    "            get_full_path: bool = True\n",
    "            ) -> list[str]:\n",
    "        \n",
    "        path = Path(path)\n",
    "        files = []\n",
    "        for file in os.listdir(path):\n",
    "            if file_type is None or file.endswith(file_type):\n",
    "                if get_full_path:\n",
    "                    files.append(path / file)\n",
    "                else:\n",
    "                    files.append(file)\n",
    "        return files\n",
    "    \n",
    "    def reading_all_metadata(\n",
    "            self,\n",
    "            list_of_files: list[PathLike],\n",
    "            categories_to_drop: list[str] = []\n",
    "            ) -> pd.DataFrame:\n",
    "        \n",
    "        metadata = pd.DataFrame()\n",
    "        for file in list_of_files:\n",
    "            metadata = pd.concat([metadata, pd.read_csv(file)], ignore_index=True)\n",
    "            metadata = metadata.dropna(subset=['label2'])\n",
    "            metadata = metadata[~metadata['label2'].isin(categories_to_drop)]\n",
    "        return metadata\n",
    "    \n",
    "    def exclude_ids_with_no_detections(\n",
    "            self,\n",
    "            set_type: str,\n",
    "            sequences_to_filter: list[int],\n",
    "            ) -> list[int]:\n",
    "        \n",
    "        detection_summary = self.get_detection_summary(\n",
    "            usecols=[\"seq_id\", \"max_conf\"]\n",
    "            )\n",
    "        \n",
    "        seq_ids_to_exclude_set = set(detection_summary[detection_summary[\"max_conf\"] < self.detection_confidence][\"seq_id\"].tolist())\n",
    "        seq_ids_to_filter_set = set(sequences_to_filter)\n",
    "\n",
    "        excluded_seq_ids = list(seq_ids_to_filter_set & seq_ids_to_exclude_set)\n",
    "\n",
    "        if excluded_seq_ids:\n",
    "            suffix = \"\" if len(excluded_seq_ids) <= 10 else \" ...\"\n",
    "            warnings.warn(\n",
    "                f\"With the current detection confidence of {self.detection_confidence},\\n\"\n",
    "                f\"{len(excluded_seq_ids)} sequences of the {set_type} set had no detections and will be excluded.\\n\"\n",
    "                f\"Excluded sequences: {excluded_seq_ids[:10]}{suffix}\",\n",
    "                UserWarning\n",
    "            )\n",
    "        \n",
    "        return list(seq_ids_to_filter_set - seq_ids_to_exclude_set)\n",
    "\n",
    "    def get_detection_summary(\n",
    "            self,\n",
    "            usecols: list[str] = None,\n",
    "            ) -> pd.DataFrame:\n",
    "        \n",
    "        return pd.read_csv(\n",
    "                self.path_to_detector_output / \"detection_summary.csv\",\n",
    "                usecols=usecols\n",
    "                )\n",
    "    \n",
    "    def get_all_images_of_sequence(\n",
    "            self, \n",
    "            seq_id: int,\n",
    "            )-> dict[str, PathLike]:\n",
    "        image_dict = {}\n",
    "        row = self.ds_full.loc[self.ds_full['seq_id'] == seq_id].squeeze()\n",
    "        seq_path = Path(row['Directory'])\n",
    "        all_files = row['all_files'].split(',')\n",
    "        for file in all_files:\n",
    "            image_dict[file] = self.path_to_dataset / seq_path / file\n",
    "        return image_dict\n",
    "\n",
    "    def run_detector(\n",
    "            self,\n",
    "            ) -> None:\n",
    "        \n",
    "        if self.detector_model is None:\n",
    "            raise ValueError('Method not available - No detector model provided.')\n",
    "        elif self.detector_model not in model_string_to_model_version.keys():\n",
    "            raise ValueError(f\"The model {self.detector_model} is not supported. Please choose from {model_string_to_model_version.keys()}.\")\n",
    "        elif not self.path_to_detector_output.exists():\n",
    "                os.makedirs(self.path_to_detector_output)\n",
    "        elif any(self.path_to_detector_output.iterdir()):\n",
    "            raise ValueError(\"The path to the detector output contains files. Please clear or choose a different path.\")\n",
    "          \n",
    "        runner = MegaDetectorRunner(\n",
    "            model_path=self.detector_model,\n",
    "            confidence=0.25\n",
    "            )\n",
    "\n",
    "        metadata = self.ds_full = self.reading_all_metadata(\n",
    "                    list_of_files = self.getting_all_files_of_type(self.path_labelfiles, file_type='.csv'),\n",
    "                    )\n",
    "            \n",
    "        sequences = metadata['seq_id'].unique().tolist()\n",
    "\n",
    "        detection_rows = []\n",
    "\n",
    "        for seq_id in sequences:\n",
    "            seq_images = list(self.get_all_images_of_sequence(seq_id).values())\n",
    "            output_file_path = self.path_to_detector_output / f\"{seq_id}.json\"\n",
    "            detections = runner.run_on_images(\n",
    "                images=seq_images,\n",
    "                output_file_path=output_file_path\n",
    "                )\n",
    "\n",
    "            detection_row = {\n",
    "                    \"seq_id\": seq_id,\n",
    "                    \"max_conf\": max(detections) if len(detections) > 0 else 0,\n",
    "                    \"n_detections\": len(detections),\n",
    "                    \"conf_list\": json.dumps(detections)\n",
    "                }\n",
    "            \n",
    "            detection_rows.append(detection_row)\n",
    "        \n",
    "        all_detections = pd.DataFrame(detection_rows, columns=[\"seq_id\", \"max_conf\", \"n_detections\", \"conf_list\"])\n",
    "\n",
    "        all_detections.to_csv(\n",
    "            self.path_to_detector_output / \"detection_summary.csv\", \n",
    "            index=False,\n",
    "            quoting=csv.QUOTE_NONNUMERIC\n",
    "            )\n",
    "            \n",
    "    def getting_bb_list_for_seq(\n",
    "            self,\n",
    "            seq_id: int,\n",
    "            confidence: float = None,\n",
    "            ) -> list[dict]:\n",
    "        \n",
    "        if self.mode != 'detect':\n",
    "            raise ValueError(\"Only available if dataset is in detect mode.\")\n",
    "        \n",
    "        if confidence is None:\n",
    "            confidence = self.detection_confidence\n",
    "\n",
    "        path_to_detection_results = self.path_to_detector_output / f\"{seq_id}.json\"\n",
    "        with open(path_to_detection_results, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        bb_list = []\n",
    "\n",
    "        for entry in data:\n",
    "            file_name = entry['file']\n",
    "            detections = entry.get('detections', [])\n",
    "\n",
    "            for det in detections:\n",
    "                if det['category'] == \"1\" and det['conf'] >= confidence:\n",
    "                    bb_list({\n",
    "                        'file': file_name,\n",
    "                        'conf': det['conf'],\n",
    "                        'bbox': det['bbox']\n",
    "                    })\n",
    "        \n",
    "        bb_list = sorted(bb_list, key=lambda x: x['conf'], reverse=True)\n",
    "\n",
    "        return bb_list\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any: # still to be implemented\n",
    "        seq_id = self.seq_ids[index]\n",
    "\n",
    "        images = self.get_all_images_of_sequence(seq_id)\n",
    "        bounding_boxes = self.getting_bb_list_for_seq(seq_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bypassing download of already-downloaded file md_v5a.0.0.pt\n",
      "Model v5a.0.0 available at /tmp/megadetector_models/md_v5a.0.0.pt\n",
      "Bypassing imports for model type yolov5\n",
      "Loading PT detector with compatibility mode classic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n",
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "dataset = MammaliaData(\n",
    "    path_to_dataset = path_to_testset,\n",
    "    path_labelfiles = path_to_testset,\n",
    "    path_to_detector_output = output_path / 'MD_out',\n",
    "    detector_model = 'mdv5a',\n",
    "    mode = 'train',\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
