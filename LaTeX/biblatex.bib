@report{aegerterMonitoringKleinmustelidenSchlaefern2019,
  title = {Monitoring von Kleinmusteliden, Schläfern und anderen Kleinsäugern : Weiterentwicklung der Nachweismethoden mit Fotofalle},
  shorttitle = {Monitoring von Kleinmusteliden, Schläfern und anderen Kleinsäugern},
  author = {Aegerter, Silvio},
  date = {2019},
  institution = {ZHAW Zürcher Hochschule für Angewandte Wissenschaften},
  doi = {10.21256/zhaw-19209},
  url = {https://digitalcollection.zhaw.ch/handle/11475/19209},
  urldate = {2025-06-16},
  abstract = {Viele Kleinsäuger der Schweiz sind nur schwer direkt nachzuweisen, weshalb allgemein wenig über ihre Verbreitung und deren Bestandesänderungen bekannt ist. Für den indirekten Nachweis hat sich in den letzten Jahren der Gebrauch von Spurentunnel bewährt. Eine vielversprechende Alternative bieten Nachweismethoden mit Fotofallen. Um das ganze Potential dieser Methoden ausschöpfen zu können, besteht jedoch noch Optimierungsbedarf. Ziel dieser Arbeit war die Weiterentwicklung und Optimierung der Fotofallenbox «Mostela». Hauptaugenmerk lag dabei auf der Verbesserung der Handhabung, sowie der Verbesserung der Bildqualität zur zuverlässigen Bestimmung von Kleinsäugern. Bisher wurden in der Mostela lediglich Infrarotblitzfotofallen verwendet. Die Aufnahmen dieser Kameras sind monochrom. Weil für die Unterscheidung einiger Kleinsäuger die Fellfärbung ent-scheidend ist, wurde zur Erhöhung der Bildqualität erstmals Weissblitzfotofallen zur Aufnahme von Farbfotos getestet. Zur weiteren Verbesserung der Bildqualität konnte in Feldversuchen verschiedene Modifikationen zur Kontrast- und Belichtungsoptimierung erprobt werden. Durch Umbau der Weissblitzfotofallen wurde versucht, deren Fokus und Bildwinkel so anzupassen, dass sie auch in einer kleineren Bauweise eingesetzt werden können. Unter Verwendung verschiedener Baumaterialien wurde versucht, kompaktere Prototypen in besonders handlicher Bauweise anzufertigen. Durch den Austausch der Objektive gelang es, den Fokus und Bildwinkel der verwendeten Fotofallen soweit anzupassen, dass die gebauten Prototypen nur noch halb so lang sind wie die Mostela. Der am besten gelungene Prototyp kann zudem für Transport und Lagerung in seine Einzelteile zerlegt werden. Da die Prototypen nur noch einen Bruchteil der Mostela wiegen, wird die zerlegbare Kiste rucksacktauglich und kann so auch problemlos an entlegenen Orten eingesetzt werden. Durch Verwendung von Farbfotos und Anpassungen der Bauweise konnte die Bildqualität und somit die Bestimmbarkeit von Kleinsäuger erhöht werden. In den Feldversuchen zeigte sich, dass durch eine geeignete Hintergrundwahl und die Verwendung eines passenden Blitzdiffusors, die Bestimmbarkeit weiter verbessern werden kann. Um letztendlich das ganze Potential der Nachweismethode aus-schöpfen zu können, wird in Zukunft neben der Handhabung und Bildqualität auch die Effizienz der Bildauswertung verbessert werden müssen.},
  langid = {ngerman},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/BX9E8YQV/Aegerter - 2019 - Monitoring von Kleinmusteliden, Schläfern und anderen Kleinsäugern  Weiterentwicklung der Nachweism.pdf}
}

@report{bafuListeNationalPrioritaren2019,
  title = {Liste Der {{National Prioritären Arten}} Und {{Lebensräume}}. {{In}} Der {{Schweiz}} Zu Fördernde Prioritäre {{Arten}} Und {{Lebensräume}}},
  author = {{BAFU}},
  date = {2019},
  number = {Umwelt-Vollzug Nr. 1709},
  pages = {99},
  institution = {Bundesamt für Umwelt},
  location = {Bern},
  url = {https://www.bafu.admin.ch/uv-1709-d},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/KX2KL5BV/Bundesamt für Umwelt (BAFU) - 2019 - Liste der National Prioritären Arten und Lebensräume. In der Schweiz zu fördernde prioritäre Arten u.pdf}
}

@online{beeryEfficientPipelineCamera2019,
  title = {Efficient {{Pipeline}} for {{Camera Trap Image Review}}},
  author = {Beery, Sara and Morris, Dan and Yang, Siyu},
  date = {2019-07-15},
  eprint = {1907.06772},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.06772},
  url = {http://arxiv.org/abs/1907.06772},
  urldate = {2025-06-24},
  abstract = {Biologists all over the world use camera traps to monitor biodiversity and wildlife population density. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but it has proven difficult to to apply models trained in one region to images collected in different geographic areas. In some cases, accuracy falls off catastrophically in new region, due to both changes in background and the presence of previously-unseen species. We propose a pipeline that takes advantage of a pre-trained general animal detector and a smaller set of labeled images to train a classification model that can efficiently achieve accurate results in a new region.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/BAYEYFKS/Beery et al. - 2019 - Efficient Pipeline for Camera Trap Image Review.pdf;/Volumes/ExSSD/UserData/Zotero/storage/UPR24FKE/1907.html}
}

@online{beeryRecognitionTerraIncognita2018,
  title = {Recognition in {{Terra Incognita}}},
  author = {Beery, Sara and family=Horn, given=Grant, prefix=van, useprefix=false and Perona, Pietro},
  date = {2018-07-25},
  eprint = {1807.04975},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1807.04975},
  url = {http://arxiv.org/abs/1807.04975},
  urldate = {2025-03-18},
  abstract = {It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Populations and Evolution},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/7ZXRYK8G/Beery et al. - 2018 - Recognition in Terra Incognita.pdf;/Volumes/ExSSD/UserData/Zotero/storage/EPFWEW9V/1807.html}
}

@book{brondizioGlobalAssessmentReport2019,
  title = {The Global Assessment Report of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services},
  editor = {Brondízio, Eduardo Sonnewend and Settele, Josef and Díaz, Sandra and Ngo, Hien Thu},
  date = {2019},
  publisher = {{Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES)}},
  location = {Bonn},
  isbn = {978-3-947851-20-1},
  langid = {english},
  annotation = {OCLC: 1336011247},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/UK6DXMJ3/Brondízio et al. - 2019 - The global assessment report of the intergovernmen.pdf}
}

@article{cardinaleBiodiversityLossIts2012,
  title = {Biodiversity Loss and Its Impact on Humanity},
  author = {Cardinale, Bradley J. and Duffy, J. Emmett and Gonzalez, Andrew and Hooper, David U. and Perrings, Charles and Venail, Patrick and Narwani, Anita and Mace, Georgina M. and Tilman, David and Wardle, David A. and Kinzig, Ann P. and Daily, Gretchen C. and Loreau, Michel and Grace, James B. and Larigauderie, Anne and Srivastava, Diane S. and Naeem, Shahid},
  date = {2012-06},
  journaltitle = {Nature},
  volume = {486},
  number = {7401},
  pages = {59--67},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11148},
  url = {https://www.nature.com/articles/nature11148},
  urldate = {2024-04-18},
  abstract = {Two decades ago the first Earth Summit raised the question of how biological diversity loss alters ecosystem functioning and affects humanity; this Review looks at the progress made towards answering this question.},
  langid = {english},
  keywords = {Biodiversity,Ecosystem services},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZC2YLYKA/Cardinale et al. - 2012 - Biodiversity loss and its impact on humanity.pdf}
}

@inproceedings{chenDeepConvolutionalNeural2014,
  title = {Deep Convolutional Neural Network Based Species Recognition for Wild Animal Monitoring},
  booktitle = {2014 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Chen, Guobin and Han, Tony X. and He, Zhihai and Kays, Roland and Forrester, Tavis},
  date = {2014-10},
  pages = {858--862},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2014.7025172},
  url = {https://ieeexplore.ieee.org/document/7025172},
  urldate = {2025-06-19},
  abstract = {We proposed a novel deep convolutional neural network based species recognition algorithm for wild animal classification on very challenging camera-trap imagery data. The imagery data were captured with motion triggered camera trap and were segmented automatically using the state of the art graph-cut algorithm. The moving foreground is selected as the region of interests and is fed to the proposed species recognition algorithm. For the comparison purpose, we use the traditional bag of visual words model as the baseline species recognition algorithm. It is clear that the proposed deep convolutional neural network based species recognition achieves superior performance. To our best knowledge, this is the first attempt to the fully automatic computer vision based species recognition on the real camera-trap images. We also collected and annotated a standard camera-trap dataset of 20 species common in North America, which contains 14, 346 training images and 9, 530 testing images, and is available to public for evaluation and benchmark purpose.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  keywords = {Birds,deep convolutional neural networks,image classification,Image recognition,large scale learning,Sociology,Species recognition,Statistics,Visualization,wild animal monitor,Wildlife},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/HZZECAVY/Chen et al. - 2014 - Deep convolutional neural network based species recognition for wild animal monitoring.pdf}
}

@article{clucasCameraTrapMethod2025,
  title = {Camera Trap Method Effectively Identifies Small Mammal Species in Forested Habitats},
  author = {Clucas, Barbara and McCluskey, Sydney L.},
  date = {2025-05-21},
  journaltitle = {California Fish and Wildlife Journal},
  volume = {111},
  number = {2},
  issn = {2689-4203, 2689-419X},
  doi = {10.51492/cfwj.111.8},
  url = {https://journal.wildlife.ca.gov/2025/05/20/camera-trap-method-effectively-identifies-small-mammal-species-in-forested-habitats/},
  urldate = {2025-06-19},
  abstract = {Effective survey methods to detect small mammal species are often needed to develop conservation and management plans in forested ecosystems. The ability to use non-invasive methods to identify small mammal species in the field is particularly useful as live trapping can be time consuming and potentially harmful to the study species. We tested a camera trap method in a coastal redwood (Sequoia sempervirens) forest for small mammals, originally designed by Gracanin et al. (2019) and called the “selfie trap”, that uses a camera trap with a modified lens in a baited PVC tube. We determined if we could use this camera trap set-up on the ground to accurately identify small mammals to species to assess species diversity in a forested ecosystem as well as if it could withstand disturbance from larger mammals (e.g., bears). We surveyed for small mammals in areas of old-growth and second-growth coastal redwood forests in northwestern California. We detected 10 small mammal species and were able to identify most individuals to species including squirrel, chipmunk, mice, woodrat, shrew, vole and mole species. This camera trap set up also detected approximately 77\% of small mammal species known to potentially occur in the area. Moreover, although larger mammals could interact with the camera trap set up, their disturbance was limited to when they were interacting with the trap, and the bait and camera set-up remained functional for subsequent small mammal detections. Thus, this method could be used instead of live trapping in complex forested ecosystems to effectively determine small mammal species presence, diversity, and activity levels, avoiding disturbance from large mammals.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/7DTIHR2J/Clucas and McCluskey - 2025 - Camera trap method effectively identifies small mammal species in forested habitats.pdf}
}

@article{crooseAssessingDetectabilityIrish2022,
  title = {Assessing the Detectability of the {{Irish}} Stoat {{Mustela}} Erminea Hibernica Using Two Camera Trap-Based Survey Methods},
  author = {Croose, Elizabeth and Hanniffy, Ruth and Hughes, Brian and McAney, Kate and MacPherson, Jenny and Carter, Stephen P.},
  date = {2022-01},
  journaltitle = {Mammal Research},
  shortjournal = {Mamm Res},
  volume = {67},
  number = {1},
  pages = {1--8},
  issn = {2199-2401, 2199-241X},
  doi = {10.1007/s13364-021-00598-z},
  url = {https://link.springer.com/10.1007/s13364-021-00598-z},
  urldate = {2025-06-23},
  abstract = {Monitoring small mustelids like weasels Mustela nivalis and stoats M. erminea is challenging as they are rarely seen, leave scant field signs and display avoidance behaviour towards traps and monitoring devices. The Irish stoat M. erminea hibernica is a subspecies endemic to Ireland and the Isle of Man, and despite being widespread in Ireland, no information exists on its population status due to the difficulty of detection. We compared the efficacy of two camera trap methods to detect Irish stoats in counties Mayo and Galway, Republic of Ireland. Firstly, the ‘Mostela’ (a modified camera trapping device comprising a camera trap and a tracking tunnel inside a wooden box) and secondly, an external camera trap deployed outside the box. We used a single-season occupancy model to estimate the probability of detection and occupancy of Irish stoat using these two methods at 12 sites. Both methods detected stoats, at 17\% of sites inside the Mostela and 33\% of sites on the external camera, although this non-agreement was not statistically significant. Detection probabilities were low, with wide and largely overlapping confidence intervals for both methods. Occupancy probabilities were relatively low, and the occupancy probability for the external camera was very close to the naïve occupancy estimate. We evaluate the potential applicability of both methods for future work to assess the population and conservation status of this little-studied species.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/TQ6RTSH7/Croose et al. - 2022 - Assessing the detectability of the Irish stoat Mustela erminea hibernica using two camera trap-based.pdf}
}

@article{delisleNextGenerationCameraTrapping2021,
  title = {Next-{{Generation Camera Trapping}}: {{Systematic Review}} of {{Historic Trends Suggests Keys}} to {{Expanded Research Applications}} in {{Ecology}} and {{Conservation}}},
  shorttitle = {Next-{{Generation Camera Trapping}}},
  author = {Delisle, Zackary J. and Flaherty, Elizabeth A. and Nobbe, Mackenzie R. and Wzientek, Cole M. and Swihart, Robert K.},
  date = {2021-02-26},
  journaltitle = {Frontiers in Ecology and Evolution},
  shortjournal = {Front. Ecol. Evol.},
  volume = {9},
  publisher = {Frontiers},
  issn = {2296-701X},
  doi = {10.3389/fevo.2021.617996},
  url = {https://www.frontiersin.org/journals/ecology-and-evolution/articles/10.3389/fevo.2021.617996/full},
  urldate = {2025-06-19},
  abstract = {Camera trapping is an effective noninvasive method for collecting data on wildlife species to address questions of ecological and conservation interest. We reviewed 2,167 camera trap (CT) articles from 1994 to 2020. Through the lens of technological diffusion, we assessed trends in: (1) CT adoption measured by published research output, (2) topic, taxonomic, and geographic diversification and composition of CT applications, and (3) sampling effort, spatial extent, and temporal duration of CT studies. Annual publications of CT articles have grown 81-fold since 1994, increasing at a rate of 1.26 (SE = 0.068) per year since 2005, but with decelerating growth since 2017. Topic, taxonomic, and geographic richness of CT studies increased to encompass 100\% of topics, 59.4\% of ecoregions, and 6.4\% of terrestrial vertebrates. However, declines in per article rates of accretion and plateaus in Shannon’s H for topics and major taxa studied suggest upper limits to further diversification of CT research as currently practiced. Notable compositional changes of topics included a decrease in capture-recapture, recent decrease in spatial-capture-recapture, and increases in occupancy, interspecific interactions, and automated image classification. Mammals were the dominant taxon studied; within mammalian orders carnivores exhibited a unimodal peak whereas primates, rodents and lagomorphs steadily increased. Among biogeographic realms we observed decreases in Oceania and Nearctic, increases in Afrotropic and Palearctic, and unimodal peaks for Indomalayan and Neotropic. Camera days, temporal extent, and area sampled increased, with much greater rates for the 0.90 quantile of CT studies compared to the median. Next-generation CT studies are poised to expand knowledge valuable to wildlife ecology and conservation by posing previously infeasible questions at unprecedented spatiotemporal scales, on a greater array of species, and in a wider variety of environments. Converting potential into broad-based application will require transferable models of automated image classification, and data sharing among users across multiple platforms in a coordinated manner. Further taxonomic diversification likely will require technological modifications that permit more efficient sampling of smaller species and adoption of recent improvements in modeling of unmarked populations. Environmental diversification can benefit from engineering solutions that expand ease of CT sampling in traditionally challenging sites.},
  langid = {english},
  keywords = {Camera trap,diversity,ecoregions,image classification,Occupancy,Population attributes,Technological diffusion,wildlife},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/W8RLHMQY/Delisle et al. - 2021 - Next-Generation Camera Trapping Systematic Review of Historic Trends Suggests Keys to Expanded Rese.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2025-06-02},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/J72A7EIB/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf}
}

@article{doanWildlifeSpeciesClassification2024,
  title = {Wildlife {{Species Classification}} from {{Camera Trap Images Using Fine-tuning EfficientNetV2}}},
  author = {Doan, Thanh-Nghi and Le-Thi, Duc-Ngoc},
  date = {2024-12-31},
  journaltitle = {International Journal of Intelligent Engineering and Systems},
  shortjournal = {IJIES},
  volume = {17},
  number = {6},
  pages = {624--638},
  issn = {21853118},
  doi = {10.22266/ijies2024.1231.48},
  url = {https://inass.org/wp-content/uploads/2024/08/2024123148-3.pdf},
  urldate = {2025-06-23},
  abstract = {Camera traps are a valuable tool for wildlife research and conservation, but wildlife species classification in camera trap imagery is challenging due to the variation in species appearance, pose, and lighting conditions. This study explores the use of transfer learning and fine-tuning to develop a robust deep convolutional neural network model for wildlife species classification from camera trap images. To prevent overfitting, data augmentation techniques were applied during the image pre-processing stage. ResNet-50 and various EfficientNetV2 variants have been evaluated, and the EfficientNetV2-L model emerged as the top performer. Fine-tuning methods were then applied to the EfficientNetV2-L model to further improve its performance. Experimental results show that the fine-tuned EfficientNetV2-L model outperformed other methods with an accuracy of 88.822\%, a precision of 86.941\%, a recall of 87.638\%, and an F1-score of 87.193\% on a held-out test set, demonstrating its effectiveness for wildlife species classification from camera trap images.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/BXHKAI8K/2024 - Wildlife Species Classification from Camera Trap Images Using Fine-tuning EfficientNetV2.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2025-06-02},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/DB6DBZ2X/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/Volumes/ExSSD/UserData/Zotero/storage/N2APVDTF/2010.html}
}

@article{gomezvillaAutomaticWildAnimal2017,
  title = {Towards Automatic Wild Animal Monitoring: {{Identification}} of Animal Species in Camera-Trap Images Using Very Deep Convolutional Neural Networks},
  shorttitle = {Towards Automatic Wild Animal Monitoring},
  author = {Gomez Villa, Alexander and Salazar, Augusto and Vargas, Francisco},
  date = {2017-09-01},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {41},
  pages = {24--32},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2017.07.004},
  url = {https://www.sciencedirect.com/science/article/pii/S1574954116302047},
  urldate = {2025-06-19},
  abstract = {Non-intrusive monitoring of animals in the wild is possible using camera trapping networks. The cameras are triggered by sensors in order to disturb the animals as little as possible. This approach produces a high volume of data (in the order of thousands or millions of images) that demands laborious work to analyze both useless (incorrect detections, which are the most) and useful (images with presence of animals). In this work, we show that as soon as some obstacles are overcome, deep neural networks can cope with the problem of the automated species classification appropriately. As case of study, the most common 26 of 48 species from the Snapshot Serengeti (SSe) dataset were selected and the potential of the Very Deep Convolutional neural networks framework for the species identification task was analyzed. In the worst-case scenario (unbalanced training dataset containing empty images) the method reached 35.4\% Top-1 and 60.4\% Top-5 accuracy. For the best scenario (balanced dataset, images containing foreground animals only, and manually segmented) the accuracy reached a 88.9\% Top-1 and 98.1\% Top-5, respectively. To the best of our knowledge, this is the first published attempt on solving the automatic species recognition on the SSe dataset. In addition, a comparison with other approaches on a different dataset was carried out, showing that the architectures used in this work outperformed previous approaches. The limitations of the method, drawbacks, as well as new challenges in automatic camera-trap species classification are widely discussed.},
  keywords = {Animal species recognition,Camera-trap,Deep convolutional neural networks,Snapshot Serengeti},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/GJG7MRSM/Gomez Villa et al. - 2017 - Towards automatic wild animal monitoring Identification of animal species in camera-trap images usi.pdf;/Volumes/ExSSD/UserData/Zotero/storage/TAYW5636/Gomez Villa et al. - 2017 - Towards automatic wild animal monitoring Identification of animal species in camera-trap images usi.pdf;/Volumes/ExSSD/UserData/Zotero/storage/F67GMQXC/S1574954116302047.html}
}

@article{grafWildlifeCampusKleineSaeugetiere2022,
  title = {Wildlife@Campus: Kleine Säugetiere im Fokus},
  author = {Graf, Roland F. and Dietrich, Adrian and Honetschläger, Nils and Kryszczuk, Krzysztof and Palmisano, Marilena and Pothier, Joël F. and Ratnaweera, Nils and Reifler-Bächtiger, Martina and Rhyner, Nicola and Treichler, Regula},
  date = {2022-07},
  langid = {ngerman},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/MXGX9DBR/Graf - Wildlife@Campus Kleine Säugetiere im Fokus.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2025-06-02},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/GUHR8ZH8/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@online{hendrycksBaselineDetectingMisclassified2018,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2018-10-03},
  eprint = {1610.02136},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1610.02136},
  url = {http://arxiv.org/abs/1610.02136},
  urldate = {2025-06-24},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/FCMI9SCC/Hendrycks and Gimpel - 2018 - A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks.pdf;/Volumes/ExSSD/UserData/Zotero/storage/QY6U4HV8/1610.html}
}

@online{hernandezPytorchWildlifeCollaborativeDeep2024,
  title = {Pytorch-{{Wildlife}}: {{A Collaborative Deep Learning Framework}} for {{Conservation}}},
  shorttitle = {Pytorch-{{Wildlife}}},
  author = {Hernandez, Andres and Miao, Zhongqi and Vargas, Luisa and Beery, Sara and Dodhia, Rahul and Arbelaez, Pablo and Ferres, Juan M. Lavista},
  date = {2024-11-29},
  eprint = {2405.12930},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.12930},
  url = {http://arxiv.org/abs/2405.12930},
  urldate = {2025-02-04},
  abstract = {The alarming decline in global biodiversity, driven by various factors, underscores the urgent need for large-scale wildlife monitoring. In response, scientists have turned to automated deep learning methods for data processing in wildlife monitoring. However, applying these advanced methods in real-world scenarios is challenging due to their complexity and the need for specialized knowledge, primarily because of technical challenges and interdisciplinary barriers. To address these challenges, we introduce Pytorch-Wildlife, an open-source deep learning platform built on PyTorch. It is designed for creating, modifying, and sharing powerful AI models. This platform emphasizes usability and accessibility, making it accessible to individuals with limited or no technical background. It also offers a modular codebase to simplify feature expansion and further development. Pytorch-Wildlife offers an intuitive, user-friendly interface, accessible through local installation or Hugging Face, for animal detection and classification in images and videos. As two real-world applications, Pytorch-Wildlife has been utilized to train animal classification models for species recognition in the Amazon Rainforest and for invasive opossum recognition in the Galapagos Islands. The Opossum model achieves 98\% accuracy, and the Amazon model has 92\% recognition accuracy for 36 animals in 90\% of the data. As Pytorch-Wildlife evolves, we aim to integrate more conservation tasks, addressing various environmental challenges. Pytorch-Wildlife is available at https://github.com/microsoft/CameraTraps.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/5VB3HY3J/Hernandez et al. - 2024 - Pytorch-Wildlife A Collaborative Deep Learning Fr.pdf;/Volumes/ExSSD/UserData/Zotero/storage/WZXRFIKM/2405.html}
}

@article{hopkinsDetectingMonitoringRodents2024,
  title = {Detecting and Monitoring Rodents Using Camera Traps and Machine Learning versus Live Trapping for Occupancy Modeling},
  author = {Hopkins, Jaran and Santos-Elizondo, Gabriel Marcelo and Villablanca, Francis},
  date = {2024-05-22},
  journaltitle = {Frontiers in Ecology and Evolution},
  shortjournal = {Front. Ecol. Evol.},
  volume = {12},
  publisher = {Frontiers},
  issn = {2296-701X},
  doi = {10.3389/fevo.2024.1359201},
  url = {https://www.frontiersin.org/journals/ecology-and-evolution/articles/10.3389/fevo.2024.1359201/full},
  urldate = {2025-06-19},
  abstract = {Determining best methods to detect individuals and monitor populations that balance effort and efficiency can assist conservation and land management. This may be especially true for small, noncharismatic species, such as rodents (Rodentia), which comprise 39\% of all mammal species. Given the importance of rodents to ecosystems, and the number of listed species, we tested two commonly used detection and monitoring methods, live traps and camera traps, to determine their efficiency in rodents. An artificial-intelligence machine-learning model was developed to process the camera trap images and identify the species within them which reduced camera trapping effort. We used occupancy models to compare probability of detection and occupancy estimates for six rodent species across the two methods. Camera traps yielded greater detection probability and occupancy estimates for all six species. Live trapping yielded biasedly low estimates of occupancy, required greater effort, and had a lower probability of detection. Camera traps, aimed at the ground to capture the dorsal view of an individual, combined with machine learning provided a practical, noninvasive, and low effort solution to detecting and monitoring rodents. Thus, camera trapping with machine learning is a more sustainable and practical solution for the conservation and land management of rodents.},
  langid = {english},
  keywords = {camera trapping5,detection1,effort4,live trapping6,Machine Learning3,occupancy2,small mammel},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/I94MJNNI/Hopkins et al. - 2024 - Detecting and monitoring rodents using camera traps and machine learning versus live trapping for oc.pdf}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  date = {2017-07},
  pages = {2261--2269},
  publisher = {IEEE},
  location = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.243},
  url = {https://ieeexplore.ieee.org/document/8099726/},
  urldate = {2025-06-02},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/E9WTWW4Z/Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf}
}

@online{iucnIUCNRedList2025,
  title = {The {{IUCN Red List}} of {{Threatened Species}}},
  author = {{IUCN}},
  date = {2025},
  url = {https://www.iucnredlist.org},
  urldate = {2025-06-23},
  abstract = {Established in 1964, the IUCN Red List of Threatened Species has evolved to become the world’s most comprehensive information source on the global conservation status of animal, fungi and plant species.},
  organization = {The IUCN Red List of Threatened Species},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/GKX3HEAN/en.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2025-06-19},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/4BZGALZU/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  url = {https://ieeexplore.ieee.org/document/726791},
  urldate = {2025-06-19},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/QAWV7JZB/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf}
}

@article{leornaHumanVsMachine2022,
  title = {Human vs. Machine: {{Detecting}} Wildlife in Camera Trap Images},
  shorttitle = {Human vs. Machine},
  author = {Leorna, Scott and Brinkman, Todd},
  date = {2022-12-01},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {72},
  pages = {101876},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2022.101876},
  url = {https://www.sciencedirect.com/science/article/pii/S1574954122003260},
  urldate = {2025-06-23},
  abstract = {As the capacity to collect and store large amounts of data expands, identifying and evaluating strategies to efficiently convert raw data into meaningful information is increasingly necessary. Across disciplines, this data processing task has become a significant challenge, delaying progress and actionable insights. In ecology, the growing use of camera traps (i.e., remotely triggered cameras) to collect information on wildlife has led to an enormous volume of raw data (i.e., images) in need of review and annotation. To expedite camera trap image processing, many have turned to the field of artificial intelligence (AI) and use machine learning models to automate tasks such as detecting and classifying wildlife in images. To contribute understanding of the utility of AI tools for processing wildlife camera trap images, we evaluated the performance of a state-of-the-art computer vision model developed by Microsoft AI for Earth named MegaDetector using data from an ongoing camera trap study in Arctic Alaska, USA. Compared to image labels determined by manual human review, we found MegaDetector reliably determined the presence or absence of wildlife in images generated by motion detection camera settings (≥94.6\% accuracy), however, performance was substantially poorer for images collected with time-lapse camera settings (≤61.6\% accuracy). By examining time-lapse images where MegaDetector failed to detect wildlife, we gained practical insights into animal size and distance detection limits and discuss how those may impact the performance of MegaDetector in other systems. We anticipate our findings will stimulate critical thinking about the tradeoffs of using automated AI tools or manual human review to process camera trap images and help to inform effective implementation of study designs.},
  keywords = {Confusion matrix,Deep learning,Image analysis,Software,Trail camera},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/948YGNWZ/Leorna and Brinkman - 2022 - Human vs. machine Detecting wildlife in camera trap images.pdf;/Volumes/ExSSD/UserData/Zotero/storage/7EMAUSIB/S1574954122003260.html}
}

@article{littlewoodUseNovelCamera2021,
  title = {Use of a Novel Camera Trapping Approach to Measure Small Mammal Responses to Peatland Restoration},
  author = {Littlewood, Nick A. and Hancock, Mark H. and Newey, Scott and Shackelford, Gorm and Toney, Rose},
  date = {2021-01-12},
  journaltitle = {European Journal of Wildlife Research},
  shortjournal = {Eur J Wildl Res},
  volume = {67},
  number = {1},
  pages = {12},
  issn = {1439-0574},
  doi = {10.1007/s10344-020-01449-z},
  url = {https://doi.org/10.1007/s10344-020-01449-z},
  urldate = {2025-06-19},
  abstract = {Small mammals, such as small rodents (Rodentia: Muroidea) and shrews (Insectivora: Soricidae), present particular challenges in camera trap surveys. Their size is often insufficient to trigger infra-red sensors, whilst resultant images may be of inadequate quality for species identification. The conventional survey method for small mammals, live-trapping, can be both labour-intensive and detrimental to animal welfare. Here, we describe a method for using camera traps for monitoring small mammals. We show that by attaching the camera trap to a baited tunnel, fixing a close-focus lens over the camera trap lens, and reducing the flash intensity, pictures or videos can be obtained of sufficient quality for identifying species. We demonstrate the use of the method by comparing occurrences of small mammals in a peatland landscape containing (i) plantation forestry (planted on drained former blanket bog), (ii) ex-forestry areas undergoing bog restoration, and (iii) unmodified blanket bog habitat. Rodents were detected only in forestry and restoration areas, whilst shrews were detected across all habitat. The odds of detecting small mammals were 7.6 times higher on camera traps set in plantation forestry than in unmodified bog, and 3.7 times higher on camera traps in restoration areas than in bog. When absolute abundance estimates are not required, and camera traps are available, this technique provides a low-cost survey method that is labour-efficient and has minimal animal welfare implications.},
  langid = {english},
  keywords = {Animal Geography,Blanket bog,Ethology,Mammalogy,Photography,Plantation forestry,Primatology,Restoration Ecology,Rodent,Shrew,Trail camera,Vole},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/AJHL3UJ9/Littlewood et al. - 2021 - Use of a novel camera trapping approach to measure small mammal responses to peatland restoration.pdf}
}

@online{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-03-25},
  url = {https://arxiv.org/abs/2103.14030v2},
  urldate = {2025-06-19},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  langid = {english},
  organization = {arXiv.org},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/EM7E8938/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2025-06-01},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/Q5ZUFSHE/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Volumes/ExSSD/UserData/Zotero/storage/245STX57/1711.html}
}

@online{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2017-05-03},
  eprint = {1608.03983},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.03983},
  url = {http://arxiv.org/abs/1608.03983},
  urldate = {2025-06-23},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/A63UJCRK/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restarts.pdf;/Volumes/ExSSD/UserData/Zotero/storage/TGEQMISP/1608.html}
}

@inproceedings{muhammadTemporalSwinFPNNetNovel2024,
  title = {{{TemporalSwin-FPN Net}}: {{A Novel Pipeline}} for {{Metadata-Driven Sequence Classification}} in {{Camera Trap Imagery}}},
  shorttitle = {{{TemporalSwin-FPN Net}}},
  booktitle = {2024 {{International Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  author = {Muhammad, Sameeruddin and Xiang, Wei and Mann, Scott and Han, Kang and Nair, Supriya},
  date = {2024-11},
  pages = {616--623},
  doi = {10.1109/DICTA63115.2024.00094},
  url = {https://ieeexplore.ieee.org/abstract/document/10869574},
  urldate = {2025-03-21},
  abstract = {In wildlife conservation, camera traps generate a significant volume of data that requires extensive manual analysis to extract insights, particularly for species identification. Existing tools and methods for analysing camera trap data typically classify images individually, while the devices are configured to capture multiple pictures in burst mode, often accompanied by metadata. In this paper, we introduce a novel metadata-driven pipeline based on the enhanced Swin Transformer architecture, named TemporalSwin-FPN Net, which utilises metadata to improve classification through a sequential analysis pipeline. This approach shifts the focus from analysing individual snapshots to sequences of images, thereby enriching the architecture with more comprehensive data for species classification. TemporalSwin-FPN Net has achieved a remarkable accuracy of 99.7\% on the Australian camera trap dataset, and consistently outperformed the baseline models with 94.89\% and 93.32\% on the Snapshot Safari dataset for Camdeboo and Mountain Zebra projects, surpassing them by 9.62\% and 8.54\%. Moreover, integrating the TemporalSwin-FPN Net into our proposed pipeline has significantly reduced the end-to-end processing time for a test dataset of 12,922 images, from 10 minutes 54 seconds to just 6 minutes 41 seconds, thereby substantially enhancing data processing efficiency.},
  eventtitle = {2024 {{International Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  keywords = {Accuracy,camera traps,Cameras,deep learning,image classification,Metadata,Monitoring,Pipelines,Reliability,sequence,sequence-based image classification,Sequential analysis,Stakeholders,swin transformer,Transformers,Wildlife},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/28FMXUG4/Muhammad et al. - 2024 - TemporalSwin-FPN Net A Novel Pipeline for Metadata-Driven Sequence Classification in Camera Trap Im.pdf;/Volumes/ExSSD/UserData/Zotero/storage/G7E4YXEC/10869574.html}
}

@article{norouzzadehAutomaticallyIdentifyingCounting2018,
  title = {Automatically Identifying, Counting, and Describing Wild Animals in Camera-Trap Images with Deep Learning},
  author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
  date = {2018-06-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {25},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1719367115},
  url = {https://pnas.org/doi/full/10.1073/pnas.1719367115},
  urldate = {2025-03-12},
  abstract = {Significance             Motion-sensor cameras in natural habitats offer the opportunity to inexpensively and unobtrusively gather vast amounts of data on animals in the wild. A key obstacle to harnessing their potential is the great cost of having humans analyze each image. Here, we demonstrate that a cutting-edge type of artificial intelligence called deep neural networks can automatically extract such invaluable information. For example, we show deep learning can automate animal identification for 99.3\% of the 3.2 million-image Snapshot Serengeti dataset while performing at the same 96.6\% accuracy of crowdsourced teams of human volunteers. Automatically, accurately, and inexpensively collecting such data could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences.           ,              Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {$>$}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {$>$}8.4 y (i.e., {$>$}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/AFBKWGRC/Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describing wild animals in camera-trap images with deep lea.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, Fabian and Pedregosa, Fabian and Varoquaux, Gael and Varoquaux, Gael and Org, Normalesup and Gramfort, Alexandre and Gramfort, Alexandre and Michel, Vincent and Michel, Vincent and Fr, Logilab and Thirion, Bertrand and Thirion, Bertrand and Grisel, Olivier and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Tp, Alexandre and Cournapeau, David},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/JGKDP9KT/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf}
}

@inproceedings{rameshExploringGeneralizabilityTransfer2025,
  title = {Exploring the~{{Generalizability}} of~{{Transfer Learning}} for~{{Camera Trap Animal Image Classification}}},
  booktitle = {Machine {{Learning}} and {{Principles}} and {{Practice}} of {{Knowledge Discovery}} in {{Databases}}},
  author = {Ramesh, Keshav and Darwish, Mahmoud and Zibli, Ahmed Sharafath Ahamed and Miller, Nikita Christ and Sajun, Ali Reza and Zualkernan, Imran and Habib, Altaf and Gardner, Andrew},
  editor = {Meo, Rosa and Silvestri, Fabrizio},
  date = {2025},
  pages = {212--227},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-74627-7_15},
  abstract = {Animal extinction and biodiversity loss are critical issues impacting our planet today. This paper presents an interdisciplinary approach that merges the power of deep learning with the urgent necessity of wildlife conservation to aid ecologists in their fight against animal extinction. Utilizing a labeled and unlabeled dataset obtained from camera trap images from the Hajar Mountains of the United Arab Emirates, we trained and evaluated six different pre-trained deep learning models to assess how well they can be applied in real-world scenarios. Our analysis yielded promising results in accuracy and generalizability, with Adjusted Rand Index (ARI) scores comparing model predictions exceeding 0.6 for all models. Despite the substantial challenge presented by the imbalanced nature of our dataset, our models successfully classified a wide array of species, with our best model (DenseNet121) averaging a weighted accuracy of 72.78\% with an F1 score of 0.957. The robustness of our models was further validated by a t-SNE analysis, which revealed coherent clustering of high-dimensional data. We developed a Graphical User Interface (GUI) application to bring our technology to non-technical users, allowing ecologists to classify images easily and leverage the power of AI in their conservation efforts. This study is a stride forward in leveraging artificial intelligence to aid ecological conservation, demonstrating the potential for machine learning to provide practical, effective solutions in real-world scenarios.},
  isbn = {978-3-031-74627-7},
  langid = {english},
  keywords = {Generalizability,Image Classification,Imbalanced Data,Wildlife Conservation},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/MIU4WKNG/Ramesh et al. - 2025 - Exploring the Generalizability of Transfer Learning for Camera Trap Animal Image Classification.pdf}
}

@online{ratnaweeraWildlifeCampusProgressReports2021,
  title = {Wildlife @ {{Campus}}: {{ProgressReports}}},
  shorttitle = {Wildlife @ {{Campus}}},
  author = {Ratnaweera, Nils},
  date = {2021},
  url = {https://github.zhaw.ch/pages/Wildlife-Campus/ProgressReports/},
  urldate = {2025-06-03},
  organization = {Wildlife @ Campus: ProgressReports},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/HUTHINIH/ProgressReports.html}
}

@article{schneiderRecognitionEuropeanMammals2024,
  title = {Recognition of {{European}} Mammals and Birds in Camera Trap Images Using Deep Neural Networks},
  author = {Schneider, Daniel and Lindner, Kim and Vogelbacher, Markus and Bellafkir, Hicham and Farwig, Nina and Freisleben, Bernd},
  date = {2024},
  journaltitle = {IET Computer Vision},
  volume = {18},
  number = {8},
  pages = {1162--1192},
  issn = {1751-9640},
  doi = {10.1049/cvi2.12294},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cvi2.12294},
  urldate = {2025-03-16},
  abstract = {Most machine learning methods for animal recognition in camera trap images are limited to mammal identification and group birds into a single class. Machine learning methods for visually discriminating birds, in turn, cannot discriminate between mammals and are not designed for camera trap images. The authors present deep neural network models to recognise both mammals and bird species in camera trap images. They train neural network models for species classification as well as for predicting the animal taxonomy, that is, genus, family, order, group, and class names. Different neural network architectures, including ResNet, EfficientNetV2, Vision Transformer, Swin Transformer, and ConvNeXt, are compared for these tasks. Furthermore, the authors investigate approaches to overcome various challenges associated with camera trap image analysis. The authors’ best species classification models achieve a mean average precision (mAP) of 97.91\% on a validation data set and mAPs of 90.39\% and 82.77\% on test data sets recorded in forests in Germany and Poland, respectively. Their best taxonomic classification models reach a validation mAP of 97.18\% and mAPs of 94.23\% and 79.92\% on the two test data sets, respectively.},
  langid = {english},
  keywords = {computer vision,convolutional neural nets,image classification,image recognition,neural nets},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/4VLADYG6/Schneider et al. - 2024 - Recognition of European mammals and birds in camera trap images using deep neural networks.pdf}
}

@article{shortenSurveyImageData2019,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  date = {2019-07-06},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  url = {https://doi.org/10.1186/s40537-019-0197-0},
  urldate = {2025-03-25},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZEEC95RE/Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learning.pdf}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2025-06-19},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/K7LL92RV/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/Volumes/ExSSD/UserData/Zotero/storage/K9X6NTGM/1409.html}
}

@article{stancicClassificationEfficiencyPreTrained2022,
  title = {Classification {{Efficiency}} of {{Pre-Trained Deep CNN Models}} on {{Camera Trap Images}}},
  author = {Stančić, Adam and Vyroubal, Vedran and Slijepčević, Vedran},
  date = {2022-02},
  journaltitle = {Journal of Imaging},
  volume = {8},
  number = {2},
  pages = {20},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-433X},
  doi = {10.3390/jimaging8020020},
  url = {https://www.mdpi.com/2313-433X/8/2/20},
  urldate = {2025-06-23},
  abstract = {This paper presents the evaluation of 36 convolutional neural network (CNN) models, which were trained on the same dataset (ImageNet). The aim of this research was to evaluate the performance of pre-trained models on the binary classification of images in a “real-world” application. The classification of wildlife images was the use case, in particular, those of the Eurasian lynx (lat. “Lynx lynx”), which were collected by camera traps in various locations in Croatia. The collected images varied greatly in terms of image quality, while the dataset itself was highly imbalanced in terms of the percentage of images that depicted lynxes.},
  issue = {2},
  langid = {english},
  keywords = {camera trap,classification,CNN,efficiency,pre-trained},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/KDPT4P63/Stančić et al. - 2022 - Classification Efficiency of Pre-Trained Deep CNN Models on Camera Trap Images.pdf}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2015-06},
  pages = {1--9},
  publisher = {IEEE},
  location = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298594},
  url = {http://ieeexplore.ieee.org/document/7298594/},
  urldate = {2025-06-19},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/6WPXTUVZ/Szegedy et al. - 2015 - Going deeper with convolutions.pdf}
}

@article{tabakMachineLearningClassify2019,
  title = {Machine Learning to Classify Animal Species in Camera Trap Images: {{Applications}} in Ecology},
  shorttitle = {Machine Learning to Classify Animal Species in Camera Trap Images},
  author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Di Salvo, Paul A. and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and Moeller, Anna K. and Mandeville, Elizabeth G. and Clune, Jeff and Miller, Ryan S.},
  date = {2019},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {585--590},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13120},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13120},
  urldate = {2025-03-23},
  abstract = {Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
  langid = {english},
  keywords = {artificial intelligence,camera trap,convolutional neural network,deep neural networks,image classification,machine learning,r package,remote sensing},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZFW8RP7B/Tabak et al. - 2019 - Machine learning to classify animal species in camera trap images Applications in ecology.pdf;/Volumes/ExSSD/UserData/Zotero/storage/DXF2KQ26/2041-210X.html}
}

@inproceedings{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  date = {2019-05-24},
  pages = {6105--6114},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/tan19a.html},
  urldate = {2025-06-02},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/879X2CG2/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf}
}

@article{thomsenEnvironmentalDNAEmerging2015,
  title = {Environmental {{DNA}} – {{An}} Emerging Tool in Conservation for Monitoring Past and Present Biodiversity},
  author = {Thomsen, Philip Francis and Willerslev, Eske},
  date = {2015-03-01},
  journaltitle = {Biological Conservation},
  shortjournal = {Biological Conservation},
  series = {Special {{Issue}}: {{Environmental DNA}}: {{A}} Powerful New Tool for Biological Conservation},
  volume = {183},
  pages = {4--18},
  issn = {0006-3207},
  doi = {10.1016/j.biocon.2014.11.019},
  url = {https://www.sciencedirect.com/science/article/pii/S0006320714004443},
  urldate = {2025-06-19},
  abstract = {The continuous decline in Earth’s biodiversity represents a major crisis and challenge for the 21st century, and there is international political agreement to slow down or halt this decline. The challenge is in large part impeded by the lack of knowledge on the state and distribution of biodiversity – especially since the majority of species on Earth are un-described by science. All conservation efforts to save biodiversity essentially depend on the monitoring of species and populations to obtain reliable distribution patterns and population size estimates. Such monitoring has traditionally relied on physical identification of species by visual surveys and counting of individuals. However, traditional monitoring techniques remain problematic due to difficulties associated with correct identification of cryptic species or juvenile life stages, a continuous decline in taxonomic expertise, non-standardized sampling, and the invasive nature of some survey techniques. Hence, there is urgent need for alternative and efficient techniques for large-scale biodiversity monitoring. Environmental DNA (eDNA) – defined here as: genetic material obtained directly from environmental samples (soil, sediment, water, etc.) without any obvious signs of biological source material – is an efficient, non-invasive and easy-to-standardize sampling approach. Coupled with sensitive, cost-efficient and ever-advancing DNA sequencing technology, it may be an appropriate candidate for the challenge of biodiversity monitoring. Environmental DNA has been obtained from ancient as well as modern samples and encompasses single species detection to analyses of ecosystems. The research on eDNA initiated in microbiology, recognizing that culture-based methods grossly misrepresent the microbial diversity in nature. Subsequently, as a method to assess the diversity of macro-organismal communities, eDNA was first analyzed in sediments, revealing DNA from extinct and extant animals and plants, but has since been obtained from various terrestrial and aquatic environmental samples. Results from eDNA approaches have provided valuable insights to the study of ancient environments and proven useful for monitoring contemporary biodiversity in terrestrial and aquatic ecosystems. In the future, we expect the eDNA-based approaches to move from single-marker analyses of species or communities to meta-genomic surveys of entire ecosystems to predict spatial and temporal biodiversity patterns. Such advances have applications for a range of biological, geological and environmental sciences. Here we review the achievements gained through analyses of eDNA from macro-organisms in a conservation context, and discuss its potential advantages and limitations for biodiversity monitoring.},
  keywords = {Biodiversity,Conservation,DNA metabarcoding,eDNA,Environmental DNA,Extinction,Monitoring},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/K5Y5CVIP/Thomsen and Willerslev - 2015 - Environmental DNA – An emerging tool in conservation for monitoring past and present biodiversity.pdf;/Volumes/ExSSD/UserData/Zotero/storage/SHN4NA9J/Thomsen and Willerslev - 2015 - Environmental DNA – An emerging tool in conservation for monitoring past and present biodiversity.pdf;/Volumes/ExSSD/UserData/Zotero/storage/LI8KFPUH/S0006320714004443.html}
}

@online{velezChoosingAppropriatePlatform2022,
  title = {Choosing an {{Appropriate Platform}} and {{Workflow}} for {{Processing Camera Trap Data}} Using {{Artificial Intelligence}}},
  author = {Vélez, Juliana and Castiblanco-Camacho, Paula J. and Tabak, Michael A. and Chalmers, Carl and Fergus, Paul and Fieberg, John},
  date = {2022-02-04},
  eprint = {2202.02283},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.02283},
  url = {http://arxiv.org/abs/2202.02283},
  urldate = {2025-03-16},
  abstract = {Camera traps have transformed how ecologists study wildlife species distributions, activity patterns, and interspecific interactions. Although camera traps provide a cost-effective method for monitoring species, the time required for data processing can limit survey efficiency. Thus, the potential of Artificial Intelligence (AI), specifically Deep Learning (DL), to process camera-trap data has gained considerable attention. Using DL for these applications involves training algorithms, such as Convolutional Neural Networks (CNNs), to automatically detect objects and classify species. To overcome technical challenges associated with training CNNs, several research communities have recently developed platforms that incorporate DL in easy-to-use interfaces. We review key characteristics of four AI-powered platforms -- Wildlife Insights (WI), MegaDetector (MD), Machine Learning for Wildlife Image Classification (MLWIC2), and Conservation AI -- including data management tools and AI features. We also provide R code in an open-source GitBook, to demonstrate how users can evaluate model performance, and incorporate AI output in semi-automated workflows. We found that species classifications from WI and MLWIC2 generally had low recall values (animals that were present in the images often were not classified to the correct species). Yet, the precision of WI and MLWIC2 classifications for some species was high (i.e., when classifications were made, they were generally accurate). MD, which classifies images using broader categories (e.g., "blank" or "animal"), also performed well. Thus, we conclude that, although species classifiers were not accurate enough to automate image processing, DL could be used to improve efficiencies by accepting classifications with high confidence values for certain species or by filtering images containing blanks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/MB59NJDB/Vélez et al. - 2022 - Choosing an Appropriate Platform and Workflow for Processing Camera Trap Data using Artificial Intel.pdf}
}

@article{yarnellUsingOccupancyAnalysis2014,
  title = {Using Occupancy Analysis to Validate the Use of Footprint Tunnels as a Method for Monitoring the Hedgehog {{Erinaceus}} Europaeus},
  author = {Yarnell, Richard W. and Pacheco, Marina and Williams, Ben and Neumann, Jessica L. and Rymer, David J. and Baker, Philip J.},
  date = {2014},
  journaltitle = {Mammal Review},
  volume = {44},
  number = {3--4},
  pages = {234--238},
  issn = {1365-2907},
  doi = {10.1111/mam.12026},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mam.12026},
  urldate = {2025-06-19},
  abstract = {Indirect survey methods are often used in studies of mammals but are susceptible to biases caused by failure to detect species where they are present. Occupancy analysis is an analytical technique which enables non-detection rates to be estimated and which can be used to develop and refine novel survey methods. In this study, we investigated the use of footprint tunnels by volunteers as a method for surveying occupancy of sites by hedgehogs Erinaceus europaeus. The survey protocol led to a very low non-detection rate and could reasonably be used to detect occupancy changes of 25\% with statistical power of 0.95 in a national survey.},
  langid = {english},
  keywords = {citizen science,Erinaceidae,field sign surveys,occupancy modelling,population monitoring},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/746A7WX4/Yarnell et al. - 2014 - Using occupancy analysis to validate the use of footprint tunnels as a method for monitoring the hed.pdf;/Volumes/ExSSD/UserData/Zotero/storage/N4SEFJGS/mam.html}
}

@article{zotinANIMALDETECTIONUSING2019,
  title = {{{ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS}}},
  author = {Zotin, A. G. and Proskurin, A. V.},
  date = {2019-05-09},
  journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XLII-2-W12},
  pages = {249--257},
  publisher = {Copernicus GmbH},
  issn = {1682-1750},
  doi = {10.5194/isprs-archives-XLII-2-W12-249-2019},
  url = {https://isprs-archives.copernicus.org/articles/XLII-2-W12/249/2019/isprs-archives-XLII-2-W12-249-2019.html},
  urldate = {2025-03-16},
  abstract = {Camera traps providing enormous number of images during a season help to observe remotely animals in the wild. However, analysis of such image collection manually is impossible. In this research, we develop a method for automatic animal detection based on background modeling of scene under complex shooting. First, we design a fast algorithm for image selection without motions. Second, the images are processed by modified Multi-Scale Retinex algorithm in order to align uneven illumination. Finally, background is subtracted from incoming image using adaptive threshold. A threshold value is adjusted by saliency map, which is calculated using pyramid consisting of the original image and images modified by MSR algorithm. Proposed method allows to achieve high estimators of animals detection.},
  eventtitle = {International {{Workshop}} on “{{Photogrammetric}} and {{Computer Vision Techniques}} for {{Video Surveillance}}, {{Biometrics}} and {{Biomedicine}}” ({{Volume XLII-2}}/{{W12}}) - 13\&ndash;15 {{May}} 2019, {{Moscow}}, {{Russia}}},
  langid = {english},
  keywords = {Animal Detection,Background Modeling,Camera Traps,MSR Algorithm},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/LQHGNGWC/Zotin and Proskurin - 2019 - ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS.pdf}
}
