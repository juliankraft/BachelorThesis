% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 05_conclusion_outlook
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion and Outlook}
\label{conclusion_outlook}

\subsection{Conclusion}

This thesis demonstrated the effectiveness of deep learning models for detecting and classifying small mammals from camera trap images.
The pretrained EfficientNet-B0 model provided superior classification accuracy, quickly converging and demonstrating robustness across validation folds.
The integration of automated detections utilizing \ac{MD}, while beneficial, revealed some room for improvement, particularly concerning misdetections and missed out detections.
It was found that some finetuning of the \ac{MD} to the specific MammaliaBox camera trap setup could improve the results.
Despite these limitations, the processing pipeline and the trained models provide a promising start for developing an applicable tool and a workflow to reduce manual effort.

\subsection{Outlook}
The sequence-based classification did not improve classification performance significantly as it was done in this thesis.
It still seems a very promising approach, since camera trap images are often taken in sequences and the sequence information could be used to improve classification performance.
Further research could explore different options for sequence-based classification.
As a first step, the model's output could be evaluated on the logits level to determine how this information could be utilized to improve classification performance.
More sophisticated approaches could involve utilizing temporally aware models, as demonstrated by \textcite{muhammadTemporalSwinFPNNetNovel2024}.
Since the initial detection process using \ac{MD} could still be improved, utilizing sequence information for the detection process could be explored further.
\textcite{zotinANIMALDETECTIONUSING2019} demonstrated a promising non-\ac{DL} approach for detecting animals in camera trap images using sequence information.

Future enhancements should focus on addressing current limitations by introducing an explicit category for non-target species to improve classification accuracy and reduce false predictions with high confidence.
To allow for broader application of the model, additional categories would be needed --- such as the here explicitly ignored \textit{Glis glis}, a common small mammal in Switzerland on the \textcite{iucnIUCNRedList2025} Red List of Threatened Species.
To improve the model's robustness while adding more categories --- possibly with limited data availability --- data augmentation techniques could be implemented, as they have been shown to enhance model performance and generalization \autocite{shortenSurveyImageData2019}.

Another important step toward practical application is to develop an interface or integrated software solution.
This would allow researchers to actually use the model in their workflows for monitoring small mammal populations.
This could be done in a way that integrates manual review --- which is still necessary for reliable results --- as a means of further improving the model.
Currently, the data processing pipeline still depends on manual preprocessing of the image metadata to extract sequence information.
This step would benefit from automation to streamline the workflow --- ideally, the input for the classification task would be just the raw images as retained from the camera trap.
The only available information in the \ac{EXIF} metadata that potentially allows grouping images into sequences is the timestamp.
However, this alone is not sufficient to determine sequence length, as it does not account for the actual number of images per trigger.

Better sequence information is available visually imprinted on top of the images, which could be extracted using an \ac{OCR} model --- refer to \autoref{fig:ocr_sample}.
Since the information is imprinted on the images with high contrast and no distortion, this should be a straightforward task for an \ac{OCR} model.
The fact that the first information imprinted on the image is the timestamp — which is also available in the \ac{EXIF} metadata --- could be used to match the \ac{OCR} output with the \ac{EXIF} information to quickly determine the reliability of the \ac{OCR} output.

\begin{figure}[ht]
\centering
\includegraphics{figures/ocr_example.pdf}
\caption{Top 5\% of a camera trap image —-- it was processed with the \texttt{Tesseract} \ac{OCR} model. The output string was: \texttt{2019-09-04 1:02:09 AM M 1/3 \#9 10\textdegree C}.}
\label{fig:ocr_sample}
\end{figure}
