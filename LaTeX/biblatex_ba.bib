@article{beaudrotStandardizedAssessmentBiodiversity2016,
  title = {Standardized {{Assessment}} of {{Biodiversity Trends}} in {{Tropical Forest Protected Areas}}: {{The End Is Not}} in {{Sight}}},
  shorttitle = {Standardized {{Assessment}} of {{Biodiversity Trends}} in {{Tropical Forest Protected Areas}}},
  author = {Beaudrot, Lydia and Ahumada, Jorge A. and O'Brien, Timothy and Alvarez-Loayza, Patricia and Boekee, Kelly and Campos-Arceiz, Ahimsa and Eichberg, David and Espinosa, Santiago and Fegraus, Eric and Fletcher, Christine and Gajapersad, Krisna and Hallam, Chris and Hurtado, Johanna and Jansen, Patrick A. and Kumar, Amit and Larney, Eileen and Lima, Marcela Guimarães Moreira and Mahony, Colin and Martin, Emanuel H. and McWilliam, Alex and Mugerwa, Badru and Ndoundou-Hockemba, Mireille and Razafimahaimodison, Jean Claude and Romero-Saltos, Hugo and Rovero, Francesco and Salvador, Julia and Santos, Fernanda and Sheil, Douglas and Spironello, Wilson R. and Willig, Michael R. and Winarni, Nurul L. and Zvoleff, Alex and Andelman, Sandy J.},
  date = {2016-01-19},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {14},
  number = {1},
  pages = {e1002357},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002357},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002357},
  urldate = {2025-03-23},
  abstract = {Extinction rates in the Anthropocene are three orders of magnitude higher than background and disproportionately occur in the tropics, home of half the world’s species. Despite global efforts to combat tropical species extinctions, lack of high-quality, objective information on tropical biodiversity has hampered quantitative evaluation of conservation strategies. In particular, the scarcity of population-level monitoring in tropical forests has stymied assessment of biodiversity outcomes, such as the status and trends of animal populations in protected areas. Here, we evaluate occupancy trends for 511 populations of terrestrial mammals and birds, representing 244 species from 15 tropical forest protected areas on three continents. For the first time to our knowledge, we use annual surveys from tropical forests worldwide that employ a standardized camera trapping protocol, and we compute data analytics that correct for imperfect detection. We found that occupancy declined in 22\%, increased in 17\%, and exhibited no change in 22\% of populations during the last 3–8 years, while 39\% of populations were detected too infrequently to assess occupancy changes. Despite extensive variability in occupancy trends, these 15 tropical protected areas have not exhibited systematic declines in biodiversity (i.e., occupancy, richness, or evenness) at the community level. Our results differ from reports of widespread biodiversity declines based on aggregated secondary data and expert opinion and suggest less extreme deterioration in tropical forest protected areas. We simultaneously fill an important conservation data gap and demonstrate the value of large-scale monitoring infrastructure and powerful analytics, which can be scaled to incorporate additional sites, ecosystems, and monitoring methods. In an era of catastrophic biodiversity loss, robust indicators produced from standardized monitoring infrastructure are critical to accurately assess population outcomes and identify conservation strategies that can avert biodiversity collapse.},
  langid = {english},
  keywords = {Biodiversity,Birds,Conservation science,Forests,Mammals,Species extinction,Tropical forests,Vertebrates},
  file = {/Users/jk/Zotero/storage/UJTDFLYN/Beaudrot et al. - 2016 - Standardized Assessment of Biodiversity Trends in Tropical Forest Protected Areas The End Is Not in.pdf}
}

@online{beeryEfficientPipelineCamera2019,
  title = {Efficient {{Pipeline}} for {{Camera Trap Image Review}}},
  author = {Beery, Sara and Morris, Dan and Yang, Siyu},
  date = {2019-07-15},
  eprint = {1907.06772},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.06772},
  url = {http://arxiv.org/abs/1907.06772},
  urldate = {2025-06-12},
  abstract = {Biologists all over the world use camera traps to monitor biodiversity and wildlife population density. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but it has proven difficult to to apply models trained in one region to images collected in different geographic areas. In some cases, accuracy falls off catastrophically in new region, due to both changes in background and the presence of previously-unseen species. We propose a pipeline that takes advantage of a pre-trained general animal detector and a smaller set of labeled images to train a classification model that can efficiently achieve accurate results in a new region.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jk/Zotero/storage/3U5GEMPZ/Beery et al. - 2019 - Efficient Pipeline for Camera Trap Image Review.pdf;/Users/jk/Zotero/storage/B7LQXLJL/1907.html}
}

@online{beeryRecognitionTerraIncognita2018,
  title = {Recognition in {{Terra Incognita}}},
  author = {Beery, Sara and family=Horn, given=Grant, prefix=van, useprefix=false and Perona, Pietro},
  date = {2018-07-25},
  eprint = {1807.04975},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1807.04975},
  url = {http://arxiv.org/abs/1807.04975},
  urldate = {2025-03-18},
  abstract = {It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Populations and Evolution},
  file = {/Users/jk/Zotero/storage/7ZXRYK8G/Beery et al. - 2018 - Recognition in Terra Incognita.pdf;/Users/jk/Zotero/storage/EPFWEW9V/1807.html}
}

@article{bintaislamAnimalSpeciesRecognition2023,
  title = {Animal {{Species Recognition}} with {{Deep Convolutional Neural Networks}} from {{Ecological Camera Trap Images}}},
  author = {Binta Islam, Sazida and Valles, Damian and Hibbitts, Toby J. and Ryberg, Wade A. and Walkup, Danielle K. and Forstner, Michael R. J.},
  date = {2023-01},
  journaltitle = {Animals},
  volume = {13},
  number = {9},
  pages = {1526},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-2615},
  doi = {10.3390/ani13091526},
  url = {https://www.mdpi.com/2076-2615/13/9/1526},
  urldate = {2025-03-25},
  abstract = {Accurate identification of animal species is necessary to understand biodiversity richness, monitor endangered species, and study the impact of climate change on species distribution within a specific region. Camera traps represent a passive monitoring technique that generates millions of ecological images. The vast numbers of images drive automated ecological analysis as essential, given that manual assessment of large datasets is laborious, time-consuming, and expensive. Deep learning networks have been advanced in the last few years to solve object and species identification tasks in the computer vision domain, providing state-of-the-art results. In our work, we trained and tested machine learning models to classify three animal groups (snakes, lizards, and toads) from camera trap images. We experimented with two pretrained models, VGG16 and ResNet50, and a self-trained convolutional neural network (CNN-1) with varying CNN layers and augmentation parameters. For multiclassification, CNN-1 achieved 72\% accuracy, whereas VGG16 reached 87\%, and ResNet50 attained 86\% accuracy. These results demonstrate that the transfer learning approach outperforms the self-trained model performance. The models showed promising results in identifying species, especially those with challenging body sizes and vegetation.},
  issue = {9},
  langid = {english},
  keywords = {camera trap,convolutional neural network,deep learning,endangered species,image augmentation,image classification,lizard,machine learning,snake,toad},
  file = {/Users/jk/Zotero/storage/IFIQBMSW/Binta Islam et al. - 2023 - Animal Species Recognition with Deep Convolutional Neural Networks from Ecological Camera Trap Image.pdf}
}

@article{bohnerSemiautomaticWorkflowProcess2023,
  title = {A Semi-Automatic Workflow to Process Images from Small Mammal Camera Traps},
  author = {Böhner, Hanna and Kleiven, Eivind Flittie and Ims, Rolf Anker and Soininen, Eeva M.},
  date = {2023-09-01},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {76},
  pages = {102150},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2023.102150},
  url = {https://www.sciencedirect.com/science/article/pii/S1574954123001796},
  urldate = {2025-03-16},
  abstract = {Camera traps have become popular for monitoring biodiversity, but the huge amounts of image data that arise from camera trap monitoring represent a challenge and artificial intelligence is increasingly used to automatically classify large image data sets. However, it is still challenging to combine automatic classification with other steps and tools needed for efficient, quality-assured and adaptive processing of camera trap images in long-term monitoring programs. Here we propose a semi-automatic workflow to process images from small mammal cameras that combines all necessary steps from downloading camera trap images in the field to a quality checked data set ready to be used in ecological analyses. The workflow is implemented in R and includes (1) managing raw images, (2) automatic image classification, (3) quality check of automatic image labels, as well as the possibilities to (4) retrain the model with new images and to (5) manually review subsets of images to correct image labels. We illustrate the application of this workflow for the development of a new monitoring program of an Arctic small mammal community. We first trained a classification model for the specific small mammal community based on images from an initial set of camera traps. As the monitoring program evolved, the classification model was retrained with a small subset of images from new camera traps. This case study highlights the importance of model retraining in adaptive monitoring programs based on camera traps as this step in the workflow increases model performance and substantially decreases the total time needed for manually reviewing images and correcting image labels. We provide all R scripts to make the workflow accessible to other ecologists.},
  keywords = {Adaptive monitoring,Automatic image classification,Camera trap,Data processing,Deep learning,Rodent},
  file = {/Users/jk/Zotero/storage/YWP8GERF/Böhner et al. - 2023 - A semi-automatic workflow to process images from small mammal camera traps.pdf;/Users/jk/Zotero/storage/LA2CIZIK/S1574954123001796.html}
}

@article{bothmannAutomatedWildlifeImage2023,
  title = {Automated Wildlife Image Classification: {{An}} Active Learning Tool for Ecological Applications},
  shorttitle = {Automated Wildlife Image Classification},
  author = {Bothmann, Ludwig and Wimmer, Lisa and Charrakh, Omid and Weber, Tobias and Edelhoff, Hendrik and Peters, Wibke and Nguyen, Hien and Benjamin, Caryl and Menzel, Annette},
  date = {2023-11},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {77},
  eprint = {2303.15823},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {102231},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2023.102231},
  url = {http://arxiv.org/abs/2303.15823},
  urldate = {2025-03-12},
  abstract = {Wildlife camera trap images are being used extensively to investigate animal abundance, habitat associations, and behavior, which is complicated by the fact that experts must first classify the images manually. Artificial intelligence systems can take over this task but usually need a large number of already-labeled training images to achieve sufficient performance. This requirement necessitates human expert labor and poses a particular challenge for projects with few cameras or short durations. We propose a label-efficient learning strategy that enables researchers with small or medium-sized image databases to leverage the potential of modern machine learning, thus freeing crucial resources for subsequent analyses. Our methodological proposal is two-fold: (1) We improve current strategies of combining object detection and image classification by tuning the hyperparameters of both models. (2) We provide an active learning (AL) system that allows training deep learning models very efficiently in terms of required human-labeled training images. We supply a software package that enables researchers to use these methods directly and thereby ensure the broad applicability of the proposed framework in ecological practice. We show that our tuning strategy improves predictive performance. We demonstrate how the AL pipeline reduces the amount of pre-labeled data needed to achieve a specific predictive performance and that it is especially valuable for improving out-of-sample predictive performance. We conclude that the combination of tuning and AL increases predictive performance substantially. Furthermore, we argue that our work can broadly impact the community through the ready-to-use software package provided. Finally, the publication of our models tailored to European wildlife data enriches existing model bases mostly trained on data from Africa and North America.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Applications}
}

@article{bothmannAutomatedWildlifeImage2023a,
  title = {Automated Wildlife Image Classification: {{An}} Active Learning Tool for Ecological Applications},
  shorttitle = {Automated Wildlife Image Classification},
  author = {Bothmann, Ludwig and Wimmer, Lisa and Charrakh, Omid and Weber, Tobias and Edelhoff, Hendrik and Peters, Wibke and Nguyen, Hien and Benjamin, Caryl and Menzel, Annette},
  date = {2023-11},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {77},
  eprint = {2303.15823},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {102231},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2023.102231},
  url = {http://arxiv.org/abs/2303.15823},
  urldate = {2025-03-12},
  abstract = {Wildlife camera trap images are being used extensively to investigate animal abundance, habitat associations, and behavior, which is complicated by the fact that experts must first classify the images manually. Artificial intelligence systems can take over this task but usually need a large number of already-labeled training images to achieve sufficient performance. This requirement necessitates human expert labor and poses a particular challenge for projects with few cameras or short durations. We propose a label-efficient learning strategy that enables researchers with small or medium-sized image databases to leverage the potential of modern machine learning, thus freeing crucial resources for subsequent analyses. Our methodological proposal is two-fold: (1) We improve current strategies of combining object detection and image classification by tuning the hyperparameters of both models. (2) We provide an active learning (AL) system that allows training deep learning models very efficiently in terms of required human-labeled training images. We supply a software package that enables researchers to use these methods directly and thereby ensure the broad applicability of the proposed framework in ecological practice. We show that our tuning strategy improves predictive performance. We demonstrate how the AL pipeline reduces the amount of pre-labeled data needed to achieve a specific predictive performance and that it is especially valuable for improving out-of-sample predictive performance. We conclude that the combination of tuning and AL increases predictive performance substantially. Furthermore, we argue that our work can broadly impact the community through the ready-to-use software package provided. Finally, the publication of our models tailored to European wildlife data enriches existing model bases mostly trained on data from Africa and North America.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Applications}
}

@inproceedings{brodersenBalancedAccuracyIts2010,
  title = {The {{Balanced Accuracy}} and {{Its Posterior Distribution}}},
  booktitle = {2010 20th {{International Conference}} on {{Pattern Recognition}}},
  author = {Brodersen, Kay Henning and Ong, Cheng Soon and Stephan, Klaas Enno and Buhmann, Joachim M.},
  date = {2010-08},
  pages = {3121--3124},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2010.764},
  url = {https://ieeexplore.ieee.org/document/5597285},
  urldate = {2025-06-02},
  abstract = {Evaluating the performance of a classification algorithm critically requires a measure of the degree to which unseen examples have been identified with their correct class labels. In practice, generalizability is frequently estimated by averaging the accuracies obtained on individual cross-validation folds. This procedure, however, is problematic in two ways. First, it does not allow for the derivation of meaningful confidence intervals. Second, it leads to an optimistic estimate when a biased classifier is tested on an imbalanced dataset. We show that both problems can be overcome by replacing the conventional point estimate of accuracy by an estimate of the posterior distribution of the balanced accuracy.},
  eventtitle = {2010 20th {{International Conference}} on {{Pattern Recognition}}},
  keywords = {Accuracy,Approximation algorithms,bias,class imbalance,classification performance,generalizability,Inference algorithms,Machine learning,Prediction algorithms,Probabilistic logic,Training},
  file = {/Users/jk/Zotero/storage/EF3K38HV/Brodersen et al. - 2010 - The Balanced Accuracy and Its Posterior Distribution.pdf}
}

@book{brondizioGlobalAssessmentReport2019,
  title = {The Global Assessment Report of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services},
  editor = {Brondízio, Eduardo Sonnewend and Settele, Josef and Díaz, Sandra and Ngo, Hien Thu},
  date = {2019},
  publisher = {{Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES)}},
  location = {Bonn},
  isbn = {978-3-947851-20-1},
  langid = {english},
  annotation = {OCLC: 1336011247},
  file = {/Users/jk/Zotero/storage/UK6DXMJ3/Brondízio et al. - 2019 - The global assessment report of the intergovernmen.pdf}
}

@article{cardinaleBiodiversityLossIts2012,
  title = {Biodiversity Loss and Its Impact on Humanity},
  author = {Cardinale, Bradley J. and Duffy, J. Emmett and Gonzalez, Andrew and Hooper, David U. and Perrings, Charles and Venail, Patrick and Narwani, Anita and Mace, Georgina M. and Tilman, David and Wardle, David A. and Kinzig, Ann P. and Daily, Gretchen C. and Loreau, Michel and Grace, James B. and Larigauderie, Anne and Srivastava, Diane S. and Naeem, Shahid},
  date = {2012-06},
  journaltitle = {Nature},
  volume = {486},
  number = {7401},
  pages = {59--67},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11148},
  url = {https://www.nature.com/articles/nature11148},
  urldate = {2024-04-18},
  abstract = {Two decades ago the first Earth Summit raised the question of how biological diversity loss alters ecosystem functioning and affects humanity; this Review looks at the progress made towards answering this question.},
  langid = {english},
  keywords = {Biodiversity,Ecosystem services},
  file = {/Users/jk/Zotero/storage/ZC2YLYKA/Cardinale et al. - 2012 - Biodiversity loss and its impact on humanity.pdf}
}

@article{computerscienceandengineeringsrminstituteofscienceandtechnologykattankulathur603203india.ImageClassificationUsing2019,
  title = {Image {{Classification}} Using a {{Hybrid LSTM-CNN Deep Neural Network}}},
  author = {{Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India.} and {Aditi*} and Nagda, Mayank Kumar and {Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India.} and Poovammal*, E. and {Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India.}},
  date = {2019-08-30},
  journaltitle = {International Journal of Engineering and Advanced Technology},
  shortjournal = {IJEAT},
  volume = {8},
  number = {6},
  pages = {1342--1348},
  issn = {22498958},
  doi = {10.35940/ijeat.F8602.088619},
  url = {https://www.ijeat.org/portfolio-item/F8602088619/},
  urldate = {2025-03-21},
  abstract = {This work elaborates on the integration of the rudimentary Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM), resulting in a new paradigm in the well-explored field of image classification. LSTM is one kind of Recurrent Neural Network (RNN) which has the potential to memorize long-term dependencies. It was observed that LSTMs are able to complement the feature extraction ability of CNN when used in a layered order. LSTMs have the capacity to selectively remember patterns for a long duration of time and CNNs are able to extract the important features out of it. This LSTM-CNN layered structure, when used for image classification, has an edge over conventional CNN classifier. The model which has been proposed is based on the sets of Artificial Neural Network like Recurrent and Convolutional neural network; hence this model is robust and suitable to a wide spectrum of classification tasks. To validate these results, we have tested our model on two standard datasets. The results have been compared with other classifiers to establish the significance of our proposed model.},
  langid = {english},
  file = {/Users/jk/Zotero/storage/LNJB3VWS/Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India. et al. - 2019 - Image Classification using a Hybrid LSTM-CNN Deep Neural Network.pdf}
}

@article{cordierCameraTrapResearch2022,
  title = {Camera Trap Research in {{Africa}}: {{A}} Systematic Review to Show Trends in Wildlife Monitoring and Its Value as a Research Tool},
  shorttitle = {Camera Trap Research in {{Africa}}},
  author = {Cordier, Craig P. and Ehlers Smith, David A. and Ehlers Smith, Yvette and Downs, Colleen T.},
  date = {2022-12-01},
  journaltitle = {Global Ecology and Conservation},
  shortjournal = {Global Ecology and Conservation},
  volume = {40},
  pages = {e02326},
  issn = {2351-9894},
  doi = {10.1016/j.gecco.2022.e02326},
  url = {https://www.sciencedirect.com/science/article/pii/S2351989422003286},
  urldate = {2025-03-23},
  abstract = {Camera traps have been used increasingly as a research tool to monitor wildlife globally, and have become more advanced, thereby improving their performance and lowering costs. Their use has allowed researchers to study a range of species, including rare and elusive species, particularly in remote areas, in a non-invasive, reliable and cost-effective way. In this review, we sought to document the camera trapping research on terrestrial wildlife conducted in Africa, identifying countries and habitat types of focus, and how these camera trap research trends in Africa could be improved in the future. Through a systematic literature search, we found 408 peer-reviewed publications using camera traps to study terrestrial wildlife in Africa, with the first being in 2005 and up to 2021. Although camera trap studies were conducted in 38 African countries, most were in South Africa (28.9~\%). Most studies assessed the occupancy of species (41.4~\%). The studies covered a range of taxa, with mammals being the most popular. The majority of research focussed on large carnivores (24.8~\%), with a particular focus on leopards (Panthera pardus) (60 studies). Most studies (43.9~\%) focused on a single species, with forests (174 studies, 42.6~\%) and savannah/bushveld (145 studies, 35.5~\%) being the habitat type of focus. There was also a strong preference for camera trap studies to be conducted in protected areas (68.9~\%). The camera trap methods used varied considerably between studies, which included: the number of camera trap stations, survey length, trap effort, camera trap make and model, camera trap flash type, interval delay, camera multishot, height of camera trap placement, camera trap layout method and whether the camera trap was baited. These variations are expected because of the difference in research goals posed by each study. However, studies with similar objectives and/or focus still display a clear lack of standardisation (studies do not conform to a specific standard), which could negatively impact the results obtained, as inappropriate camera trap protocols could affect the detectability of certain species. Future camera trap studies will hopefully extend to countries and taxa that have received little attention, with further research informing appropriate conservation strategies that could reduce the threats to biodiversity.},
  keywords = {Camera traps,Forest,Large carnivore,Monitor wildlife,Protocols,Terrestrial wildlife},
  file = {/Users/jk/Zotero/storage/55KML7HA/Cordier et al. - 2022 - Camera trap research in Africa A systematic review to show trends in wildlife monitoring and its va.pdf;/Users/jk/Zotero/storage/CJQUR2B6/S2351989422003286.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2025-06-02},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/Users/jk/Zotero/storage/J72A7EIB/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2025-06-02},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jk/Zotero/storage/DB6DBZ2X/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/Users/jk/Zotero/storage/N2APVDTF/2010.html}
}

@software{falconPyTorchLightning2025,
  title = {{{PyTorch Lightning}}},
  author = {Falcon, William and {The PyTorch Lightning Team}},
  date = {2025-04-25},
  doi = {10.5281/zenodo.15284694},
  url = {https://zenodo.org/records/15284694},
  urldate = {2025-05-31},
  abstract = {The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.},
  organization = {Zenodo},
  version = {2.5.1.post0},
  keywords = {artificial intelligence,deep learning,machine learning},
  file = {/Users/jk/Zotero/storage/H9NQRLLE/15284694.html}
}

@article{gaoCNNBiLSTMComplexEnvironmentOriented2023,
  title = {{{CNN-Bi-LSTM}}: {{A Complex Environment-Oriented Cattle Behavior Classification Network Based}} on the {{Fusion}} of {{CNN}} and {{Bi-LSTM}}},
  shorttitle = {{{CNN-Bi-LSTM}}},
  author = {Gao, Guohong and Wang, Chengchao and Wang, Jianping and Lv, Yingying and Li, Qian and Ma, Yuxin and Zhang, Xueyan and Li, Zhiyu and Chen, Guanglan},
  date = {2023-01},
  journaltitle = {Sensors},
  volume = {23},
  number = {18},
  pages = {7714},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s23187714},
  url = {https://www.mdpi.com/1424-8220/23/18/7714},
  urldate = {2025-03-21},
  abstract = {Cattle behavior classification technology holds a crucial position within the realm of smart cattle farming. Addressing the requisites of cattle behavior classification in the agricultural sector, this paper presents a novel cattle behavior classification network tailored for intricate environments. This network amalgamates the capabilities of CNN and Bi-LSTM. Initially, a data collection method is devised within an authentic farm setting, followed by the delineation of eight fundamental cattle behaviors. The foundational step involves utilizing VGG16 as the cornerstone of the CNN network, thereby extracting spatial feature vectors from each video data sequence. Subsequently, these features are channeled into a Bi-LSTM classification model, adept at unearthing semantic insights from temporal data in both directions. This process ensures precise recognition and categorization of cattle behaviors. To validate the model’s efficacy, ablation experiments, generalization effect assessments, and comparative analyses under consistent experimental conditions are performed. These investigations, involving module replacements within the classification model and comprehensive analysis of ablation experiments, affirm the model’s effectiveness. The self-constructed dataset about cattle is subjected to evaluation using cross-entropy loss, assessing the model’s generalization efficacy across diverse subjects and viewing perspectives. Classification performance accuracy is quantified through the application of a confusion matrix. Furthermore, a set of comparison experiments is conducted, involving three pertinent deep learning models: MASK-RCNN, CNN-LSTM, and EfficientNet-LSTM. The outcomes of these experiments unequivocally substantiate the superiority of the proposed model. Empirical results underscore the CNN-Bi-LSTM model’s commendable performance metrics: achieving 94.3\% accuracy, 94.2\% precision, and 93.4\% recall while navigating challenges such as varying light conditions, occlusions, and environmental influences. The objective of this study is to employ a fusion of CNN and Bi-LSTM to autonomously extract features from multimodal data, thereby addressing the challenge of classifying cattle behaviors within intricate scenes. By surpassing the constraints imposed by conventional methodologies and the analysis of single-sensor data, this approach seeks to enhance the precision and generalizability of cattle behavior classification. The consequential practical, economic, and societal implications for the agricultural sector are of considerable significance.},
  issue = {18},
  langid = {english},
  keywords = {behavior classification,Bi-LSTM,cattle,CNN},
  file = {/Users/jk/Zotero/storage/Y45E442K/Gao et al. - 2023 - CNN-Bi-LSTM A Complex Environment-Oriented Cattle Behavior Classification Network Based on the Fusi.pdf}
}

@book{Goodfellow-et-al-201,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Benigo, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {http://www.deeplearningbook.org}
}

@article{grafWildlifeCampusKleineSaeugetiere2022,
  title = {Wildlife@Campus: Kleine Säugetiere im Fokus},
  author = {Graf, Roland F. and Dietrich, Adrian and Honetschläger, Nils and Kryszczuk, Krzysztof and Palmisano, Marilena and Pothier, Joël F. and Ratnaweera, Nils and Reifler-Bächtiger, Martina and Rhyner, Nicola and Treichler, Regula},
  date = {2022-07},
  langid = {ngerman},
  file = {/Users/jk/Zotero/storage/MXGX9DBR/Graf - Wildlife@Campus Kleine Säugetiere im Fokus.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2025-06-02},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/jk/Zotero/storage/GUHR8ZH8/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@online{hernandezPytorchWildlifeCollaborativeDeep2024,
  title = {Pytorch-{{Wildlife}}: {{A Collaborative Deep Learning Framework}} for {{Conservation}}},
  shorttitle = {Pytorch-{{Wildlife}}},
  author = {Hernandez, Andres and Miao, Zhongqi and Vargas, Luisa and Beery, Sara and Dodhia, Rahul and Arbelaez, Pablo and Ferres, Juan M. Lavista},
  date = {2024-11-29},
  eprint = {2405.12930},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.12930},
  url = {http://arxiv.org/abs/2405.12930},
  urldate = {2025-02-04},
  abstract = {The alarming decline in global biodiversity, driven by various factors, underscores the urgent need for large-scale wildlife monitoring. In response, scientists have turned to automated deep learning methods for data processing in wildlife monitoring. However, applying these advanced methods in real-world scenarios is challenging due to their complexity and the need for specialized knowledge, primarily because of technical challenges and interdisciplinary barriers. To address these challenges, we introduce Pytorch-Wildlife, an open-source deep learning platform built on PyTorch. It is designed for creating, modifying, and sharing powerful AI models. This platform emphasizes usability and accessibility, making it accessible to individuals with limited or no technical background. It also offers a modular codebase to simplify feature expansion and further development. Pytorch-Wildlife offers an intuitive, user-friendly interface, accessible through local installation or Hugging Face, for animal detection and classification in images and videos. As two real-world applications, Pytorch-Wildlife has been utilized to train animal classification models for species recognition in the Amazon Rainforest and for invasive opossum recognition in the Galapagos Islands. The Opossum model achieves 98\% accuracy, and the Amazon model has 92\% recognition accuracy for 36 animals in 90\% of the data. As Pytorch-Wildlife evolves, we aim to integrate more conservation tasks, addressing various environmental challenges. Pytorch-Wildlife is available at https://github.com/microsoft/CameraTraps.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jk/Zotero/storage/5VB3HY3J/Hernandez et al. - 2024 - Pytorch-Wildlife A Collaborative Deep Learning Fr.pdf;/Users/jk/Zotero/storage/WZXRFIKM/2405.html}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  date = {2017-07},
  pages = {2261--2269},
  publisher = {IEEE},
  location = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.243},
  url = {https://ieeexplore.ieee.org/document/8099726/},
  urldate = {2025-06-02},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/jk/Zotero/storage/E9WTWW4Z/Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf}
}

@online{ilseAttentionbasedDeepMultiple2018,
  title = {Attention-Based {{Deep Multiple Instance Learning}}},
  author = {Ilse, Maximilian and Tomczak, Jakub M. and Welling, Max},
  date = {2018-06-28},
  eprint = {1802.04712},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.04712},
  url = {http://arxiv.org/abs/1802.04712},
  urldate = {2025-04-03},
  abstract = {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jk/Zotero/storage/9CJKP97Q/Ilse et al. - 2018 - Attention-based Deep Multiple Instance Learning.pdf;/Users/jk/Zotero/storage/GIA3T2FT/1802.html}
}

@article{krukLookAdvancedLearners2017,
  title = {A Look at Advanced Learners’ Use of Mobile Devices for {{English}} Language Study: {{Insights}} from Interview Data},
  author = {Kruk, Mariusz},
  date = {2017},
  journaltitle = {The EUROCALL Review},
  volume = {25},
  number = {2},
  abstract = {The paper discusses the results of a study which explored advanced learners of English engagement with their mobile devices to develop learning experiences that meet their needs and goals as foreign language learners. The data were collected from 20 students by means of a semi-structured interview. The gathered data were subjected to qualitative and quantitative analysis. The results of the study demonstrated that, on the one hand, some subjects manifested heightened awareness relating to the advantageous role of mobile devices in their learning endeavors, their ability to reach for suitable tools and retrieve necessary information so as to achieve their goals, meet their needs and adjust their learning of English to their personal learning styles, and on the other, a rather intuitive and/or ad hoc use of their mobile devices in the classroom.},
  langid = {english},
  file = {/Users/jk/Zotero/storage/SY7D4UAJ/Kruk - 2017 - A look at advanced learners’ use of mobile devices for English language study Insights from intervi.pdf}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2025-06-01},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/jk/Zotero/storage/Q5ZUFSHE/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/jk/Zotero/storage/245STX57/1711.html}
}

@online{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2017-05-03},
  eprint = {1608.03983},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.03983},
  url = {http://arxiv.org/abs/1608.03983},
  urldate = {2025-06-01},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/jk/Zotero/storage/EAUV37KB/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restarts.pdf;/Users/jk/Zotero/storage/479WEHTB/1608.html}
}

@software{morrisEfficientPipelineCamera2025,
  title = {Efficient {{Pipeline}} for {{Camera Trap Image Review}}},
  author = {Morris, Dan and Beery, Sara and Yang, Siyu},
  date = {2025-05-07T01:41:51Z},
  origdate = {2023-05-20T01:46:37Z},
  url = {http://github.com/agentmorris/MegaDetector},
  urldate = {2025-05-13},
  abstract = {MegaDetector is an AI model that helps conservation folks spend less time doing boring things with camera trap images.}
}

@inproceedings{muhammadTemporalSwinFPNNetNovel2024,
  title = {{{TemporalSwin-FPN Net}}: {{A Novel Pipeline}} for {{Metadata-Driven Sequence Classification}} in {{Camera Trap Imagery}}},
  shorttitle = {{{TemporalSwin-FPN Net}}},
  booktitle = {2024 {{International Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  author = {Muhammad, Sameeruddin and Xiang, Wei and Mann, Scott and Han, Kang and Nair, Supriya},
  date = {2024-11},
  pages = {616--623},
  doi = {10.1109/DICTA63115.2024.00094},
  url = {https://ieeexplore.ieee.org/abstract/document/10869574},
  urldate = {2025-03-21},
  abstract = {In wildlife conservation, camera traps generate a significant volume of data that requires extensive manual analysis to extract insights, particularly for species identification. Existing tools and methods for analysing camera trap data typically classify images individually, while the devices are configured to capture multiple pictures in burst mode, often accompanied by metadata. In this paper, we introduce a novel metadata-driven pipeline based on the enhanced Swin Transformer architecture, named TemporalSwin-FPN Net, which utilises metadata to improve classification through a sequential analysis pipeline. This approach shifts the focus from analysing individual snapshots to sequences of images, thereby enriching the architecture with more comprehensive data for species classification. TemporalSwin-FPN Net has achieved a remarkable accuracy of 99.7\% on the Australian camera trap dataset, and consistently outperformed the baseline models with 94.89\% and 93.32\% on the Snapshot Safari dataset for Camdeboo and Mountain Zebra projects, surpassing them by 9.62\% and 8.54\%. Moreover, integrating the TemporalSwin-FPN Net into our proposed pipeline has significantly reduced the end-to-end processing time for a test dataset of 12,922 images, from 10 minutes 54 seconds to just 6 minutes 41 seconds, thereby substantially enhancing data processing efficiency.},
  eventtitle = {2024 {{International Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  keywords = {Accuracy,camera traps,Cameras,deep learning,image classification,Metadata,Monitoring,Pipelines,Reliability,sequence,sequence-based image classification,Sequential analysis,Stakeholders,swin transformer,Transformers,Wildlife},
  file = {/Users/jk/Zotero/storage/28FMXUG4/Muhammad et al. - 2024 - TemporalSwin-FPN Net A Novel Pipeline for Metadata-Driven Sequence Classification in Camera Trap Im.pdf;/Users/jk/Zotero/storage/G7E4YXEC/10869574.html}
}

@inproceedings{nEnhancedImageClassification2025,
  title = {Enhanced {{Image Classification Using Transfer Learning}} with {{ResNet50-V2}}: {{A Case Study}} on {{Wildlife Recognition}}},
  shorttitle = {Enhanced {{Image Classification Using Transfer Learning}} with {{ResNet50-V2}}},
  booktitle = {2025 6th {{International Conference}} on {{Mobile Computing}} and {{Sustainable Informatics}} ({{ICMCSI}})},
  author = {N, Chandan and B, Manimekala and R V, Siva Balan},
  date = {2025-01},
  pages = {1565--1570},
  doi = {10.1109/ICMCSI64620.2025.10883359},
  url = {https://ieeexplore.ieee.org/abstract/document/10883359?casa_token=H3ocNM47ONUAAAAA:eVizOgmEGQc7NEWnHTr7VfZG-jiGkzB6xVtrqqpmuidTuB8ZCN2oV9utdB5fuoVSCKjTOEGZutMa},
  urldate = {2025-03-24},
  abstract = {This study explores the application of transfer learning using the ResNet50-V2 architecture for accurate classification of Arctic wildlife species, including Arctic foxes, polar bears, and walruses. Transfer learning leverages pre-trained networks to enhance performance in new tasks with limited labeled data, reducing the need for extensive data collection and computational resources. In this work, we utilized a dataset of 1000 labeled images across the three species and applied ResNet50-V2, pre-trained on ImageNet, as a feature extractor. The model achieved high accuracy, with training and validation accuracies nearing 99\% and 95-97\%, respectively, though minor overfitting was observed. This indicates the model's strong ability to generalize across the dataset while benefiting from pre-trained weights on diverse, non-related images. Additionally it compares with models like SSD and CycleGAN, emphasizing its capability to generalize well, handle small datasets, and mitigate overfitting. We discuss model architecture, data preprocessing, and the experimental results, focusing on improvements achievable through regularization techniques to counteract overfitting. This study demonstrates the effectiveness of transfer learning for wildlife classification, providing insights into optimizing CNNs for ecological and conservation applications.},
  eventtitle = {2025 6th {{International Conference}} on {{Mobile Computing}} and {{Sustainable Informatics}} ({{ICMCSI}})},
  keywords = {Accuracy,Arctic,Biological system modeling,CNN,Computational modeling,Computer architecture,CycleGAN,Feature extraction,Model Selection,Overfitting,ResNet50-V2,Training,Transfer learning,Transfer Learning,Wildlife,wildlife classification},
  file = {/Users/jk/Zotero/storage/RW8KH5P3/N et al. - 2025 - Enhanced Image Classification Using Transfer Learning with ResNet50-V2 A Case Study on Wildlife Rec.pdf;/Users/jk/Zotero/storage/9WTZEEG4/10883359.html}
}

@article{norouzzadehAutomaticallyIdentifyingCounting2018,
  title = {Automatically Identifying, Counting, and Describing Wild Animals in Camera-Trap Images with Deep Learning},
  author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
  date = {2018-06-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {25},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1719367115},
  url = {https://pnas.org/doi/full/10.1073/pnas.1719367115},
  urldate = {2025-03-12},
  abstract = {Significance             Motion-sensor cameras in natural habitats offer the opportunity to inexpensively and unobtrusively gather vast amounts of data on animals in the wild. A key obstacle to harnessing their potential is the great cost of having humans analyze each image. Here, we demonstrate that a cutting-edge type of artificial intelligence called deep neural networks can automatically extract such invaluable information. For example, we show deep learning can automate animal identification for 99.3\% of the 3.2 million-image Snapshot Serengeti dataset while performing at the same 96.6\% accuracy of crowdsourced teams of human volunteers. Automatically, accurately, and inexpensively collecting such data could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences.           ,              Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {$>$}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {$>$}8.4 y (i.e., {$>$}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
  langid = {english},
  file = {/Users/jk/Zotero/storage/AFBKWGRC/Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describing wild animals in camera-trap images with deep lea.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, Fabian and Pedregosa, Fabian and Varoquaux, Gael and Varoquaux, Gael and Org, Normalesup and Gramfort, Alexandre and Gramfort, Alexandre and Michel, Vincent and Michel, Vincent and Fr, Logilab and Thirion, Bertrand and Thirion, Bertrand and Grisel, Olivier and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Tp, Alexandre and Cournapeau, David},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  langid = {english},
  file = {/Users/jk/Zotero/storage/JGKDP9KT/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf}
}

@online{ratnaweeraWildlifeCampusProgressReports2021,
  title = {Wildlife @ {{Campus}}: {{ProgressReports}}},
  shorttitle = {Wildlife @ {{Campus}}},
  author = {Ratnaweera, Nils},
  date = {2021},
  url = {https://github.zhaw.ch/pages/Wildlife-Campus/ProgressReports/},
  urldate = {2025-06-03},
  organization = {Wildlife @ Campus: ProgressReports},
  file = {/Users/jk/Zotero/storage/HUTHINIH/ProgressReports.html}
}

@online{razavianCNNFeaturesOfftheshelf2014,
  title = {{{CNN Features}} Off-the-Shelf: An {{Astounding Baseline}} for {{Recognition}}},
  shorttitle = {{{CNN Features}} Off-the-Shelf},
  author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  date = {2014-05-12},
  eprint = {1403.6382},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1403.6382},
  url = {http://arxiv.org/abs/1403.6382},
  urldate = {2025-03-25},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \textbackslash overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \textbackslash overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \textbackslash overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jk/Zotero/storage/VYU55X2Z/Razavian et al. - 2014 - CNN Features off-the-shelf an Astounding Baseline for Recognition.pdf;/Users/jk/Zotero/storage/SAYAE4PW/1403.html}
}

@article{schneiderRecognitionEuropeanMammals2024,
  title = {Recognition of {{European}} Mammals and Birds in Camera Trap Images Using Deep Neural Networks},
  author = {Schneider, Daniel and Lindner, Kim and Vogelbacher, Markus and Bellafkir, Hicham and Farwig, Nina and Freisleben, Bernd},
  date = {2024},
  journaltitle = {IET Computer Vision},
  volume = {18},
  number = {8},
  pages = {1162--1192},
  issn = {1751-9640},
  doi = {10.1049/cvi2.12294},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cvi2.12294},
  urldate = {2025-03-16},
  abstract = {Most machine learning methods for animal recognition in camera trap images are limited to mammal identification and group birds into a single class. Machine learning methods for visually discriminating birds, in turn, cannot discriminate between mammals and are not designed for camera trap images. The authors present deep neural network models to recognise both mammals and bird species in camera trap images. They train neural network models for species classification as well as for predicting the animal taxonomy, that is, genus, family, order, group, and class names. Different neural network architectures, including ResNet, EfficientNetV2, Vision Transformer, Swin Transformer, and ConvNeXt, are compared for these tasks. Furthermore, the authors investigate approaches to overcome various challenges associated with camera trap image analysis. The authors’ best species classification models achieve a mean average precision (mAP) of 97.91\% on a validation data set and mAPs of 90.39\% and 82.77\% on test data sets recorded in forests in Germany and Poland, respectively. Their best taxonomic classification models reach a validation mAP of 97.18\% and mAPs of 94.23\% and 79.92\% on the two test data sets, respectively.},
  langid = {english},
  keywords = {computer vision,convolutional neural nets,image classification,image recognition,neural nets},
  file = {/Users/jk/Zotero/storage/4VLADYG6/Schneider et al. - 2024 - Recognition of European mammals and birds in camera trap images using deep neural networks.pdf}
}

@article{sharmaTransferLearningWildlife2024,
  title = {Transfer {{Learning}} for {{Wildlife Classification}}: {{Evaluating YOLOv8}} against {{DenseNet}}, {{ResNet}}, and {{VGGNet}} on a {{Custom Dataset}}},
  shorttitle = {Transfer {{Learning}} for {{Wildlife Classification}}},
  author = {Sharma, Subek and Dhakal, Sisir and Bhavsar, Mansi},
  date = {2024-12},
  journaltitle = {Journal of Artificial Intelligence and Capsule Networks},
  shortjournal = {JAICN},
  volume = {6},
  number = {4},
  eprint = {2408.00002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {415--435},
  issn = {2582-2012},
  doi = {10.36548/jaicn.2024.4.003},
  url = {http://arxiv.org/abs/2408.00002},
  urldate = {2025-02-04},
  abstract = {This study evaluates the performance of various deep learning models, specifically DenseNet, ResNet, VGGNet, and YOLOv8, for wildlife species classification on a custom dataset. The dataset comprises 575 images of 23 endangered species sourced from reputable online repositories. The study utilizes transfer learning to fine-tune pre-trained models on the dataset, focusing on reducing training time and enhancing classification accuracy. The results demonstrate that YOLOv8 outperforms other models, achieving a training accuracy of 97.39\% and a validation F1-score of 96.50\%. These findings suggest that YOLOv8, with its advanced architecture and efficient feature extraction capabilities, holds great promise for automating wildlife monitoring and conservation efforts.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jk/Zotero/storage/ZY2U2RFU/Sharma et al. - 2024 - Transfer Learning for Wildlife Classification Eva.pdf;/Users/jk/Zotero/storage/AGMH53IS/2408.html}
}

@article{shortenSurveyImageData2019,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  date = {2019-07-06},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  url = {https://doi.org/10.1186/s40537-019-0197-0},
  urldate = {2025-03-25},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
  file = {/Users/jk/Zotero/storage/ZEEC95RE/Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learning.pdf}
}

@online{tabakCameraTrapDetectoRAutomaticallyDetect2022,
  title = {{{CameraTrapDetectoR}}: {{Automatically}} Detect, Classify, and Count Animals in Camera Trap Images Using Artificial Intelligence},
  shorttitle = {{{CameraTrapDetectoR}}},
  author = {Tabak, Michael A. and Falbel, Daniel and Hamzeh, Tess and Brook, Ryan K. and Goolsby, John A. and Zoromski, Lisa D. and Boughton, Raoul K. and Snow, Nathan P. and VerCauteren, Kurt C. and Miller, Ryan S.},
  date = {2022-02-09},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2022.02.07.479461},
  doi = {10.1101/2022.02.07.479461},
  url = {https://www.biorxiv.org/content/10.1101/2022.02.07.479461v1},
  urldate = {2025-03-16},
  abstract = {Motion-activated wildlife cameras, or camera traps, are widely used in biological monitoring of wildlife. Studies using camera traps amass large numbers of images and analyzing these images can be a large burden that inhibits research progress. We trained deep learning computer vision models using data for 168 species that automatically detect, count, and classify common North American domestic and wild species in camera trap images. We provide our trained models in an R package, CameraTrapDetectoR. Three types of models are available: a taxonomic class model classifies objects as mammal (human and non-human) or avian; a taxonomic family model that recognizes 31 mammal, avian, and reptile families; a species model that recognizes 75 domestic and wild species including all North American wild cat species, bear species, and Canid species. Each model also includes a category for vehicles and empty images. The models performed well on both validation datasets and out-of-distribution testing datasets as mean average precision values ranged from 0.80 to 0.96. CameraTrapDetectoR provides predictions as an R object (a data frame) and flat file and provides the option to create plots of the original camera trap image with the predicted bounding box and label. There is also the option to apply models using a Shiny Application, with a point-and-click graphical user interface. This R package has the potential to facilitate application of deep learning models by biologists using camera traps.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/jk/Zotero/storage/23M7FSML/Tabak et al. - 2022 - CameraTrapDetectoR Automatically detect, classify, and count animals in camera trap images using ar.pdf}
}

@article{tabakMachineLearningClassify2019,
  title = {Machine Learning to Classify Animal Species in Camera Trap Images: {{Applications}} in Ecology},
  shorttitle = {Machine Learning to Classify Animal Species in Camera Trap Images},
  author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Di Salvo, Paul A. and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and Moeller, Anna K. and Mandeville, Elizabeth G. and Clune, Jeff and Miller, Ryan S.},
  date = {2019},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {585--590},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13120},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13120},
  urldate = {2025-03-23},
  abstract = {Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
  langid = {english},
  keywords = {artificial intelligence,camera trap,convolutional neural network,deep neural networks,image classification,machine learning,r package,remote sensing},
  file = {/Users/jk/Zotero/storage/ZFW8RP7B/Tabak et al. - 2019 - Machine learning to classify animal species in camera trap images Applications in ecology.pdf;/Users/jk/Zotero/storage/DXF2KQ26/2041-210X.html}
}

@article{tanAnimalDetectionClassification2022,
  title = {Animal {{Detection}} and {{Classification}} from {{Camera Trap Images Using Different Mainstream Object Detection Architectures}}},
  author = {Tan, Mengyu and Chao, Wentao and Cheng, Jo-Ku and Zhou, Mo and Ma, Yiwen and Jiang, Xinyi and Ge, Jianping and Yu, Lian and Feng, Limin},
  date = {2022-08-04},
  journaltitle = {Animals},
  shortjournal = {Animals},
  volume = {12},
  number = {15},
  pages = {1976},
  issn = {2076-2615},
  doi = {10.3390/ani12151976},
  url = {https://www.mdpi.com/2076-2615/12/15/1976},
  urldate = {2025-03-21},
  abstract = {Camera traps are widely used in wildlife surveys and biodiversity monitoring. Depending on its triggering mechanism, a large number of images or videos are sometimes accumulated. Some literature has proposed the application of deep learning techniques to automatically identify wildlife in camera trap imagery, which can significantly reduce manual work and speed up analysis processes. However, there are few studies validating and comparing the applicability of different models for object detection in real field monitoring scenarios. In this study, we firstly constructed a wildlife image dataset of the Northeast Tiger and Leopard National Park (NTLNP dataset). Furthermore, we evaluated the recognition performance of three currently mainstream object detection architectures and compared the performance of training models on day and night data separately versus together. In this experiment, we selected YOLOv5 series models (anchor-based one-stage), Cascade R-CNN under feature extractor HRNet32 (anchor-based two-stage), and FCOS under feature extractors ResNet50 and ResNet101 (anchor-free one-stage). The experimental results showed that performance of the object detection models of the day-night joint training is satisfying. Specifically, the average result of our models was 0.98 mAP (mean average precision) in the animal image detection and 88\% accuracy in the animal video classification. One-stage YOLOv5m achieved the best recognition accuracy. With the help of AI technology, ecologists can extract information from masses of imagery potentially quickly and efficiently, saving much time.},
  langid = {english},
  file = {/Users/jk/Zotero/storage/BVJ258D6/Tan et al. - 2022 - Animal Detection and Classification from Camera Trap Images Using Different Mainstream Object Detect.pdf}
}

@inproceedings{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  date = {2019-05-24},
  pages = {6105--6114},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/tan19a.html},
  urldate = {2025-06-02},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/jk/Zotero/storage/879X2CG2/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf}
}

@online{velezChoosingAppropriatePlatform2022,
  title = {Choosing an {{Appropriate Platform}} and {{Workflow}} for {{Processing Camera Trap Data}} Using {{Artificial Intelligence}}},
  author = {Vélez, Juliana and Castiblanco-Camacho, Paula J. and Tabak, Michael A. and Chalmers, Carl and Fergus, Paul and Fieberg, John},
  date = {2022-02-04},
  eprint = {2202.02283},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.02283},
  url = {http://arxiv.org/abs/2202.02283},
  urldate = {2025-03-16},
  abstract = {Camera traps have transformed how ecologists study wildlife species distributions, activity patterns, and interspecific interactions. Although camera traps provide a cost-effective method for monitoring species, the time required for data processing can limit survey efficiency. Thus, the potential of Artificial Intelligence (AI), specifically Deep Learning (DL), to process camera-trap data has gained considerable attention. Using DL for these applications involves training algorithms, such as Convolutional Neural Networks (CNNs), to automatically detect objects and classify species. To overcome technical challenges associated with training CNNs, several research communities have recently developed platforms that incorporate DL in easy-to-use interfaces. We review key characteristics of four AI-powered platforms -- Wildlife Insights (WI), MegaDetector (MD), Machine Learning for Wildlife Image Classification (MLWIC2), and Conservation AI -- including data management tools and AI features. We also provide R code in an open-source GitBook, to demonstrate how users can evaluate model performance, and incorporate AI output in semi-automated workflows. We found that species classifications from WI and MLWIC2 generally had low recall values (animals that were present in the images often were not classified to the correct species). Yet, the precision of WI and MLWIC2 classifications for some species was high (i.e., when classifications were made, they were generally accurate). MD, which classifies images using broader categories (e.g., "blank" or "animal"), also performed well. Thus, we conclude that, although species classifiers were not accurate enough to automate image processing, DL could be used to improve efficiencies by accepting classifications with high confidence values for certain species or by filtering images containing blanks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jk/Zotero/storage/MB59NJDB/Vélez et al. - 2022 - Choosing an Appropriate Platform and Workflow for Processing Camera Trap Data using Artificial Intel.pdf}
}

@online{wangTemporalSegmentNetworks2016,
  title = {Temporal {{Segment Networks}}: {{Towards Good Practices}} for {{Deep Action Recognition}}},
  shorttitle = {Temporal {{Segment Networks}}},
  author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Gool, Luc Van},
  date = {2016-08-02},
  eprint = {1608.00859},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.00859},
  url = {http://arxiv.org/abs/1608.00859},
  urldate = {2025-03-21},
  abstract = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( \$ 69.4\textbackslash\% \$) and UCF101 (\$ 94.2\textbackslash\% \$). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.},
  pubstate = {prepublished},
  keywords = {action recognition,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jk/Zotero/storage/PPXY4X5L/Wang et al. - 2016 - Temporal Segment Networks Towards Good Practices for Deep Action Recognition.pdf;/Users/jk/Zotero/storage/J5AFQPGD/1608.html}
}

@article{wearnCAMERATRAPPINGPAGE2,
  title = {{{CAMERA-TRAPPING PAGE}} 2},
  author = {Wearn, Oliver R and Glover-Kapfer, Paul},
  langid = {english},
  file = {/Users/jk/Zotero/storage/ALC8FECH/Wearn and Glover-Kapfer - CAMERA-TRAPPING PAGE 2.pdf}
}

@article{zotinANIMALDETECTIONUSING2019,
  title = {{{ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS}}},
  author = {Zotin, A. G. and Proskurin, A. V.},
  date = {2019-05-09},
  journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XLII-2-W12},
  pages = {249--257},
  publisher = {Copernicus GmbH},
  issn = {1682-1750},
  doi = {10.5194/isprs-archives-XLII-2-W12-249-2019},
  url = {https://isprs-archives.copernicus.org/articles/XLII-2-W12/249/2019/isprs-archives-XLII-2-W12-249-2019.html},
  urldate = {2025-03-16},
  abstract = {Camera traps providing enormous number of images during a season help to observe remotely animals in the wild. However, analysis of such image collection manually is impossible. In this research, we develop a method for automatic animal detection based on background modeling of scene under complex shooting. First, we design a fast algorithm for image selection without motions. Second, the images are processed by modified Multi-Scale Retinex algorithm in order to align uneven illumination. Finally, background is subtracted from incoming image using adaptive threshold. A threshold value is adjusted by saliency map, which is calculated using pyramid consisting of the original image and images modified by MSR algorithm. Proposed method allows to achieve high estimators of animals detection.},
  eventtitle = {International {{Workshop}} on “{{Photogrammetric}} and {{Computer Vision Techniques}} for {{Video Surveillance}}, {{Biometrics}} and {{Biomedicine}}” ({{Volume XLII-2}}/{{W12}}) - 13\&ndash;15 {{May}} 2019, {{Moscow}}, {{Russia}}},
  langid = {english},
  keywords = {Animal Detection,Background Modeling,Camera Traps,MSR Algorithm},
  file = {/Users/jk/Zotero/storage/LQHGNGWC/Zotin and Proskurin - 2019 - ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS.pdf}
}
