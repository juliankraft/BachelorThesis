@report{aegerterMonitoringKleinmustelidenSchlaefern2019,
  title = {Monitoring von Kleinmusteliden, Schläfern und anderen Kleinsäugern : Weiterentwicklung der Nachweismethoden mit Fotofalle},
  shorttitle = {Monitoring von Kleinmusteliden, Schläfern und anderen Kleinsäugern},
  author = {Aegerter, Silvio},
  date = {2019},
  institution = {ZHAW Zürcher Hochschule für Angewandte Wissenschaften},
  doi = {10.21256/zhaw-19209},
  abstract = {Viele Kleinsäuger der Schweiz sind nur schwer direkt nachzuweisen, weshalb allgemein wenig über ihre Verbreitung und deren Bestandesänderungen bekannt ist. Für den indirekten Nachweis hat sich in den letzten Jahren der Gebrauch von Spurentunnel bewährt. Eine vielversprechende Alternative bieten Nachweismethoden mit Fotofallen. Um das ganze Potential dieser Methoden ausschöpfen zu können, besteht jedoch noch Optimierungsbedarf. Ziel dieser Arbeit war die Weiterentwicklung und Optimierung der Fotofallenbox «Mostela». Hauptaugenmerk lag dabei auf der Verbesserung der Handhabung, sowie der Verbesserung der Bildqualität zur zuverlässigen Bestimmung von Kleinsäugern. Bisher wurden in der Mostela lediglich Infrarotblitzfotofallen verwendet. Die Aufnahmen dieser Kameras sind monochrom. Weil für die Unterscheidung einiger Kleinsäuger die Fellfärbung ent-scheidend ist, wurde zur Erhöhung der Bildqualität erstmals Weissblitzfotofallen zur Aufnahme von Farbfotos getestet. Zur weiteren Verbesserung der Bildqualität konnte in Feldversuchen verschiedene Modifikationen zur Kontrast- und Belichtungsoptimierung erprobt werden. Durch Umbau der Weissblitzfotofallen wurde versucht, deren Fokus und Bildwinkel so anzupassen, dass sie auch in einer kleineren Bauweise eingesetzt werden können. Unter Verwendung verschiedener Baumaterialien wurde versucht, kompaktere Prototypen in besonders handlicher Bauweise anzufertigen. Durch den Austausch der Objektive gelang es, den Fokus und Bildwinkel der verwendeten Fotofallen soweit anzupassen, dass die gebauten Prototypen nur noch halb so lang sind wie die Mostela. Der am besten gelungene Prototyp kann zudem für Transport und Lagerung in seine Einzelteile zerlegt werden. Da die Prototypen nur noch einen Bruchteil der Mostela wiegen, wird die zerlegbare Kiste rucksacktauglich und kann so auch problemlos an entlegenen Orten eingesetzt werden. Durch Verwendung von Farbfotos und Anpassungen der Bauweise konnte die Bildqualität und somit die Bestimmbarkeit von Kleinsäuger erhöht werden. In den Feldversuchen zeigte sich, dass durch eine geeignete Hintergrundwahl und die Verwendung eines passenden Blitzdiffusors, die Bestimmbarkeit weiter verbessern werden kann. Um letztendlich das ganze Potential der Nachweismethode aus-schöpfen zu können, wird in Zukunft neben der Handhabung und Bildqualität auch die Effizienz der Bildauswertung verbessert werden müssen.},
  langid = {ngerman},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/BX9E8YQV/Aegerter - 2019 - Monitoring von Kleinmusteliden, Schläfern und anderen Kleinsäugern  Weiterentwicklung der Nachweism.pdf}
}

@online{anevins1CameraTrapMethod2025,
  title = {Camera Trap Method Effectively Identifies Small Mammal Species in Forested Habitats},
  author = {{anevins1}},
  date = {2025-05-20T22:29:00+00:00},
  url = {https://journal.wildlife.ca.gov/2025/05/20/camera-trap-method-effectively-identifies-small-mammal-species-in-forested-habitats/},
  urldate = {2025-06-19},
  abstract = {FULL RESEARCH ARTICLE Barbara Clucas* and Sydney L. McCluskey California State Polytechnic University, Humboldt, Department of Wildlife, 1 Harpst Street, Arcata, CA 95521, USA~ (BC) (SLM) *Cor…},
  langid = {american},
  organization = {California Fish and Wildlife Scientific Journal},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/L5B9WRBU/camera-trap-method-effectively-identifies-small-mammal-species-in-forested-habitats.html}
}

@report{bafuListeNationalPrioritaren2019,
  title = {Liste Der {{National Prioritären Arten}} Und {{Lebensräume}}. {{In}} Der {{Schweiz}} Zu Fördernde Prioritäre {{Arten}} Und {{Lebensräume}}},
  author = {{BAFU}},
  date = {2019},
  number = {Umwelt-Vollzug Nr. 1709},
  pages = {99},
  institution = {Bundesamt für Umwelt},
  location = {Bern},
  url = {https://www.bafu.admin.ch/uv-1709-d},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/KX2KL5BV/Bundesamt für Umwelt (BAFU) - 2019 - Liste der National Prioritären Arten und Lebensräume. In der Schweiz zu fördernde prioritäre Arten u.pdf}
}

@article{beaudrotStandardizedAssessmentBiodiversity2016,
  title = {Standardized {{Assessment}} of {{Biodiversity Trends}} in {{Tropical Forest Protected Areas}}: {{The End Is Not}} in {{Sight}}},
  shorttitle = {Standardized {{Assessment}} of {{Biodiversity Trends}} in {{Tropical Forest Protected Areas}}},
  author = {Beaudrot, Lydia and Ahumada, Jorge A. and O'Brien, Timothy and Alvarez-Loayza, Patricia and Boekee, Kelly and Campos-Arceiz, Ahimsa and Eichberg, David and Espinosa, Santiago and Fegraus, Eric and Fletcher, Christine and Gajapersad, Krisna and Hallam, Chris and Hurtado, Johanna and Jansen, Patrick A. and Kumar, Amit and Larney, Eileen and Lima, Marcela Guimarães Moreira and Mahony, Colin and Martin, Emanuel H. and McWilliam, Alex and Mugerwa, Badru and Ndoundou-Hockemba, Mireille and Razafimahaimodison, Jean Claude and Romero-Saltos, Hugo and Rovero, Francesco and Salvador, Julia and Santos, Fernanda and Sheil, Douglas and Spironello, Wilson R. and Willig, Michael R. and Winarni, Nurul L. and Zvoleff, Alex and Andelman, Sandy J.},
  date = {2016-01-19},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {14},
  number = {1},
  pages = {e1002357},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002357},
  abstract = {Extinction rates in the Anthropocene are three orders of magnitude higher than background and disproportionately occur in the tropics, home of half the world’s species. Despite global efforts to combat tropical species extinctions, lack of high-quality, objective information on tropical biodiversity has hampered quantitative evaluation of conservation strategies. In particular, the scarcity of population-level monitoring in tropical forests has stymied assessment of biodiversity outcomes, such as the status and trends of animal populations in protected areas. Here, we evaluate occupancy trends for 511 populations of terrestrial mammals and birds, representing 244 species from 15 tropical forest protected areas on three continents. For the first time to our knowledge, we use annual surveys from tropical forests worldwide that employ a standardized camera trapping protocol, and we compute data analytics that correct for imperfect detection. We found that occupancy declined in 22\%, increased in 17\%, and exhibited no change in 22\% of populations during the last 3–8 years, while 39\% of populations were detected too infrequently to assess occupancy changes. Despite extensive variability in occupancy trends, these 15 tropical protected areas have not exhibited systematic declines in biodiversity (i.e., occupancy, richness, or evenness) at the community level. Our results differ from reports of widespread biodiversity declines based on aggregated secondary data and expert opinion and suggest less extreme deterioration in tropical forest protected areas. We simultaneously fill an important conservation data gap and demonstrate the value of large-scale monitoring infrastructure and powerful analytics, which can be scaled to incorporate additional sites, ecosystems, and monitoring methods. In an era of catastrophic biodiversity loss, robust indicators produced from standardized monitoring infrastructure are critical to accurately assess population outcomes and identify conservation strategies that can avert biodiversity collapse.},
  langid = {english},
  keywords = {Biodiversity,Birds,Conservation science,Forests,Mammals,Species extinction,Tropical forests,Vertebrates},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/UJTDFLYN/Beaudrot et al. - 2016 - Standardized Assessment of Biodiversity Trends in Tropical Forest Protected Areas The End Is Not in.pdf}
}

@online{beeryEfficientPipelineCamera2019,
  title = {Efficient {{Pipeline}} for {{Camera Trap Image Review}}},
  author = {Beery, Sara and Morris, Dan and Yang, Siyu},
  date = {2019-07-15},
  eprint = {1907.06772},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.06772},
  abstract = {Biologists all over the world use camera traps to monitor biodiversity and wildlife population density. The computer vision community has been making strides towards automating the species classification challenge in camera traps, but it has proven difficult to to apply models trained in one region to images collected in different geographic areas. In some cases, accuracy falls off catastrophically in new region, due to both changes in background and the presence of previously-unseen species. We propose a pipeline that takes advantage of a pre-trained general animal detector and a smaller set of labeled images to train a classification model that can efficiently achieve accurate results in a new region.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/3U5GEMPZ/Beery et al. - 2019 - Efficient Pipeline for Camera Trap Image Review.pdf;/Volumes/ExSSD/UserData/Zotero/storage/B7LQXLJL/1907.html}
}

@online{beeryRecognitionTerraIncognita2018,
  title = {Recognition in {{Terra Incognita}}},
  author = {Beery, Sara and family=Horn, given=Grant, prefix=van, useprefix=false and Perona, Pietro},
  date = {2018-07-25},
  eprint = {1807.04975},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1807.04975},
  abstract = {It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Populations and Evolution},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/7ZXRYK8G/Beery et al. - 2018 - Recognition in Terra Incognita.pdf;/Volumes/ExSSD/UserData/Zotero/storage/EPFWEW9V/1807.html}
}

@article{bintaislamAnimalSpeciesRecognition2023,
  title = {Animal {{Species Recognition}} with {{Deep Convolutional Neural Networks}} from {{Ecological Camera Trap Images}}},
  author = {Binta Islam, Sazida and Valles, Damian and Hibbitts, Toby J. and Ryberg, Wade A. and Walkup, Danielle K. and Forstner, Michael R. J.},
  date = {2023-01},
  journaltitle = {Animals},
  volume = {13},
  number = {9},
  pages = {1526},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-2615},
  doi = {10.3390/ani13091526},
  abstract = {Accurate identification of animal species is necessary to understand biodiversity richness, monitor endangered species, and study the impact of climate change on species distribution within a specific region. Camera traps represent a passive monitoring technique that generates millions of ecological images. The vast numbers of images drive automated ecological analysis as essential, given that manual assessment of large datasets is laborious, time-consuming, and expensive. Deep learning networks have been advanced in the last few years to solve object and species identification tasks in the computer vision domain, providing state-of-the-art results. In our work, we trained and tested machine learning models to classify three animal groups (snakes, lizards, and toads) from camera trap images. We experimented with two pretrained models, VGG16 and ResNet50, and a self-trained convolutional neural network (CNN-1) with varying CNN layers and augmentation parameters. For multiclassification, CNN-1 achieved 72\% accuracy, whereas VGG16 reached 87\%, and ResNet50 attained 86\% accuracy. These results demonstrate that the transfer learning approach outperforms the self-trained model performance. The models showed promising results in identifying species, especially those with challenging body sizes and vegetation.},
  issue = {9},
  langid = {english},
  keywords = {camera trap,convolutional neural network,deep learning,endangered species,image augmentation,image classification,lizard,machine learning,snake,toad},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/IFIQBMSW/Binta Islam et al. - 2023 - Animal Species Recognition with Deep Convolutional Neural Networks from Ecological Camera Trap Image.pdf}
}

@article{bohnerSemiautomaticWorkflowProcess2023,
  title = {A Semi-Automatic Workflow to Process Images from Small Mammal Camera Traps},
  author = {Böhner, Hanna and Kleiven, Eivind Flittie and Ims, Rolf Anker and Soininen, Eeva M.},
  date = {2023-09-01},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {76},
  pages = {102150},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2023.102150},
  abstract = {Camera traps have become popular for monitoring biodiversity, but the huge amounts of image data that arise from camera trap monitoring represent a challenge and artificial intelligence is increasingly used to automatically classify large image data sets. However, it is still challenging to combine automatic classification with other steps and tools needed for efficient, quality-assured and adaptive processing of camera trap images in long-term monitoring programs. Here we propose a semi-automatic workflow to process images from small mammal cameras that combines all necessary steps from downloading camera trap images in the field to a quality checked data set ready to be used in ecological analyses. The workflow is implemented in R and includes (1) managing raw images, (2) automatic image classification, (3) quality check of automatic image labels, as well as the possibilities to (4) retrain the model with new images and to (5) manually review subsets of images to correct image labels. We illustrate the application of this workflow for the development of a new monitoring program of an Arctic small mammal community. We first trained a classification model for the specific small mammal community based on images from an initial set of camera traps. As the monitoring program evolved, the classification model was retrained with a small subset of images from new camera traps. This case study highlights the importance of model retraining in adaptive monitoring programs based on camera traps as this step in the workflow increases model performance and substantially decreases the total time needed for manually reviewing images and correcting image labels. We provide all R scripts to make the workflow accessible to other ecologists.},
  keywords = {Adaptive monitoring,Automatic image classification,Camera trap,Data processing,Deep learning,Rodent},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/YWP8GERF/Böhner et al. - 2023 - A semi-automatic workflow to process images from small mammal camera traps.pdf;/Volumes/ExSSD/UserData/Zotero/storage/LA2CIZIK/S1574954123001796.html}
}

@article{bothmannAutomatedWildlifeImage2023,
  title = {Automated Wildlife Image Classification: {{An}} Active Learning Tool for Ecological Applications},
  shorttitle = {Automated Wildlife Image Classification},
  author = {Bothmann, Ludwig and Wimmer, Lisa and Charrakh, Omid and Weber, Tobias and Edelhoff, Hendrik and Peters, Wibke and Nguyen, Hien and Benjamin, Caryl and Menzel, Annette},
  date = {2023-11},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {77},
  eprint = {2303.15823},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {102231},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2023.102231},
  abstract = {Wildlife camera trap images are being used extensively to investigate animal abundance, habitat associations, and behavior, which is complicated by the fact that experts must first classify the images manually. Artificial intelligence systems can take over this task but usually need a large number of already-labeled training images to achieve sufficient performance. This requirement necessitates human expert labor and poses a particular challenge for projects with few cameras or short durations. We propose a label-efficient learning strategy that enables researchers with small or medium-sized image databases to leverage the potential of modern machine learning, thus freeing crucial resources for subsequent analyses. Our methodological proposal is two-fold: (1) We improve current strategies of combining object detection and image classification by tuning the hyperparameters of both models. (2) We provide an active learning (AL) system that allows training deep learning models very efficiently in terms of required human-labeled training images. We supply a software package that enables researchers to use these methods directly and thereby ensure the broad applicability of the proposed framework in ecological practice. We show that our tuning strategy improves predictive performance. We demonstrate how the AL pipeline reduces the amount of pre-labeled data needed to achieve a specific predictive performance and that it is especially valuable for improving out-of-sample predictive performance. We conclude that the combination of tuning and AL increases predictive performance substantially. Furthermore, we argue that our work can broadly impact the community through the ready-to-use software package provided. Finally, the publication of our models tailored to European wildlife data enriches existing model bases mostly trained on data from Africa and North America.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Applications}
}

@inproceedings{brodersenBalancedAccuracyIts2010,
  title = {The {{Balanced Accuracy}} and {{Its Posterior Distribution}}},
  booktitle = {2010 20th {{International Conference}} on {{Pattern Recognition}}},
  author = {Brodersen, Kay Henning and Ong, Cheng Soon and Stephan, Klaas Enno and Buhmann, Joachim M.},
  date = {2010-08},
  pages = {3121--3124},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2010.764},
  abstract = {Evaluating the performance of a classification algorithm critically requires a measure of the degree to which unseen examples have been identified with their correct class labels. In practice, generalizability is frequently estimated by averaging the accuracies obtained on individual cross-validation folds. This procedure, however, is problematic in two ways. First, it does not allow for the derivation of meaningful confidence intervals. Second, it leads to an optimistic estimate when a biased classifier is tested on an imbalanced dataset. We show that both problems can be overcome by replacing the conventional point estimate of accuracy by an estimate of the posterior distribution of the balanced accuracy.},
  eventtitle = {2010 20th {{International Conference}} on {{Pattern Recognition}}},
  keywords = {Accuracy,Approximation algorithms,bias,class imbalance,classification performance,generalizability,Inference algorithms,Machine learning,Prediction algorithms,Probabilistic logic,Training},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/EF3K38HV/Brodersen et al. - 2010 - The Balanced Accuracy and Its Posterior Distribution.pdf}
}

@book{brondizioGlobalAssessmentReport2019,
  title = {The Global Assessment Report of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services},
  editor = {Brondízio, Eduardo Sonnewend and Settele, Josef and Díaz, Sandra and Ngo, Hien Thu},
  date = {2019},
  publisher = {{Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES)}},
  location = {Bonn},
  isbn = {978-3-947851-20-1},
  langid = {english},
  annotation = {OCLC: 1336011247},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/UK6DXMJ3/Brondízio et al. - 2019 - The global assessment report of the intergovernmen.pdf}
}

@article{bujangElaborationSampleSize2024,
  title = {An Elaboration on Sample Size Determination for Correlations Based on Effect Sizes and Confidence Interval Width: A Guide for Researchers},
  shorttitle = {An Elaboration on Sample Size Determination for Correlations Based on Effect Sizes and Confidence Interval Width},
  author = {Bujang, Mohamad Adam},
  date = {2024-05-02},
  journaltitle = {Restorative Dentistry \& Endodontics},
  shortjournal = {Restor Dent Endod},
  volume = {49},
  number = {2},
  eprint = {38841381},
  eprinttype = {pubmed},
  pages = {e21},
  issn = {2234-7658},
  doi = {10.5395/rde.2024.49.e21},
  abstract = {Objectives This paper aims to serve as a useful guide for sample size determination for various correlation analyses that are based on effect sizes and confidence interval width. Materials and Methods Sample size determinations are calculated for Pearson’s correlation, Spearman’s rank correlation, and Kendall’s Tau-b correlation. Examples of sample size statements and their justification are also included. Results Using the same effect sizes, there are differences between the sample size determination of the 3 statistical tests. Based on an empirical calculation, a minimum sample size of 149 is usually adequate for performing both parametric and non-parametric correlation analysis to determine at least a moderate to an excellent degree of correlation with acceptable confidence interval width. Conclusions Determining data assumption(s) is one of the challenges to offering a valid technique to estimate the required sample size for correlation analyses. Sample size tables are provided and these will help researchers to estimate a minimum sample size requirement based on correlation analyses.},
  pmcid = {PMC11148401},
  keywords = {statistics},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/SYZZIMB7/Bujang - 2024 - An elaboration on sample size determination for correlations based on effect sizes and confidence in.pdf}
}

@article{cardinaleBiodiversityLossIts2012,
  title = {Biodiversity Loss and Its Impact on Humanity},
  author = {Cardinale, Bradley J. and Duffy, J. Emmett and Gonzalez, Andrew and Hooper, David U. and Perrings, Charles and Venail, Patrick and Narwani, Anita and Mace, Georgina M. and Tilman, David and Wardle, David A. and Kinzig, Ann P. and Daily, Gretchen C. and Loreau, Michel and Grace, James B. and Larigauderie, Anne and Srivastava, Diane S. and Naeem, Shahid},
  date = {2012-06},
  journaltitle = {Nature},
  volume = {486},
  number = {7401},
  pages = {59--67},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11148},
  abstract = {Two decades ago the first Earth Summit raised the question of how biological diversity loss alters ecosystem functioning and affects humanity; this Review looks at the progress made towards answering this question.},
  langid = {english},
  keywords = {Biodiversity,Ecosystem services},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZC2YLYKA/Cardinale et al. - 2012 - Biodiversity loss and its impact on humanity.pdf}
}

@inproceedings{chenDeepConvolutionalNeural2014,
  title = {Deep Convolutional Neural Network Based Species Recognition for Wild Animal Monitoring},
  booktitle = {2014 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Chen, Guobin and Han, Tony X. and He, Zhihai and Kays, Roland and Forrester, Tavis},
  date = {2014-10},
  pages = {858--862},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2014.7025172},
  abstract = {We proposed a novel deep convolutional neural network based species recognition algorithm for wild animal classification on very challenging camera-trap imagery data. The imagery data were captured with motion triggered camera trap and were segmented automatically using the state of the art graph-cut algorithm. The moving foreground is selected as the region of interests and is fed to the proposed species recognition algorithm. For the comparison purpose, we use the traditional bag of visual words model as the baseline species recognition algorithm. It is clear that the proposed deep convolutional neural network based species recognition achieves superior performance. To our best knowledge, this is the first attempt to the fully automatic computer vision based species recognition on the real camera-trap images. We also collected and annotated a standard camera-trap dataset of 20 species common in North America, which contains 14, 346 training images and 9, 530 testing images, and is available to public for evaluation and benchmark purpose.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  keywords = {Birds,deep convolutional neural networks,image classification,Image recognition,large scale learning,Sociology,Species recognition,Statistics,Visualization,wild animal monitor,Wildlife},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/HZZECAVY/Chen et al. - 2014 - Deep convolutional neural network based species recognition for wild animal monitoring.pdf}
}

@article{clucasCameraTrapMethod2025,
  title = {Camera Trap Method Effectively Identifies Small Mammal Species in Forested Habitats},
  author = {Clucas, Barbara and McCluskey, Sydney L.},
  date = {2025-05-21},
  journaltitle = {California Fish and Wildlife Journal},
  volume = {111},
  number = {2},
  issn = {2689-4203, 2689-419X},
  doi = {10.51492/cfwj.111.8},
  abstract = {Effective survey methods to detect small mammal species are often needed to develop conservation and management plans in forested ecosystems. The ability to use non-invasive methods to identify small mammal species in the field is particularly useful as live trapping can be time consuming and potentially harmful to the study species. We tested a camera trap method in a coastal redwood (Sequoia sempervirens) forest for small mammals, originally designed by Gracanin et al. (2019) and called the “selfie trap”, that uses a camera trap with a modified lens in a baited PVC tube. We determined if we could use this camera trap set-up on the ground to accurately identify small mammals to species to assess species diversity in a forested ecosystem as well as if it could withstand disturbance from larger mammals (e.g., bears). We surveyed for small mammals in areas of old-growth and second-growth coastal redwood forests in northwestern California. We detected 10 small mammal species and were able to identify most individuals to species including squirrel, chipmunk, mice, woodrat, shrew, vole and mole species. This camera trap set up also detected approximately 77\% of small mammal species known to potentially occur in the area. Moreover, although larger mammals could interact with the camera trap set up, their disturbance was limited to when they were interacting with the trap, and the bait and camera set-up remained functional for subsequent small mammal detections. Thus, this method could be used instead of live trapping in complex forested ecosystems to effectively determine small mammal species presence, diversity, and activity levels, avoiding disturbance from large mammals.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/7DTIHR2J/Clucas and McCluskey - 2025 - Camera trap method effectively identifies small mammal species in forested habitats.pdf}
}

@article{computerscienceandengineeringsrminstituteofscienceandtechnologykattankulathur603203india.ImageClassificationUsing2019,
  title = {Image {{Classification}} Using a {{Hybrid LSTM-CNN Deep Neural Network}}},
  author = {{Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India.} and {Aditi*} and Nagda, Mayank Kumar and {Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India.} and Poovammal*, E. and {Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India.}},
  date = {2019-08-30},
  journaltitle = {International Journal of Engineering and Advanced Technology},
  shortjournal = {IJEAT},
  volume = {8},
  number = {6},
  pages = {1342--1348},
  issn = {22498958},
  doi = {10.35940/ijeat.F8602.088619},
  abstract = {This work elaborates on the integration of the rudimentary Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM), resulting in a new paradigm in the well-explored field of image classification. LSTM is one kind of Recurrent Neural Network (RNN) which has the potential to memorize long-term dependencies. It was observed that LSTMs are able to complement the feature extraction ability of CNN when used in a layered order. LSTMs have the capacity to selectively remember patterns for a long duration of time and CNNs are able to extract the important features out of it. This LSTM-CNN layered structure, when used for image classification, has an edge over conventional CNN classifier. The model which has been proposed is based on the sets of Artificial Neural Network like Recurrent and Convolutional neural network; hence this model is robust and suitable to a wide spectrum of classification tasks. To validate these results, we have tested our model on two standard datasets. The results have been compared with other classifiers to establish the significance of our proposed model.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/LNJB3VWS/Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur 603203, India. et al. - 2019 - Image Classification using a Hybrid LSTM-CNN Deep Neural Network.pdf}
}

@article{cordierCameraTrapResearch2022,
  title = {Camera Trap Research in {{Africa}}: {{A}} Systematic Review to Show Trends in Wildlife Monitoring and Its Value as a Research Tool},
  shorttitle = {Camera Trap Research in {{Africa}}},
  author = {Cordier, Craig P. and Ehlers Smith, David A. and Ehlers Smith, Yvette and Downs, Colleen T.},
  date = {2022-12-01},
  journaltitle = {Global Ecology and Conservation},
  shortjournal = {Global Ecology and Conservation},
  volume = {40},
  pages = {e02326},
  issn = {2351-9894},
  doi = {10.1016/j.gecco.2022.e02326},
  abstract = {Camera traps have been used increasingly as a research tool to monitor wildlife globally, and have become more advanced, thereby improving their performance and lowering costs. Their use has allowed researchers to study a range of species, including rare and elusive species, particularly in remote areas, in a non-invasive, reliable and cost-effective way. In this review, we sought to document the camera trapping research on terrestrial wildlife conducted in Africa, identifying countries and habitat types of focus, and how these camera trap research trends in Africa could be improved in the future. Through a systematic literature search, we found 408 peer-reviewed publications using camera traps to study terrestrial wildlife in Africa, with the first being in 2005 and up to 2021. Although camera trap studies were conducted in 38 African countries, most were in South Africa (28.9~\%). Most studies assessed the occupancy of species (41.4~\%). The studies covered a range of taxa, with mammals being the most popular. The majority of research focussed on large carnivores (24.8~\%), with a particular focus on leopards (Panthera pardus) (60 studies). Most studies (43.9~\%) focused on a single species, with forests (174 studies, 42.6~\%) and savannah/bushveld (145 studies, 35.5~\%) being the habitat type of focus. There was also a strong preference for camera trap studies to be conducted in protected areas (68.9~\%). The camera trap methods used varied considerably between studies, which included: the number of camera trap stations, survey length, trap effort, camera trap make and model, camera trap flash type, interval delay, camera multishot, height of camera trap placement, camera trap layout method and whether the camera trap was baited. These variations are expected because of the difference in research goals posed by each study. However, studies with similar objectives and/or focus still display a clear lack of standardisation (studies do not conform to a specific standard), which could negatively impact the results obtained, as inappropriate camera trap protocols could affect the detectability of certain species. Future camera trap studies will hopefully extend to countries and taxa that have received little attention, with further research informing appropriate conservation strategies that could reduce the threats to biodiversity.},
  keywords = {Camera traps,Forest,Large carnivore,Monitor wildlife,Protocols,Terrestrial wildlife},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/55KML7HA/Cordier et al. - 2022 - Camera trap research in Africa A systematic review to show trends in wildlife monitoring and its va.pdf;/Volumes/ExSSD/UserData/Zotero/storage/CJQUR2B6/S2351989422003286.html}
}

@article{delisleNextGenerationCameraTrapping2021,
  title = {Next-{{Generation Camera Trapping}}: {{Systematic Review}} of {{Historic Trends Suggests Keys}} to {{Expanded Research Applications}} in {{Ecology}} and {{Conservation}}},
  shorttitle = {Next-{{Generation Camera Trapping}}},
  author = {Delisle, Zackary J. and Flaherty, Elizabeth A. and Nobbe, Mackenzie R. and Wzientek, Cole M. and Swihart, Robert K.},
  date = {2021-02-26},
  journaltitle = {Frontiers in Ecology and Evolution},
  shortjournal = {Front. Ecol. Evol.},
  volume = {9},
  publisher = {Frontiers},
  issn = {2296-701X},
  doi = {10.3389/fevo.2021.617996},
  abstract = {Camera trapping is an effective noninvasive method for collecting data on wildlife species to address questions of ecological and conservation interest. We reviewed 2,167 camera trap (CT) articles from 1994 to 2020. Through the lens of technological diffusion, we assessed trends in: (1) CT adoption measured by published research output, (2) topic, taxonomic, and geographic diversification and composition of CT applications, and (3) sampling effort, spatial extent, and temporal duration of CT studies. Annual publications of CT articles have grown 81-fold since 1994, increasing at a rate of 1.26 (SE = 0.068) per year since 2005, but with decelerating growth since 2017. Topic, taxonomic, and geographic richness of CT studies increased to encompass 100\% of topics, 59.4\% of ecoregions, and 6.4\% of terrestrial vertebrates. However, declines in per article rates of accretion and plateaus in Shannon’s H for topics and major taxa studied suggest upper limits to further diversification of CT research as currently practiced. Notable compositional changes of topics included a decrease in capture-recapture, recent decrease in spatial-capture-recapture, and increases in occupancy, interspecific interactions, and automated image classification. Mammals were the dominant taxon studied; within mammalian orders carnivores exhibited a unimodal peak whereas primates, rodents and lagomorphs steadily increased. Among biogeographic realms we observed decreases in Oceania and Nearctic, increases in Afrotropic and Palearctic, and unimodal peaks for Indomalayan and Neotropic. Camera days, temporal extent, and area sampled increased, with much greater rates for the 0.90 quantile of CT studies compared to the median. Next-generation CT studies are poised to expand knowledge valuable to wildlife ecology and conservation by posing previously infeasible questions at unprecedented spatiotemporal scales, on a greater array of species, and in a wider variety of environments. Converting potential into broad-based application will require transferable models of automated image classification, and data sharing among users across multiple platforms in a coordinated manner. Further taxonomic diversification likely will require technological modifications that permit more efficient sampling of smaller species and adoption of recent improvements in modeling of unmarked populations. Environmental diversification can benefit from engineering solutions that expand ease of CT sampling in traditionally challenging sites.},
  langid = {english},
  keywords = {Camera trap,diversity,ecoregions,image classification,Occupancy,Population attributes,Technological diffusion,wildlife},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/W8RLHMQY/Delisle et al. - 2021 - Next-Generation Camera Trapping Systematic Review of Historic Trends Suggests Keys to Expanded Rese.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/J72A7EIB/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/DB6DBZ2X/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/Volumes/ExSSD/UserData/Zotero/storage/N2APVDTF/2010.html}
}

@software{falconPyTorchLightning2025,
  title = {{{PyTorch Lightning}}},
  author = {Falcon, William and {The PyTorch Lightning Team}},
  date = {2025-04-25},
  doi = {10.5281/zenodo.15284694},
  abstract = {The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.},
  organization = {Zenodo},
  version = {2.5.1.post0},
  keywords = {artificial intelligence,deep learning,machine learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/H9NQRLLE/15284694.html}
}

@article{gaoCNNBiLSTMComplexEnvironmentOriented2023,
  title = {{{CNN-Bi-LSTM}}: {{A Complex Environment-Oriented Cattle Behavior Classification Network Based}} on the {{Fusion}} of {{CNN}} and {{Bi-LSTM}}},
  shorttitle = {{{CNN-Bi-LSTM}}},
  author = {Gao, Guohong and Wang, Chengchao and Wang, Jianping and Lv, Yingying and Li, Qian and Ma, Yuxin and Zhang, Xueyan and Li, Zhiyu and Chen, Guanglan},
  date = {2023-01},
  journaltitle = {Sensors},
  volume = {23},
  number = {18},
  pages = {7714},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s23187714},
  abstract = {Cattle behavior classification technology holds a crucial position within the realm of smart cattle farming. Addressing the requisites of cattle behavior classification in the agricultural sector, this paper presents a novel cattle behavior classification network tailored for intricate environments. This network amalgamates the capabilities of CNN and Bi-LSTM. Initially, a data collection method is devised within an authentic farm setting, followed by the delineation of eight fundamental cattle behaviors. The foundational step involves utilizing VGG16 as the cornerstone of the CNN network, thereby extracting spatial feature vectors from each video data sequence. Subsequently, these features are channeled into a Bi-LSTM classification model, adept at unearthing semantic insights from temporal data in both directions. This process ensures precise recognition and categorization of cattle behaviors. To validate the model’s efficacy, ablation experiments, generalization effect assessments, and comparative analyses under consistent experimental conditions are performed. These investigations, involving module replacements within the classification model and comprehensive analysis of ablation experiments, affirm the model’s effectiveness. The self-constructed dataset about cattle is subjected to evaluation using cross-entropy loss, assessing the model’s generalization efficacy across diverse subjects and viewing perspectives. Classification performance accuracy is quantified through the application of a confusion matrix. Furthermore, a set of comparison experiments is conducted, involving three pertinent deep learning models: MASK-RCNN, CNN-LSTM, and EfficientNet-LSTM. The outcomes of these experiments unequivocally substantiate the superiority of the proposed model. Empirical results underscore the CNN-Bi-LSTM model’s commendable performance metrics: achieving 94.3\% accuracy, 94.2\% precision, and 93.4\% recall while navigating challenges such as varying light conditions, occlusions, and environmental influences. The objective of this study is to employ a fusion of CNN and Bi-LSTM to autonomously extract features from multimodal data, thereby addressing the challenge of classifying cattle behaviors within intricate scenes. By surpassing the constraints imposed by conventional methodologies and the analysis of single-sensor data, this approach seeks to enhance the precision and generalizability of cattle behavior classification. The consequential practical, economic, and societal implications for the agricultural sector are of considerable significance.},
  issue = {18},
  langid = {english},
  keywords = {behavior classification,Bi-LSTM,cattle,CNN},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/Y45E442K/Gao et al. - 2023 - CNN-Bi-LSTM A Complex Environment-Oriented Cattle Behavior Classification Network Based on the Fusi.pdf}
}

@article{gomezvillaAutomaticWildAnimal2017,
  title = {Towards Automatic Wild Animal Monitoring: {{Identification}} of Animal Species in Camera-Trap Images Using Very Deep Convolutional Neural Networks},
  shorttitle = {Towards Automatic Wild Animal Monitoring},
  author = {Gomez Villa, Alexander and Salazar, Augusto and Vargas, Francisco},
  date = {2017-09-01},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {41},
  pages = {24--32},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2017.07.004},
  abstract = {Non-intrusive monitoring of animals in the wild is possible using camera trapping networks. The cameras are triggered by sensors in order to disturb the animals as little as possible. This approach produces a high volume of data (in the order of thousands or millions of images) that demands laborious work to analyze both useless (incorrect detections, which are the most) and useful (images with presence of animals). In this work, we show that as soon as some obstacles are overcome, deep neural networks can cope with the problem of the automated species classification appropriately. As case of study, the most common 26 of 48 species from the Snapshot Serengeti (SSe) dataset were selected and the potential of the Very Deep Convolutional neural networks framework for the species identification task was analyzed. In the worst-case scenario (unbalanced training dataset containing empty images) the method reached 35.4\% Top-1 and 60.4\% Top-5 accuracy. For the best scenario (balanced dataset, images containing foreground animals only, and manually segmented) the accuracy reached a 88.9\% Top-1 and 98.1\% Top-5, respectively. To the best of our knowledge, this is the first published attempt on solving the automatic species recognition on the SSe dataset. In addition, a comparison with other approaches on a different dataset was carried out, showing that the architectures used in this work outperformed previous approaches. The limitations of the method, drawbacks, as well as new challenges in automatic camera-trap species classification are widely discussed.},
  keywords = {Animal species recognition,Camera-trap,Deep convolutional neural networks,Snapshot Serengeti},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/GJG7MRSM/Gomez Villa et al. - 2017 - Towards automatic wild animal monitoring Identification of animal species in camera-trap images usi.pdf;/Volumes/ExSSD/UserData/Zotero/storage/TAYW5636/Gomez Villa et al. - 2017 - Towards automatic wild animal monitoring Identification of animal species in camera-trap images usi.pdf;/Volumes/ExSSD/UserData/Zotero/storage/F67GMQXC/S1574954116302047.html}
}

@book{Goodfellow-et-al-201,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Benigo, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {http://www.deeplearningbook.org}
}

@article{grafWildlifeCampusKleineSaeugetiere2022,
  title = {Wildlife@Campus: Kleine Säugetiere im Fokus},
  author = {Graf, Roland F. and Dietrich, Adrian and Honetschläger, Nils and Kryszczuk, Krzysztof and Palmisano, Marilena and Pothier, Joël F. and Ratnaweera, Nils and Reifler-Bächtiger, Martina and Rhyner, Nicola and Treichler, Regula},
  date = {2022-07},
  langid = {ngerman},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/MXGX9DBR/Graf - Wildlife@Campus Kleine Säugetiere im Fokus.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2025-06-02},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/GUHR8ZH8/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@online{hernandezPytorchWildlifeCollaborativeDeep2024,
  title = {Pytorch-{{Wildlife}}: {{A Collaborative Deep Learning Framework}} for {{Conservation}}},
  shorttitle = {Pytorch-{{Wildlife}}},
  author = {Hernandez, Andres and Miao, Zhongqi and Vargas, Luisa and Beery, Sara and Dodhia, Rahul and Arbelaez, Pablo and Ferres, Juan M. Lavista},
  date = {2024-11-29},
  eprint = {2405.12930},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.12930},
  abstract = {The alarming decline in global biodiversity, driven by various factors, underscores the urgent need for large-scale wildlife monitoring. In response, scientists have turned to automated deep learning methods for data processing in wildlife monitoring. However, applying these advanced methods in real-world scenarios is challenging due to their complexity and the need for specialized knowledge, primarily because of technical challenges and interdisciplinary barriers. To address these challenges, we introduce Pytorch-Wildlife, an open-source deep learning platform built on PyTorch. It is designed for creating, modifying, and sharing powerful AI models. This platform emphasizes usability and accessibility, making it accessible to individuals with limited or no technical background. It also offers a modular codebase to simplify feature expansion and further development. Pytorch-Wildlife offers an intuitive, user-friendly interface, accessible through local installation or Hugging Face, for animal detection and classification in images and videos. As two real-world applications, Pytorch-Wildlife has been utilized to train animal classification models for species recognition in the Amazon Rainforest and for invasive opossum recognition in the Galapagos Islands. The Opossum model achieves 98\% accuracy, and the Amazon model has 92\% recognition accuracy for 36 animals in 90\% of the data. As Pytorch-Wildlife evolves, we aim to integrate more conservation tasks, addressing various environmental challenges. Pytorch-Wildlife is available at https://github.com/microsoft/CameraTraps.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/5VB3HY3J/Hernandez et al. - 2024 - Pytorch-Wildlife A Collaborative Deep Learning Fr.pdf;/Volumes/ExSSD/UserData/Zotero/storage/WZXRFIKM/2405.html}
}

@article{hopkinsDetectingMonitoringRodents2024,
  title = {Detecting and Monitoring Rodents Using Camera Traps and Machine Learning versus Live Trapping for Occupancy Modeling},
  author = {Hopkins, Jaran and Santos-Elizondo, Gabriel Marcelo and Villablanca, Francis},
  date = {2024-05-22},
  journaltitle = {Frontiers in Ecology and Evolution},
  shortjournal = {Front. Ecol. Evol.},
  volume = {12},
  publisher = {Frontiers},
  issn = {2296-701X},
  doi = {10.3389/fevo.2024.1359201},
  abstract = {Determining best methods to detect individuals and monitor populations that balance effort and efficiency can assist conservation and land management. This may be especially true for small, noncharismatic species, such as rodents (Rodentia), which comprise 39\% of all mammal species. Given the importance of rodents to ecosystems, and the number of listed species, we tested two commonly used detection and monitoring methods, live traps and camera traps, to determine their efficiency in rodents. An artificial-intelligence machine-learning model was developed to process the camera trap images and identify the species within them which reduced camera trapping effort. We used occupancy models to compare probability of detection and occupancy estimates for six rodent species across the two methods. Camera traps yielded greater detection probability and occupancy estimates for all six species. Live trapping yielded biasedly low estimates of occupancy, required greater effort, and had a lower probability of detection. Camera traps, aimed at the ground to capture the dorsal view of an individual, combined with machine learning provided a practical, noninvasive, and low effort solution to detecting and monitoring rodents. Thus, camera trapping with machine learning is a more sustainable and practical solution for the conservation and land management of rodents.},
  langid = {english},
  keywords = {camera trapping5,detection1,effort4,live trapping6,Machine Learning3,occupancy2,small mammel},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/I94MJNNI/Hopkins et al. - 2024 - Detecting and monitoring rodents using camera traps and machine learning versus live trapping for oc.pdf}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  date = {2017-07},
  pages = {2261--2269},
  publisher = {IEEE},
  location = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.243},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/E9WTWW4Z/Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf}
}

@online{ilseAttentionbasedDeepMultiple2018,
  title = {Attention-Based {{Deep Multiple Instance Learning}}},
  author = {Ilse, Maximilian and Tomczak, Jakub M. and Welling, Max},
  date = {2018-06-28},
  eprint = {1802.04712},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.04712},
  abstract = {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/9CJKP97Q/Ilse et al. - 2018 - Attention-based Deep Multiple Instance Learning.pdf;/Volumes/ExSSD/UserData/Zotero/storage/GIA3T2FT/1802.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2025-06-19},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/4BZGALZU/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/QAWV7JZB/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf}
}

@article{littlewoodUseNovelCamera2021,
  title = {Use of a Novel Camera Trapping Approach to Measure Small Mammal Responses to Peatland Restoration},
  author = {Littlewood, Nick A. and Hancock, Mark H. and Newey, Scott and Shackelford, Gorm and Toney, Rose},
  date = {2021-01-12},
  journaltitle = {European Journal of Wildlife Research},
  shortjournal = {Eur J Wildl Res},
  volume = {67},
  number = {1},
  pages = {12},
  issn = {1439-0574},
  doi = {10.1007/s10344-020-01449-z},
  abstract = {Small mammals, such as small rodents (Rodentia: Muroidea) and shrews (Insectivora: Soricidae), present particular challenges in camera trap surveys. Their size is often insufficient to trigger infra-red sensors, whilst resultant images may be of inadequate quality for species identification. The conventional survey method for small mammals, live-trapping, can be both labour-intensive and detrimental to animal welfare. Here, we describe a method for using camera traps for monitoring small mammals. We show that by attaching the camera trap to a baited tunnel, fixing a close-focus lens over the camera trap lens, and reducing the flash intensity, pictures or videos can be obtained of sufficient quality for identifying species. We demonstrate the use of the method by comparing occurrences of small mammals in a peatland landscape containing (i) plantation forestry (planted on drained former blanket bog), (ii) ex-forestry areas undergoing bog restoration, and (iii) unmodified blanket bog habitat. Rodents were detected only in forestry and restoration areas, whilst shrews were detected across all habitat. The odds of detecting small mammals were 7.6 times higher on camera traps set in plantation forestry than in unmodified bog, and 3.7 times higher on camera traps in restoration areas than in bog. When absolute abundance estimates are not required, and camera traps are available, this technique provides a low-cost survey method that is labour-efficient and has minimal animal welfare implications.},
  langid = {english},
  keywords = {Animal Geography,Blanket bog,Ethology,Mammalogy,Photography,Plantation forestry,Primatology,Restoration Ecology,Rodent,Shrew,Trail camera,Vole},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/AJHL3UJ9/Littlewood et al. - 2021 - Use of a novel camera trapping approach to measure small mammal responses to peatland restoration.pdf}
}

@online{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-03-25},
  url = {https://arxiv.org/abs/2103.14030v2},
  urldate = {2025-06-19},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  langid = {english},
  organization = {arXiv.org},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/EM7E8938/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05101},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/Q5ZUFSHE/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Volumes/ExSSD/UserData/Zotero/storage/245STX57/1711.html}
}

@online{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2017-05-03},
  eprint = {1608.03983},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.03983},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/EAUV37KB/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restarts.pdf;/Volumes/ExSSD/UserData/Zotero/storage/479WEHTB/1608.html}
}

@software{morrisEfficientPipelineCamera2025,
  title = {Efficient {{Pipeline}} for {{Camera Trap Image Review}}},
  author = {Morris, Dan and Beery, Sara and Yang, Siyu},
  date = {2025-05-07T01:41:51Z},
  origdate = {2023-05-20T01:46:37Z},
  url = {http://github.com/agentmorris/MegaDetector},
  urldate = {2025-05-13},
  abstract = {MegaDetector is an AI model that helps conservation folks spend less time doing boring things with camera trap images.}
}

@inproceedings{muhammadTemporalSwinFPNNetNovel2024,
  title = {{{TemporalSwin-FPN Net}}: {{A Novel Pipeline}} for {{Metadata-Driven Sequence Classification}} in {{Camera Trap Imagery}}},
  shorttitle = {{{TemporalSwin-FPN Net}}},
  booktitle = {2024 {{International Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  author = {Muhammad, Sameeruddin and Xiang, Wei and Mann, Scott and Han, Kang and Nair, Supriya},
  date = {2024-11},
  pages = {616--623},
  doi = {10.1109/DICTA63115.2024.00094},
  abstract = {In wildlife conservation, camera traps generate a significant volume of data that requires extensive manual analysis to extract insights, particularly for species identification. Existing tools and methods for analysing camera trap data typically classify images individually, while the devices are configured to capture multiple pictures in burst mode, often accompanied by metadata. In this paper, we introduce a novel metadata-driven pipeline based on the enhanced Swin Transformer architecture, named TemporalSwin-FPN Net, which utilises metadata to improve classification through a sequential analysis pipeline. This approach shifts the focus from analysing individual snapshots to sequences of images, thereby enriching the architecture with more comprehensive data for species classification. TemporalSwin-FPN Net has achieved a remarkable accuracy of 99.7\% on the Australian camera trap dataset, and consistently outperformed the baseline models with 94.89\% and 93.32\% on the Snapshot Safari dataset for Camdeboo and Mountain Zebra projects, surpassing them by 9.62\% and 8.54\%. Moreover, integrating the TemporalSwin-FPN Net into our proposed pipeline has significantly reduced the end-to-end processing time for a test dataset of 12,922 images, from 10 minutes 54 seconds to just 6 minutes 41 seconds, thereby substantially enhancing data processing efficiency.},
  eventtitle = {2024 {{International Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  keywords = {Accuracy,camera traps,Cameras,deep learning,image classification,Metadata,Monitoring,Pipelines,Reliability,sequence,sequence-based image classification,Sequential analysis,Stakeholders,swin transformer,Transformers,Wildlife},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/28FMXUG4/Muhammad et al. - 2024 - TemporalSwin-FPN Net A Novel Pipeline for Metadata-Driven Sequence Classification in Camera Trap Im.pdf;/Volumes/ExSSD/UserData/Zotero/storage/G7E4YXEC/10869574.html}
}

@inproceedings{nEnhancedImageClassification2025,
  title = {Enhanced {{Image Classification Using Transfer Learning}} with {{ResNet50-V2}}: {{A Case Study}} on {{Wildlife Recognition}}},
  shorttitle = {Enhanced {{Image Classification Using Transfer Learning}} with {{ResNet50-V2}}},
  booktitle = {2025 6th {{International Conference}} on {{Mobile Computing}} and {{Sustainable Informatics}} ({{ICMCSI}})},
  author = {N, Chandan and B, Manimekala and R V, Siva Balan},
  date = {2025-01},
  pages = {1565--1570},
  doi = {10.1109/ICMCSI64620.2025.10883359},
  abstract = {This study explores the application of transfer learning using the ResNet50-V2 architecture for accurate classification of Arctic wildlife species, including Arctic foxes, polar bears, and walruses. Transfer learning leverages pre-trained networks to enhance performance in new tasks with limited labeled data, reducing the need for extensive data collection and computational resources. In this work, we utilized a dataset of 1000 labeled images across the three species and applied ResNet50-V2, pre-trained on ImageNet, as a feature extractor. The model achieved high accuracy, with training and validation accuracies nearing 99\% and 95-97\%, respectively, though minor overfitting was observed. This indicates the model's strong ability to generalize across the dataset while benefiting from pre-trained weights on diverse, non-related images. Additionally it compares with models like SSD and CycleGAN, emphasizing its capability to generalize well, handle small datasets, and mitigate overfitting. We discuss model architecture, data preprocessing, and the experimental results, focusing on improvements achievable through regularization techniques to counteract overfitting. This study demonstrates the effectiveness of transfer learning for wildlife classification, providing insights into optimizing CNNs for ecological and conservation applications.},
  eventtitle = {2025 6th {{International Conference}} on {{Mobile Computing}} and {{Sustainable Informatics}} ({{ICMCSI}})},
  keywords = {Accuracy,Arctic,Biological system modeling,CNN,Computational modeling,Computer architecture,CycleGAN,Feature extraction,Model Selection,Overfitting,ResNet50-V2,Training,Transfer learning,Transfer Learning,Wildlife,wildlife classification},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/RW8KH5P3/N et al. - 2025 - Enhanced Image Classification Using Transfer Learning with ResNet50-V2 A Case Study on Wildlife Rec.pdf;/Volumes/ExSSD/UserData/Zotero/storage/9WTZEEG4/10883359.html}
}

@article{norouzzadehAutomaticallyIdentifyingCounting2018,
  title = {Automatically Identifying, Counting, and Describing Wild Animals in Camera-Trap Images with Deep Learning},
  author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
  date = {2018-06-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {25},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1719367115},
  abstract = {Significance             Motion-sensor cameras in natural habitats offer the opportunity to inexpensively and unobtrusively gather vast amounts of data on animals in the wild. A key obstacle to harnessing their potential is the great cost of having humans analyze each image. Here, we demonstrate that a cutting-edge type of artificial intelligence called deep neural networks can automatically extract such invaluable information. For example, we show deep learning can automate animal identification for 99.3\% of the 3.2 million-image Snapshot Serengeti dataset while performing at the same 96.6\% accuracy of crowdsourced teams of human volunteers. Automatically, accurately, and inexpensively collecting such data could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences.           ,              Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {$>$}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {$>$}8.4 y (i.e., {$>$}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/AFBKWGRC/Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describing wild animals in camera-trap images with deep lea.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, Fabian and Pedregosa, Fabian and Varoquaux, Gael and Varoquaux, Gael and Org, Normalesup and Gramfort, Alexandre and Gramfort, Alexandre and Michel, Vincent and Michel, Vincent and Fr, Logilab and Thirion, Bertrand and Thirion, Bertrand and Grisel, Olivier and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Tp, Alexandre and Cournapeau, David},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/JGKDP9KT/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf}
}

@online{ratnaweeraWildlifeCampusProgressReports2021,
  title = {Wildlife @ {{Campus}}: {{ProgressReports}}},
  shorttitle = {Wildlife @ {{Campus}}},
  author = {Ratnaweera, Nils},
  date = {2021},
  url = {https://github.zhaw.ch/pages/Wildlife-Campus/ProgressReports/},
  urldate = {2025-06-03},
  organization = {Wildlife @ Campus: ProgressReports},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/HUTHINIH/ProgressReports.html}
}

@online{razavianCNNFeaturesOfftheshelf2014,
  title = {{{CNN Features}} Off-the-Shelf: An {{Astounding Baseline}} for {{Recognition}}},
  shorttitle = {{{CNN Features}} Off-the-Shelf},
  author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  date = {2014-05-12},
  eprint = {1403.6382},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1403.6382},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \textbackslash overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \textbackslash overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \textbackslash overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/VYU55X2Z/Razavian et al. - 2014 - CNN Features off-the-shelf an Astounding Baseline for Recognition.pdf;/Volumes/ExSSD/UserData/Zotero/storage/SAYAE4PW/1403.html}
}

@article{schneiderRecognitionEuropeanMammals2024,
  title = {Recognition of {{European}} Mammals and Birds in Camera Trap Images Using Deep Neural Networks},
  author = {Schneider, Daniel and Lindner, Kim and Vogelbacher, Markus and Bellafkir, Hicham and Farwig, Nina and Freisleben, Bernd},
  date = {2024},
  journaltitle = {IET Computer Vision},
  volume = {18},
  number = {8},
  pages = {1162--1192},
  issn = {1751-9640},
  doi = {10.1049/cvi2.12294},
  abstract = {Most machine learning methods for animal recognition in camera trap images are limited to mammal identification and group birds into a single class. Machine learning methods for visually discriminating birds, in turn, cannot discriminate between mammals and are not designed for camera trap images. The authors present deep neural network models to recognise both mammals and bird species in camera trap images. They train neural network models for species classification as well as for predicting the animal taxonomy, that is, genus, family, order, group, and class names. Different neural network architectures, including ResNet, EfficientNetV2, Vision Transformer, Swin Transformer, and ConvNeXt, are compared for these tasks. Furthermore, the authors investigate approaches to overcome various challenges associated with camera trap image analysis. The authors’ best species classification models achieve a mean average precision (mAP) of 97.91\% on a validation data set and mAPs of 90.39\% and 82.77\% on test data sets recorded in forests in Germany and Poland, respectively. Their best taxonomic classification models reach a validation mAP of 97.18\% and mAPs of 94.23\% and 79.92\% on the two test data sets, respectively.},
  langid = {english},
  keywords = {computer vision,convolutional neural nets,image classification,image recognition,neural nets},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/4VLADYG6/Schneider et al. - 2024 - Recognition of European mammals and birds in camera trap images using deep neural networks.pdf}
}

@article{sharmaTransferLearningWildlife2024,
  title = {Transfer {{Learning}} for {{Wildlife Classification}}: {{Evaluating YOLOv8}} against {{DenseNet}}, {{ResNet}}, and {{VGGNet}} on a {{Custom Dataset}}},
  shorttitle = {Transfer {{Learning}} for {{Wildlife Classification}}},
  author = {Sharma, Subek and Dhakal, Sisir and Bhavsar, Mansi},
  date = {2024-12},
  journaltitle = {Journal of Artificial Intelligence and Capsule Networks},
  shortjournal = {JAICN},
  volume = {6},
  number = {4},
  eprint = {2408.00002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {415--435},
  issn = {2582-2012},
  doi = {10.36548/jaicn.2024.4.003},
  abstract = {This study evaluates the performance of various deep learning models, specifically DenseNet, ResNet, VGGNet, and YOLOv8, for wildlife species classification on a custom dataset. The dataset comprises 575 images of 23 endangered species sourced from reputable online repositories. The study utilizes transfer learning to fine-tune pre-trained models on the dataset, focusing on reducing training time and enhancing classification accuracy. The results demonstrate that YOLOv8 outperforms other models, achieving a training accuracy of 97.39\% and a validation F1-score of 96.50\%. These findings suggest that YOLOv8, with its advanced architecture and efficient feature extraction capabilities, holds great promise for automating wildlife monitoring and conservation efforts.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZY2U2RFU/Sharma et al. - 2024 - Transfer Learning for Wildlife Classification Eva.pdf;/Volumes/ExSSD/UserData/Zotero/storage/AGMH53IS/2408.html}
}

@article{shortenSurveyImageData2019,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  date = {2019-07-06},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZEEC95RE/Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learning.pdf}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/K7LL92RV/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/Volumes/ExSSD/UserData/Zotero/storage/K9X6NTGM/1409.html}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2015-06},
  pages = {1--9},
  publisher = {IEEE},
  location = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298594},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/6WPXTUVZ/Szegedy et al. - 2015 - Going deeper with convolutions.pdf}
}

@online{tabakCameraTrapDetectoRAutomaticallyDetect2022,
  title = {{{CameraTrapDetectoR}}: {{Automatically}} Detect, Classify, and Count Animals in Camera Trap Images Using Artificial Intelligence},
  shorttitle = {{{CameraTrapDetectoR}}},
  author = {Tabak, Michael A. and Falbel, Daniel and Hamzeh, Tess and Brook, Ryan K. and Goolsby, John A. and Zoromski, Lisa D. and Boughton, Raoul K. and Snow, Nathan P. and VerCauteren, Kurt C. and Miller, Ryan S.},
  date = {2022-02-09},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2022.02.07.479461},
  doi = {10.1101/2022.02.07.479461},
  abstract = {Motion-activated wildlife cameras, or camera traps, are widely used in biological monitoring of wildlife. Studies using camera traps amass large numbers of images and analyzing these images can be a large burden that inhibits research progress. We trained deep learning computer vision models using data for 168 species that automatically detect, count, and classify common North American domestic and wild species in camera trap images. We provide our trained models in an R package, CameraTrapDetectoR. Three types of models are available: a taxonomic class model classifies objects as mammal (human and non-human) or avian; a taxonomic family model that recognizes 31 mammal, avian, and reptile families; a species model that recognizes 75 domestic and wild species including all North American wild cat species, bear species, and Canid species. Each model also includes a category for vehicles and empty images. The models performed well on both validation datasets and out-of-distribution testing datasets as mean average precision values ranged from 0.80 to 0.96. CameraTrapDetectoR provides predictions as an R object (a data frame) and flat file and provides the option to create plots of the original camera trap image with the predicted bounding box and label. There is also the option to apply models using a Shiny Application, with a point-and-click graphical user interface. This R package has the potential to facilitate application of deep learning models by biologists using camera traps.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/23M7FSML/Tabak et al. - 2022 - CameraTrapDetectoR Automatically detect, classify, and count animals in camera trap images using ar.pdf}
}

@article{tabakMachineLearningClassify2019,
  title = {Machine Learning to Classify Animal Species in Camera Trap Images: {{Applications}} in Ecology},
  shorttitle = {Machine Learning to Classify Animal Species in Camera Trap Images},
  author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Di Salvo, Paul A. and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and Moeller, Anna K. and Mandeville, Elizabeth G. and Clune, Jeff and Miller, Ryan S.},
  date = {2019},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {585--590},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13120},
  abstract = {Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
  langid = {english},
  keywords = {artificial intelligence,camera trap,convolutional neural network,deep neural networks,image classification,machine learning,r package,remote sensing},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZFW8RP7B/Tabak et al. - 2019 - Machine learning to classify animal species in camera trap images Applications in ecology.pdf;/Volumes/ExSSD/UserData/Zotero/storage/DXF2KQ26/2041-210X.html}
}

@article{tanAnimalDetectionClassification2022,
  title = {Animal {{Detection}} and {{Classification}} from {{Camera Trap Images Using Different Mainstream Object Detection Architectures}}},
  author = {Tan, Mengyu and Chao, Wentao and Cheng, Jo-Ku and Zhou, Mo and Ma, Yiwen and Jiang, Xinyi and Ge, Jianping and Yu, Lian and Feng, Limin},
  date = {2022-08-04},
  journaltitle = {Animals},
  shortjournal = {Animals},
  volume = {12},
  number = {15},
  pages = {1976},
  issn = {2076-2615},
  doi = {10.3390/ani12151976},
  abstract = {Camera traps are widely used in wildlife surveys and biodiversity monitoring. Depending on its triggering mechanism, a large number of images or videos are sometimes accumulated. Some literature has proposed the application of deep learning techniques to automatically identify wildlife in camera trap imagery, which can significantly reduce manual work and speed up analysis processes. However, there are few studies validating and comparing the applicability of different models for object detection in real field monitoring scenarios. In this study, we firstly constructed a wildlife image dataset of the Northeast Tiger and Leopard National Park (NTLNP dataset). Furthermore, we evaluated the recognition performance of three currently mainstream object detection architectures and compared the performance of training models on day and night data separately versus together. In this experiment, we selected YOLOv5 series models (anchor-based one-stage), Cascade R-CNN under feature extractor HRNet32 (anchor-based two-stage), and FCOS under feature extractors ResNet50 and ResNet101 (anchor-free one-stage). The experimental results showed that performance of the object detection models of the day-night joint training is satisfying. Specifically, the average result of our models was 0.98 mAP (mean average precision) in the animal image detection and 88\% accuracy in the animal video classification. One-stage YOLOv5m achieved the best recognition accuracy. With the help of AI technology, ecologists can extract information from masses of imagery potentially quickly and efficiently, saving much time.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/BVJ258D6/Tan et al. - 2022 - Animal Detection and Classification from Camera Trap Images Using Different Mainstream Object Detect.pdf}
}

@inproceedings{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  date = {2019-05-24},
  pages = {6105--6114},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/tan19a.html},
  urldate = {2025-06-02},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/879X2CG2/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf}
}

@article{thomsenEnvironmentalDNAEmerging2015,
  title = {Environmental {{DNA}} – {{An}} Emerging Tool in Conservation for Monitoring Past and Present Biodiversity},
  author = {Thomsen, Philip Francis and Willerslev, Eske},
  date = {2015-03-01},
  journaltitle = {Biological Conservation},
  shortjournal = {Biological Conservation},
  series = {Special {{Issue}}: {{Environmental DNA}}: {{A}} Powerful New Tool for Biological Conservation},
  volume = {183},
  pages = {4--18},
  issn = {0006-3207},
  doi = {10.1016/j.biocon.2014.11.019},
  abstract = {The continuous decline in Earth’s biodiversity represents a major crisis and challenge for the 21st century, and there is international political agreement to slow down or halt this decline. The challenge is in large part impeded by the lack of knowledge on the state and distribution of biodiversity – especially since the majority of species on Earth are un-described by science. All conservation efforts to save biodiversity essentially depend on the monitoring of species and populations to obtain reliable distribution patterns and population size estimates. Such monitoring has traditionally relied on physical identification of species by visual surveys and counting of individuals. However, traditional monitoring techniques remain problematic due to difficulties associated with correct identification of cryptic species or juvenile life stages, a continuous decline in taxonomic expertise, non-standardized sampling, and the invasive nature of some survey techniques. Hence, there is urgent need for alternative and efficient techniques for large-scale biodiversity monitoring. Environmental DNA (eDNA) – defined here as: genetic material obtained directly from environmental samples (soil, sediment, water, etc.) without any obvious signs of biological source material – is an efficient, non-invasive and easy-to-standardize sampling approach. Coupled with sensitive, cost-efficient and ever-advancing DNA sequencing technology, it may be an appropriate candidate for the challenge of biodiversity monitoring. Environmental DNA has been obtained from ancient as well as modern samples and encompasses single species detection to analyses of ecosystems. The research on eDNA initiated in microbiology, recognizing that culture-based methods grossly misrepresent the microbial diversity in nature. Subsequently, as a method to assess the diversity of macro-organismal communities, eDNA was first analyzed in sediments, revealing DNA from extinct and extant animals and plants, but has since been obtained from various terrestrial and aquatic environmental samples. Results from eDNA approaches have provided valuable insights to the study of ancient environments and proven useful for monitoring contemporary biodiversity in terrestrial and aquatic ecosystems. In the future, we expect the eDNA-based approaches to move from single-marker analyses of species or communities to meta-genomic surveys of entire ecosystems to predict spatial and temporal biodiversity patterns. Such advances have applications for a range of biological, geological and environmental sciences. Here we review the achievements gained through analyses of eDNA from macro-organisms in a conservation context, and discuss its potential advantages and limitations for biodiversity monitoring.},
  keywords = {Biodiversity,Conservation,DNA metabarcoding,eDNA,Environmental DNA,Extinction,Monitoring},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/K5Y5CVIP/Thomsen and Willerslev - 2015 - Environmental DNA – An emerging tool in conservation for monitoring past and present biodiversity.pdf;/Volumes/ExSSD/UserData/Zotero/storage/SHN4NA9J/Thomsen and Willerslev - 2015 - Environmental DNA – An emerging tool in conservation for monitoring past and present biodiversity.pdf;/Volumes/ExSSD/UserData/Zotero/storage/LI8KFPUH/S0006320714004443.html}
}

@online{velezChoosingAppropriatePlatform2022,
  title = {Choosing an {{Appropriate Platform}} and {{Workflow}} for {{Processing Camera Trap Data}} Using {{Artificial Intelligence}}},
  author = {Vélez, Juliana and Castiblanco-Camacho, Paula J. and Tabak, Michael A. and Chalmers, Carl and Fergus, Paul and Fieberg, John},
  date = {2022-02-04},
  eprint = {2202.02283},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.02283},
  abstract = {Camera traps have transformed how ecologists study wildlife species distributions, activity patterns, and interspecific interactions. Although camera traps provide a cost-effective method for monitoring species, the time required for data processing can limit survey efficiency. Thus, the potential of Artificial Intelligence (AI), specifically Deep Learning (DL), to process camera-trap data has gained considerable attention. Using DL for these applications involves training algorithms, such as Convolutional Neural Networks (CNNs), to automatically detect objects and classify species. To overcome technical challenges associated with training CNNs, several research communities have recently developed platforms that incorporate DL in easy-to-use interfaces. We review key characteristics of four AI-powered platforms -- Wildlife Insights (WI), MegaDetector (MD), Machine Learning for Wildlife Image Classification (MLWIC2), and Conservation AI -- including data management tools and AI features. We also provide R code in an open-source GitBook, to demonstrate how users can evaluate model performance, and incorporate AI output in semi-automated workflows. We found that species classifications from WI and MLWIC2 generally had low recall values (animals that were present in the images often were not classified to the correct species). Yet, the precision of WI and MLWIC2 classifications for some species was high (i.e., when classifications were made, they were generally accurate). MD, which classifies images using broader categories (e.g., "blank" or "animal"), also performed well. Thus, we conclude that, although species classifiers were not accurate enough to automate image processing, DL could be used to improve efficiencies by accepting classifications with high confidence values for certain species or by filtering images containing blanks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/MB59NJDB/Vélez et al. - 2022 - Choosing an Appropriate Platform and Workflow for Processing Camera Trap Data using Artificial Intel.pdf}
}

@online{wangTemporalSegmentNetworks2016,
  title = {Temporal {{Segment Networks}}: {{Towards Good Practices}} for {{Deep Action Recognition}}},
  shorttitle = {Temporal {{Segment Networks}}},
  author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Gool, Luc Van},
  date = {2016-08-02},
  eprint = {1608.00859},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.00859},
  abstract = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( \$ 69.4\textbackslash\% \$) and UCF101 (\$ 94.2\textbackslash\% \$). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.},
  pubstate = {prepublished},
  keywords = {action recognition,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/PPXY4X5L/Wang et al. - 2016 - Temporal Segment Networks Towards Good Practices for Deep Action Recognition.pdf;/Volumes/ExSSD/UserData/Zotero/storage/J5AFQPGD/1608.html}
}

@article{wearnCAMERATRAPPINGPAGE2,
  title = {{{CAMERA-TRAPPING PAGE}} 2},
  author = {Wearn, Oliver R and Glover-Kapfer, Paul},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ALC8FECH/Wearn and Glover-Kapfer - CAMERA-TRAPPING PAGE 2.pdf}
}

@article{yarnellUsingOccupancyAnalysis2014,
  title = {Using Occupancy Analysis to Validate the Use of Footprint Tunnels as a Method for Monitoring the Hedgehog {{Erinaceus}} Europaeus},
  author = {Yarnell, Richard W. and Pacheco, Marina and Williams, Ben and Neumann, Jessica L. and Rymer, David J. and Baker, Philip J.},
  date = {2014},
  journaltitle = {Mammal Review},
  volume = {44},
  number = {3--4},
  pages = {234--238},
  issn = {1365-2907},
  doi = {10.1111/mam.12026},
  abstract = {Indirect survey methods are often used in studies of mammals but are susceptible to biases caused by failure to detect species where they are present. Occupancy analysis is an analytical technique which enables non-detection rates to be estimated and which can be used to develop and refine novel survey methods. In this study, we investigated the use of footprint tunnels by volunteers as a method for surveying occupancy of sites by hedgehogs Erinaceus europaeus. The survey protocol led to a very low non-detection rate and could reasonably be used to detect occupancy changes of 25\% with statistical power of 0.95 in a national survey.},
  langid = {english},
  keywords = {citizen science,Erinaceidae,field sign surveys,occupancy modelling,population monitoring},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/746A7WX4/Yarnell et al. - 2014 - Using occupancy analysis to validate the use of footprint tunnels as a method for monitoring the hed.pdf;/Volumes/ExSSD/UserData/Zotero/storage/N4SEFJGS/mam.html}
}

@article{zotinANIMALDETECTIONUSING2019,
  title = {{{ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS}}},
  author = {Zotin, A. G. and Proskurin, A. V.},
  date = {2019-05-09},
  journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XLII-2-W12},
  pages = {249--257},
  publisher = {Copernicus GmbH},
  issn = {1682-1750},
  doi = {10.5194/isprs-archives-XLII-2-W12-249-2019},
  abstract = {Camera traps providing enormous number of images during a season help to observe remotely animals in the wild. However, analysis of such image collection manually is impossible. In this research, we develop a method for automatic animal detection based on background modeling of scene under complex shooting. First, we design a fast algorithm for image selection without motions. Second, the images are processed by modified Multi-Scale Retinex algorithm in order to align uneven illumination. Finally, background is subtracted from incoming image using adaptive threshold. A threshold value is adjusted by saliency map, which is calculated using pyramid consisting of the original image and images modified by MSR algorithm. Proposed method allows to achieve high estimators of animals detection.},
  eventtitle = {International {{Workshop}} on “{{Photogrammetric}} and {{Computer Vision Techniques}} for {{Video Surveillance}}, {{Biometrics}} and {{Biomedicine}}” ({{Volume XLII-2}}/{{W12}}) - 13\&ndash;15 {{May}} 2019, {{Moscow}}, {{Russia}}},
  langid = {english},
  keywords = {Animal Detection,Background Modeling,Camera Traps,MSR Algorithm},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/LQHGNGWC/Zotin and Proskurin - 2019 - ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS.pdf}
}
