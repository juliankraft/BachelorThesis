\documentclass{article}
\usepackage[margin=2.5cm]{geometry} % Set margins
\usepackage{graphicx}
\usepackage[absolute]{textpos} % Enable absolute positioning
\usepackage{titlesec} % Package for controlling section title appearance
\usepackage[scaled]{helvet}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{url} % Load the url package

% Set up colors
\usepackage[dvipsnames]{xcolor} % Color management
% ZHAW Blue: Pantone 2945 U / R0 G100 B166
\definecolor{zhawblue}{rgb}{0.00, 0.39, 0.65}
\definecolor{zhawlightblue}{rgb}{0.82, 0.88, 0.93}

% Set up hyperref
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=zhawblue,
    linkbordercolor={0 0 1}
}

% Set up references
\usepackage[
    backend=biber,             % Use biber backend (an external tool)
    sorting=none,              % Enumerates the reference in order of their appearance
    style=apa          % Choose here your preferred citation style
]{biblatex}
\addbibresource{../../biblatex_ba.bib} % The filename of the bibliography

\usepackage[english]{babel} % Set the document language to English

\usepackage[autostyle=true, english=american]{csquotes} 
                               % Required to generate language-dependent quotes 
                               % in the bibliography

\setlength{\TPHorizModule}{1cm} % Set horizontal unit of measure
\setlength{\TPVertModule}{1cm} % Set vertical unit of measure
\setlength{\parindent}{0pt}

\renewcommand{\familydefault}{\sfdefault}

\makeatletter
\renewcommand{\maketitle}{
  \begin{flushleft} 
    \Large\textmd{\@title} 
    \par
  \end{flushleft}
}
\makeatother

% Define style of sectiontitles
\titleformat{\section}
  {\normalfont\large\mdseries}{\thesection}{1em}{}

\titleformat{\subsection}
  {\normalfont\normalsize\itshape} % Adjust style: smaller size, italic
  {} % No label
  {0pt} % Spacing between label and title
  {} % Code to execute after the title
\titlespacing*{\subsection}
  {0pt} % Left margin
  {0.8em} % Space above
  {0.2em} % Space below

% Set up fancyhdr
\fancyhf{} % Clear all headers and footers
\renewcommand{\headrulewidth}{0pt} % Remove the header rule
\rfoot{\thepage} % Place the page number in the right footer
\pagestyle{fancy}

% Add listings package for code highlighting
\usepackage{listings}
\usepackage{xcolor}

%%%%% Title %%%%%
\title{Report on Workflow Research}

\begin{document}

%%%%% Header %%%%%
\begin{textblock}{5}(2.5,1) % Position 1cm from left and 1cm from top
  \includegraphics[width=5cm]{logo.jpg} % Add logo
\end{textblock}

\begin{textblock}{6}(13,1) % Position 14cm from left and 1cm from top
        \raggedleft
        Julian Kraft UI22\\
        Report on Workflow Research\\
        \today
\end{textblock}

\vspace*{1.5cm}

%%%%% Document %%%%%

\maketitle

%%%%%   %%%%%

\section*{What I have learned from the papers}

The challenge of handling large amounts of images utilizing camera traps for biodiversity seems to be a common problem in the field of ecology.
Approaching the problem by simply working trough the images manually is not feasible due to the sheer amount of data.
Therefore, researchers have developed various methods to automate the process of image classification and species identification.
\textcite{velezChoosingAppropriatePlatform2022} provide an overview of several methods and tools that can be used to analyze camera trap data.

Here an overview of this tools:

\begin{itemize}
  \item \textbf{Wildlife Insights (WI)} is a cloud-based platform for managing and analyzing camera trap data. 
  It supports AI-powered image classification, species identification, and data sharing, 
  making it valuable for large-scale ecological research. Users must share data publicly after an embargo period.
  
  \item \textbf{Machine Learning for Wildlife Image Classification (MLWIC2)} is an R package for classifying 
  wildlife images, primarily trained on North American species. It allows local model execution and custom 
  training but requires technical expertise and specific software dependencies.
  
  \item \textbf{MegaDetector (MD)} is a pre-trained deep learning model for detecting animals, humans, 
  and vehicles in camera trap images. It excels at filtering images but does not classify species. 
  MD can be integrated with other tools like Timelapse 2 or Camelot for semi-automated workflows.
  
  \item \textbf{Conservation AI} is a cloud-based AI platform for real-time species identification using camera traps, 
  drones, and acoustic data. It supports model training but requires substantial data input and relies on internet connectivity for processing.
\end{itemize}

While the paper provides a good overview of this tools and a reasonable evaluation it lacks depth when it comes to how 
this tools work, what models they are based on and how they are trained.\par
\vspace{10pt}

In "A semi-automatic workflow to process images from small mammal camera traps" \autocite{bohnerSemiautomaticWorkflowProcess2023} a Workflow
to solve a very similar problem is presented. In a straight forward approach a ResNet50 model is trained on a dataset of small mammal images.
A routine for retraining with additional data is implemented and and the improvements are documented and evaluated.
Difficulties mentioned in the paper are the class imbalance which is expected to be one of the main challenges in this project as well.
An other challenge is the limitations when it comes to the transformation of the model to different data -- an issue that
might be less of a problem in this project since the Camera Trap boxes are somehow standardized to a certain extend.\par
\vspace{10pt}

The "CameraTrapDetectoR" \autocite{tabakCameraTrapDetectoRAutomaticallyDetect2022} is a potentially fully automated solution for the problem at hand.
Still it offers the flexibility to be used in a semi-automated way. The model used in this project is a Faster R-CNN model with a ResNet50 backbone.
This is the technology used in the MegaDetector before utilizing YOLO in version 5. The MegaDetector was incorporated in the project to
assist creating the bounding boxes on the training data. In contrary to the MegaDetector, where the detection is a separated step generating
bounding boxes and a classification in the categories "animal", "human" and "vehicle". The CameraTrapDetectoR is built in a way that 
the detection and the classification of species are done in one step.\par
\vspace{10pt}

"Recognition of European mammals and birds in camera trap  images using deep neural networks" \autocite{schneiderRecognitionEuropeanMammals2024}
is a paper that describes an other possible workflow potentially applicable to this project. While incorporating many species including birds,
an interessting aspect is the incorporation of a taxonomic approach to the classification.
This means it uses a hierarchical label system -- class, family, and species -- to classify the images. This has several advantages such as
to use partially labeled data for training or usefull prediction outputs on not yet implemented species.
The workflow incorporates the MegaDetector as part of the dataprocessing pipline as well. The output of MD detection is used to crop the images
to theire relevant areas before the classification is done. In this paper a selection of different models is trained and evaluated on the same
dataset. Even though in this project the number of species is very much limited and a taxonomic approach is not needed, the potential of
the described workflow seems to be very promising.\par
\vspace{10pt}

"ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS" \autocite{zotinANIMALDETECTIONUSING2019} is a paper
that describes a workflow incorporating a series of images for detection. As I understand it, the idea is to use a series of images.
Using empty images the background is modeled and the difference of the background and the image is used to detect the animals.
The paper seems very technical and it mainly confuses me. What I do not understand how this is using a series of images to perform a
classification, since in the end it seems to provide strategy to detect the animals. I might need some help to understand how and why this
could be incorporated in this project.\par
\vspace{10pt}

\section*{Proposed Workflow}

Combining things I learned from the papers that might be applicable to this project I propose the following workflow:

\begin{enumerate}
  \item In a first step the existing data will be analyzed to get an overview of the available data per label, sequences and their length and the distribution over camera and box types.
  Building on the evaluations by Nils in the progres report.
  
  \item Processing all the images using MD to get the bounding boxes for the animals and include this information in the metadata.
  
  \item The quality of this output will have to be evaluated and documented. A protocol to do this needs to be found or developped.
  
  \item The data will be split into training and test data fixing this within the metadata. Sequences will be kept together.
  
  \item I propose to try a selection of models to see how they perform on the data. Possible models to try could be ResNet50, EfficiencyNet or a YOLO model. 
  Further research is needed to create a well informed list.

  \item The performance of the models will be evaluated and compared to each other using the test data.
  
\end{enumerate}

\section*{Questions for further Research}

\begin{itemize}

  \item How do other projects evaluate the quality of the bounding boxes generated by the MegaDetector?
  \item What Deep Learning models are used in similar projects and could be promissing for this project?
  \item An issue mentioned in some of the papers we discussed before is the question about class imbalances. A better overview how this was handled in other projects would be helpful.
  \item What metrics for performance exist and which could work best for this project?
  
\end{itemize}


\end{document}
