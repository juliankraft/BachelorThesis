@article{bohnerSemiautomaticWorkflowProcess2023,
  title = {A Semi-Automatic Workflow to Process Images from Small Mammal Camera Traps},
  author = {Böhner, Hanna and Kleiven, Eivind Flittie and Ims, Rolf Anker and Soininen, Eeva M.},
  date = {2023-09-01},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {76},
  pages = {102150},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2023.102150},
  url = {https://www.sciencedirect.com/science/article/pii/S1574954123001796},
  urldate = {2025-03-16},
  abstract = {Camera traps have become popular for monitoring biodiversity, but the huge amounts of image data that arise from camera trap monitoring represent a challenge and artificial intelligence is increasingly used to automatically classify large image data sets. However, it is still challenging to combine automatic classification with other steps and tools needed for efficient, quality-assured and adaptive processing of camera trap images in long-term monitoring programs. Here we propose a semi-automatic workflow to process images from small mammal cameras that combines all necessary steps from downloading camera trap images in the field to a quality checked data set ready to be used in ecological analyses. The workflow is implemented in R and includes (1) managing raw images, (2) automatic image classification, (3) quality check of automatic image labels, as well as the possibilities to (4) retrain the model with new images and to (5) manually review subsets of images to correct image labels. We illustrate the application of this workflow for the development of a new monitoring program of an Arctic small mammal community. We first trained a classification model for the specific small mammal community based on images from an initial set of camera traps. As the monitoring program evolved, the classification model was retrained with a small subset of images from new camera traps. This case study highlights the importance of model retraining in adaptive monitoring programs based on camera traps as this step in the workflow increases model performance and substantially decreases the total time needed for manually reviewing images and correcting image labels. We provide all R scripts to make the workflow accessible to other ecologists.},
  keywords = {Adaptive monitoring,Automatic image classification,Camera trap,Data processing,Deep learning,Rodent},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/YWP8GERF/Böhner et al. - 2023 - A semi-automatic workflow to process images from small mammal camera traps.pdf;/Volumes/ExSSD/UserData/Zotero/storage/LA2CIZIK/S1574954123001796.html}
}

@article{bothmannAutomatedWildlifeImage2023,
  title = {Automated Wildlife Image Classification: {{An}} Active Learning Tool for Ecological Applications},
  shorttitle = {Automated Wildlife Image Classification},
  author = {Bothmann, Ludwig and Wimmer, Lisa and Charrakh, Omid and Weber, Tobias and Edelhoff, Hendrik and Peters, Wibke and Nguyen, Hien and Benjamin, Caryl and Menzel, Annette},
  date = {2023-11},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {77},
  eprint = {2303.15823},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {102231},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2023.102231},
  url = {http://arxiv.org/abs/2303.15823},
  urldate = {2025-03-12},
  abstract = {Wildlife camera trap images are being used extensively to investigate animal abundance, habitat associations, and behavior, which is complicated by the fact that experts must first classify the images manually. Artificial intelligence systems can take over this task but usually need a large number of already-labeled training images to achieve sufficient performance. This requirement necessitates human expert labor and poses a particular challenge for projects with few cameras or short durations. We propose a label-efficient learning strategy that enables researchers with small or medium-sized image databases to leverage the potential of modern machine learning, thus freeing crucial resources for subsequent analyses. Our methodological proposal is two-fold: (1) We improve current strategies of combining object detection and image classification by tuning the hyperparameters of both models. (2) We provide an active learning (AL) system that allows training deep learning models very efficiently in terms of required human-labeled training images. We supply a software package that enables researchers to use these methods directly and thereby ensure the broad applicability of the proposed framework in ecological practice. We show that our tuning strategy improves predictive performance. We demonstrate how the AL pipeline reduces the amount of pre-labeled data needed to achieve a specific predictive performance and that it is especially valuable for improving out-of-sample predictive performance. We conclude that the combination of tuning and AL increases predictive performance substantially. Furthermore, we argue that our work can broadly impact the community through the ready-to-use software package provided. Finally, the publication of our models tailored to European wildlife data enriches existing model bases mostly trained on data from Africa and North America.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Applications}
}

@article{bothmannAutomatedWildlifeImage2023a,
  title = {Automated Wildlife Image Classification: {{An}} Active Learning Tool for Ecological Applications},
  shorttitle = {Automated Wildlife Image Classification},
  author = {Bothmann, Ludwig and Wimmer, Lisa and Charrakh, Omid and Weber, Tobias and Edelhoff, Hendrik and Peters, Wibke and Nguyen, Hien and Benjamin, Caryl and Menzel, Annette},
  date = {2023-11},
  journaltitle = {Ecological Informatics},
  shortjournal = {Ecological Informatics},
  volume = {77},
  eprint = {2303.15823},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {102231},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2023.102231},
  url = {http://arxiv.org/abs/2303.15823},
  urldate = {2025-03-12},
  abstract = {Wildlife camera trap images are being used extensively to investigate animal abundance, habitat associations, and behavior, which is complicated by the fact that experts must first classify the images manually. Artificial intelligence systems can take over this task but usually need a large number of already-labeled training images to achieve sufficient performance. This requirement necessitates human expert labor and poses a particular challenge for projects with few cameras or short durations. We propose a label-efficient learning strategy that enables researchers with small or medium-sized image databases to leverage the potential of modern machine learning, thus freeing crucial resources for subsequent analyses. Our methodological proposal is two-fold: (1) We improve current strategies of combining object detection and image classification by tuning the hyperparameters of both models. (2) We provide an active learning (AL) system that allows training deep learning models very efficiently in terms of required human-labeled training images. We supply a software package that enables researchers to use these methods directly and thereby ensure the broad applicability of the proposed framework in ecological practice. We show that our tuning strategy improves predictive performance. We demonstrate how the AL pipeline reduces the amount of pre-labeled data needed to achieve a specific predictive performance and that it is especially valuable for improving out-of-sample predictive performance. We conclude that the combination of tuning and AL increases predictive performance substantially. Furthermore, we argue that our work can broadly impact the community through the ready-to-use software package provided. Finally, the publication of our models tailored to European wildlife data enriches existing model bases mostly trained on data from Africa and North America.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Applications}
}

@article{grafWildlifeCampusKleineSaeugetiere2022,
  title = {Wildlife@Campus: Kleine Säugetiere im Fokus},
  author = {Graf, Roland F. and Dietrich, Adrian and Honetschläger, Nils and Kryszczuk, Krzysztof and Palmisano, Marilena and Pothier, Joël F. and Ratnaweera, Nils and Reifler-Bächtiger, Martina and Rhyner, Nicola and Treichler, Regula},
  date = {2022-07},
  langid = {ngerman},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/MXGX9DBR/Graf - Wildlife@Campus Kleine Säugetiere im Fokus.pdf}
}

@online{hernandezPytorchWildlifeCollaborativeDeep2024,
  title = {Pytorch-{{Wildlife}}: {{A Collaborative Deep Learning Framework}} for {{Conservation}}},
  shorttitle = {Pytorch-{{Wildlife}}},
  author = {Hernandez, Andres and Miao, Zhongqi and Vargas, Luisa and Beery, Sara and Dodhia, Rahul and Arbelaez, Pablo and Ferres, Juan M. Lavista},
  date = {2024-11-29},
  eprint = {2405.12930},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.12930},
  url = {http://arxiv.org/abs/2405.12930},
  urldate = {2025-02-04},
  abstract = {The alarming decline in global biodiversity, driven by various factors, underscores the urgent need for large-scale wildlife monitoring. In response, scientists have turned to automated deep learning methods for data processing in wildlife monitoring. However, applying these advanced methods in real-world scenarios is challenging due to their complexity and the need for specialized knowledge, primarily because of technical challenges and interdisciplinary barriers. To address these challenges, we introduce Pytorch-Wildlife, an open-source deep learning platform built on PyTorch. It is designed for creating, modifying, and sharing powerful AI models. This platform emphasizes usability and accessibility, making it accessible to individuals with limited or no technical background. It also offers a modular codebase to simplify feature expansion and further development. Pytorch-Wildlife offers an intuitive, user-friendly interface, accessible through local installation or Hugging Face, for animal detection and classification in images and videos. As two real-world applications, Pytorch-Wildlife has been utilized to train animal classification models for species recognition in the Amazon Rainforest and for invasive opossum recognition in the Galapagos Islands. The Opossum model achieves 98\% accuracy, and the Amazon model has 92\% recognition accuracy for 36 animals in 90\% of the data. As Pytorch-Wildlife evolves, we aim to integrate more conservation tasks, addressing various environmental challenges. Pytorch-Wildlife is available at https://github.com/microsoft/CameraTraps.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/5VB3HY3J/Hernandez et al. - 2024 - Pytorch-Wildlife A Collaborative Deep Learning Fr.pdf;/Volumes/ExSSD/UserData/Zotero/storage/WZXRFIKM/2405.html}
}

@article{norouzzadehAutomaticallyIdentifyingCounting2018,
  title = {Automatically Identifying, Counting, and Describing Wild Animals in Camera-Trap Images with Deep Learning},
  author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
  date = {2018-06-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {25},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1719367115},
  url = {https://pnas.org/doi/full/10.1073/pnas.1719367115},
  urldate = {2025-03-12},
  abstract = {Significance             Motion-sensor cameras in natural habitats offer the opportunity to inexpensively and unobtrusively gather vast amounts of data on animals in the wild. A key obstacle to harnessing their potential is the great cost of having humans analyze each image. Here, we demonstrate that a cutting-edge type of artificial intelligence called deep neural networks can automatically extract such invaluable information. For example, we show deep learning can automate animal identification for 99.3\% of the 3.2 million-image Snapshot Serengeti dataset while performing at the same 96.6\% accuracy of crowdsourced teams of human volunteers. Automatically, accurately, and inexpensively collecting such data could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences.           ,              Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {$>$}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {$>$}8.4 y (i.e., {$>$}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
  langid = {english},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/AFBKWGRC/Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describing wild animals in camera-trap images with deep lea.pdf}
}

@article{schneiderRecognitionEuropeanMammals2024,
  title = {Recognition of {{European}} Mammals and Birds in Camera Trap Images Using Deep Neural Networks},
  author = {Schneider, Daniel and Lindner, Kim and Vogelbacher, Markus and Bellafkir, Hicham and Farwig, Nina and Freisleben, Bernd},
  date = {2024},
  journaltitle = {IET Computer Vision},
  volume = {18},
  number = {8},
  pages = {1162--1192},
  issn = {1751-9640},
  doi = {10.1049/cvi2.12294},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cvi2.12294},
  urldate = {2025-03-16},
  abstract = {Most machine learning methods for animal recognition in camera trap images are limited to mammal identification and group birds into a single class. Machine learning methods for visually discriminating birds, in turn, cannot discriminate between mammals and are not designed for camera trap images. The authors present deep neural network models to recognise both mammals and bird species in camera trap images. They train neural network models for species classification as well as for predicting the animal taxonomy, that is, genus, family, order, group, and class names. Different neural network architectures, including ResNet, EfficientNetV2, Vision Transformer, Swin Transformer, and ConvNeXt, are compared for these tasks. Furthermore, the authors investigate approaches to overcome various challenges associated with camera trap image analysis. The authors’ best species classification models achieve a mean average precision (mAP) of 97.91\% on a validation data set and mAPs of 90.39\% and 82.77\% on test data sets recorded in forests in Germany and Poland, respectively. Their best taxonomic classification models reach a validation mAP of 97.18\% and mAPs of 94.23\% and 79.92\% on the two test data sets, respectively.},
  langid = {english},
  keywords = {computer vision,convolutional neural nets,image classification,image recognition,neural nets},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/4VLADYG6/Schneider et al. - 2024 - Recognition of European mammals and birds in camera trap images using deep neural networks.pdf}
}

@article{sharmaTransferLearningWildlife2024,
  title = {Transfer {{Learning}} for {{Wildlife Classification}}: {{Evaluating YOLOv8}} against {{DenseNet}}, {{ResNet}}, and {{VGGNet}} on a {{Custom Dataset}}},
  shorttitle = {Transfer {{Learning}} for {{Wildlife Classification}}},
  author = {Sharma, Subek and Dhakal, Sisir and Bhavsar, Mansi},
  date = {2024-12},
  journaltitle = {Journal of Artificial Intelligence and Capsule Networks},
  shortjournal = {JAICN},
  volume = {6},
  number = {4},
  eprint = {2408.00002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {415--435},
  issn = {2582-2012},
  doi = {10.36548/jaicn.2024.4.003},
  url = {http://arxiv.org/abs/2408.00002},
  urldate = {2025-02-04},
  abstract = {This study evaluates the performance of various deep learning models, specifically DenseNet, ResNet, VGGNet, and YOLOv8, for wildlife species classification on a custom dataset. The dataset comprises 575 images of 23 endangered species sourced from reputable online repositories. The study utilizes transfer learning to fine-tune pre-trained models on the dataset, focusing on reducing training time and enhancing classification accuracy. The results demonstrate that YOLOv8 outperforms other models, achieving a training accuracy of 97.39\% and a validation F1-score of 96.50\%. These findings suggest that YOLOv8, with its advanced architecture and efficient feature extraction capabilities, holds great promise for automating wildlife monitoring and conservation efforts.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/ZY2U2RFU/Sharma et al. - 2024 - Transfer Learning for Wildlife Classification Eva.pdf;/Volumes/ExSSD/UserData/Zotero/storage/AGMH53IS/2408.html}
}

@online{tabakCameraTrapDetectoRAutomaticallyDetect2022,
  title = {{{CameraTrapDetectoR}}: {{Automatically}} Detect, Classify, and Count Animals in Camera Trap Images Using Artificial Intelligence},
  shorttitle = {{{CameraTrapDetectoR}}},
  author = {Tabak, Michael A. and Falbel, Daniel and Hamzeh, Tess and Brook, Ryan K. and Goolsby, John A. and Zoromski, Lisa D. and Boughton, Raoul K. and Snow, Nathan P. and VerCauteren, Kurt C. and Miller, Ryan S.},
  date = {2022-02-09},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2022.02.07.479461},
  doi = {10.1101/2022.02.07.479461},
  url = {https://www.biorxiv.org/content/10.1101/2022.02.07.479461v1},
  urldate = {2025-03-16},
  abstract = {Motion-activated wildlife cameras, or camera traps, are widely used in biological monitoring of wildlife. Studies using camera traps amass large numbers of images and analyzing these images can be a large burden that inhibits research progress. We trained deep learning computer vision models using data for 168 species that automatically detect, count, and classify common North American domestic and wild species in camera trap images. We provide our trained models in an R package, CameraTrapDetectoR. Three types of models are available: a taxonomic class model classifies objects as mammal (human and non-human) or avian; a taxonomic family model that recognizes 31 mammal, avian, and reptile families; a species model that recognizes 75 domestic and wild species including all North American wild cat species, bear species, and Canid species. Each model also includes a category for vehicles and empty images. The models performed well on both validation datasets and out-of-distribution testing datasets as mean average precision values ranged from 0.80 to 0.96. CameraTrapDetectoR provides predictions as an R object (a data frame) and flat file and provides the option to create plots of the original camera trap image with the predicted bounding box and label. There is also the option to apply models using a Shiny Application, with a point-and-click graphical user interface. This R package has the potential to facilitate application of deep learning models by biologists using camera traps.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/23M7FSML/Tabak et al. - 2022 - CameraTrapDetectoR Automatically detect, classify, and count animals in camera trap images using ar.pdf}
}

@online{velezChoosingAppropriatePlatform2022,
  title = {Choosing an {{Appropriate Platform}} and {{Workflow}} for {{Processing Camera Trap Data}} Using {{Artificial Intelligence}}},
  author = {Vélez, Juliana and Castiblanco-Camacho, Paula J. and Tabak, Michael A. and Chalmers, Carl and Fergus, Paul and Fieberg, John},
  date = {2022-02-04},
  eprint = {2202.02283},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.02283},
  url = {http://arxiv.org/abs/2202.02283},
  urldate = {2025-03-16},
  abstract = {Camera traps have transformed how ecologists study wildlife species distributions, activity patterns, and interspecific interactions. Although camera traps provide a cost-effective method for monitoring species, the time required for data processing can limit survey efficiency. Thus, the potential of Artificial Intelligence (AI), specifically Deep Learning (DL), to process camera-trap data has gained considerable attention. Using DL for these applications involves training algorithms, such as Convolutional Neural Networks (CNNs), to automatically detect objects and classify species. To overcome technical challenges associated with training CNNs, several research communities have recently developed platforms that incorporate DL in easy-to-use interfaces. We review key characteristics of four AI-powered platforms -- Wildlife Insights (WI), MegaDetector (MD), Machine Learning for Wildlife Image Classification (MLWIC2), and Conservation AI -- including data management tools and AI features. We also provide R code in an open-source GitBook, to demonstrate how users can evaluate model performance, and incorporate AI output in semi-automated workflows. We found that species classifications from WI and MLWIC2 generally had low recall values (animals that were present in the images often were not classified to the correct species). Yet, the precision of WI and MLWIC2 classifications for some species was high (i.e., when classifications were made, they were generally accurate). MD, which classifies images using broader categories (e.g., "blank" or "animal"), also performed well. Thus, we conclude that, although species classifiers were not accurate enough to automate image processing, DL could be used to improve efficiencies by accepting classifications with high confidence values for certain species or by filtering images containing blanks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/MB59NJDB/Vélez et al. - 2022 - Choosing an Appropriate Platform and Workflow for Processing Camera Trap Data using Artificial Intel.pdf}
}

@article{zotinANIMALDETECTIONUSING2019,
  title = {{{ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS}}},
  author = {Zotin, A. G. and Proskurin, A. V.},
  date = {2019-05-09},
  journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XLII-2-W12},
  pages = {249--257},
  publisher = {Copernicus GmbH},
  issn = {1682-1750},
  doi = {10.5194/isprs-archives-XLII-2-W12-249-2019},
  url = {https://isprs-archives.copernicus.org/articles/XLII-2-W12/249/2019/isprs-archives-XLII-2-W12-249-2019.html},
  urldate = {2025-03-16},
  abstract = {Camera traps providing enormous number of images during a season help to observe remotely animals in the wild. However, analysis of such image collection manually is impossible. In this research, we develop a method for automatic animal detection based on background modeling of scene under complex shooting. First, we design a fast algorithm for image selection without motions. Second, the images are processed by modified Multi-Scale Retinex algorithm in order to align uneven illumination. Finally, background is subtracted from incoming image using adaptive threshold. A threshold value is adjusted by saliency map, which is calculated using pyramid consisting of the original image and images modified by MSR algorithm. Proposed method allows to achieve high estimators of animals detection.},
  eventtitle = {International {{Workshop}} on “{{Photogrammetric}} and {{Computer Vision Techniques}} for {{Video Surveillance}}, {{Biometrics}} and {{Biomedicine}}” ({{Volume XLII-2}}/{{W12}}) - 13\&ndash;15 {{May}} 2019, {{Moscow}}, {{Russia}}},
  langid = {english},
  keywords = {Animal Detection,Background Modeling,Camera Traps,MSR Algorithm},
  file = {/Volumes/ExSSD/UserData/Zotero/storage/LQHGNGWC/Zotin and Proskurin - 2019 - ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING CONDITIONS.pdf}
}
